[
  {
    "path": "langgraph/thinking-in-langgraph.mdx",
    "filename": "thinking-in-langgraph.mdx",
    "size_bytes": 40032,
    "line_count": 1129,
    "preview": "---\ntitle: Thinking in LangGraph\ndescription: Learn how to think about building agents with LangGraph\n---\n\nWhen you build an agent with LangGraph, you will first break it apart into discrete steps called **nodes**. Then, you will describe the different decisions and transitions from each of your nodes. Finally, you connect nodes together through a shared **state** that each node can read from and write to.\n\nIn this walkthrough, we'll guide you through the thought process of building a customer support email agent with LangGraph.\n\n## Start with the process you want to automate\n"
  }
,
  {
    "path": "langgraph/studio.mdx",
    "filename": "studio.mdx",
    "size_bytes": 7003,
    "line_count": 238,
    "preview": "---\ntitle: LangSmith Studio\n---\n\nWhen building agents with LangChain locally, it's helpful to visualize what's happening inside your agent, interact with it in real-time, and debug issues as they occur. **LangSmith Studio** is a free visual interface for developing and testing your LangChain agents from your local machine.\n\nStudio connects to your locally running agent to show you each step your agent takes: the prompts sent to the model, tool calls and their results, and the final output. You can test different inputs, inspect intermediate states, and iterate on your agent's behavior without additional code or deployment.\n\nThis pages describes how to set up Studio with your local LangChain agent.\n\n"
  }
,
  {
    "path": "langgraph/use-time-travel.mdx",
    "filename": "use-time-travel.mdx",
    "size_bytes": 11598,
    "line_count": 385,
    "preview": "---\ntitle: Use time-travel\nsidebarTitle: Time travel\n---\n\n\n\nWhen working with non-deterministic systems that make model-based decisions (e.g., agents powered by LLMs), it can be useful to examine their decision-making process in detail:\n\n1. <Icon icon=\"lightbulb\" size={16} /> **Understand reasoning**: Analyze the steps that led to a successful result.\n"
  }
,
  {
    "path": "langgraph/application-structure.mdx",
    "filename": "application-structure.mdx",
    "size_bytes": 6440,
    "line_count": 162,
    "preview": "---\ntitle: Application structure\n---\n\nA LangGraph application consists of one or more graphs, a configuration file (`langgraph.json`), a file that specifies dependencies, and an optional `.env` file that specifies environment variables.\n\nThis guide shows a typical structure of an application and shows you how to provide the required configuration to deploy an application with [LangSmith Deployment](/langsmith/deployments).\n\n<Info>\nLangSmith Deployment is a managed hosting platform for deploying and scaling LangGraph agents. It handles the infrastructure, scaling, and operational concerns so you can deploy your stateful, long-running agents directly from your repository. Learn more in the [Deployment documentation](/langsmith/deployments).\n"
  }
,
  {
    "path": "langgraph/sql-agent.mdx",
    "filename": "sql-agent.mdx",
    "size_bytes": 35729,
    "line_count": 986,
    "preview": "---\ntitle: Build a custom SQL agent\nsidebarTitle: Custom SQL agent\n---\n\nimport ChatModelTabsPy from '/snippets/chat-model-tabs.mdx';\nimport ChatModelTabsJS from '/snippets/chat-model-tabs-js.mdx';\n\n\nIn this tutorial we will build a custom agent that can answer questions about a SQL database using LangGraph.\n"
  }
,
  {
    "path": "langgraph/overview.mdx",
    "filename": "overview.mdx",
    "size_bytes": 5355,
    "line_count": 120,
    "preview": "---\ntitle: LangGraph overview\nsidebarTitle: Overview\ndescription: Gain control with LangGraph to design agents that reliably handle complex tasks\n---\n\nTrusted by companies shaping the future of agents-- including Klarna, Replit, Elastic, and more-- LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\n\nLangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with [models](/oss/langchain/models) and [tools](/oss/langchain/tools).\n\n"
  }
,
  {
    "path": "langgraph/graph-api.mdx",
    "filename": "graph-api.mdx",
    "size_bytes": 61363,
    "line_count": 1673,
    "preview": "---\ntitle: Graph API overview\nsidebarTitle: Graph API\n---\n\n\n\n## Graphs\n\nAt its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:\n"
  }
,
  {
    "path": "langgraph/pregel.mdx",
    "filename": "pregel.mdx",
    "size_bytes": 20586,
    "line_count": 664,
    "preview": "---\ntitle: LangGraph runtime\nsidebarTitle: Runtime\n---\n\n\n\n:::python\n@[`Pregel`] implements LangGraph's runtime, managing the execution of LangGraph applications.\n\n"
  }
,
  {
    "path": "langgraph/deploy.mdx",
    "filename": "deploy.mdx",
    "size_bytes": 4779,
    "line_count": 144,
    "preview": "---\ntitle: LangSmith Deployment\n---\n\nThis guide shows you how to deploy your agent to **[LangSmith Cloud](/langsmith/deploy-to-cloud)**, a fully managed hosting platform designed for agent workloads. With Cloud deployment, you can deploy directly from your GitHub repository—LangSmith handles the infrastructure, scaling, and operational concerns.\n\nTraditional hosting platforms are built for stateless, short-lived web applications. LangSmith Cloud is **purpose-built for stateful, long-running agents** that require persistent state and background execution.\n\n<Tip>\nLangSmith offers multiple deployment options beyond Cloud, including deploying with a [control plane (hybrid/self-hosted)](/langsmith/deploy-with-control-plane) or as [standalone servers](/langsmith/deploy-standalone-server). For more information, refer to the [Deployment overview](/langsmith/deployments).\n"
  }
,
  {
    "path": "langgraph/streaming.mdx",
    "filename": "streaming.mdx",
    "size_bytes": 46915,
    "line_count": 1496,
    "preview": "---\ntitle: Streaming\n---\n\n\n\nLangGraph implements a streaming system to surface real-time updates. Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\n\nWhat's possible with LangGraph streaming:\n\n"
  }
,
  {
    "path": "langgraph/observability.mdx",
    "filename": "observability.mdx",
    "size_bytes": 6407,
    "line_count": 224,
    "preview": "---\ntitle: LangSmith Observability\n---\n\nTraces are a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a run. You can use [LangSmith](https://smith.langchain.com/) to visualize these execution steps. To use it, [enable tracing for your application](/langsmith/trace-with-langgraph). This enables you to do the following:\n\n* [Debug a locally running application](/langsmith/observability-studio#debug-langsmith-traces).\n* [Evaluate the application performance](/oss/langchain/evals).\n* [Monitor the application](/langsmith/dashboards).\n\n"
  }
,
  {
    "path": "langgraph/install.mdx",
    "filename": "install.mdx",
    "size_bytes": 1288,
    "line_count": 85,
    "preview": "---\ntitle: Install LangGraph\nsidebarTitle: Install\n---\n\n\n\nTo install the base LangGraph package:\n\n:::python\n"
  }
,
  {
    "path": "langgraph/add-memory.mdx",
    "filename": "add-memory.mdx",
    "size_bytes": 78770,
    "line_count": 2120,
    "preview": "---\ntitle: Memory\n---\n\n\n\nAI applications need [memory](/oss/concepts/memory) to share context across multiple interactions. In LangGraph, you can add two types of memory:\n\n* [Add short-term memory](#add-short-term-memory) as a part of your agent's [state](/oss/langgraph/graph-api#state) to enable multi-turn conversations.\n* [Add long-term memory](#add-long-term-memory) to store user-specific or application-level data across sessions.\n"
  }
,
  {
    "path": "langgraph/choosing-apis.mdx",
    "filename": "choosing-apis.mdx",
    "size_bytes": 18866,
    "line_count": 703,
    "preview": "---\ntitle: Choosing between Graph and Functional APIs\nsidebarTitle: Choosing APIs\n---\n\nLangGraph provides two different APIs to build agent workflows: the **Graph API** and the **Functional API**. Both APIs share the same underlying runtime and can be used together in the same application, but they are designed for different use cases and development preferences.\n\nThis guide will help you understand when to use each API based on your specific requirements.\n\n## Quick decision guide\n"
  }
,
  {
    "path": "langgraph/local-server.mdx",
    "filename": "local-server.mdx",
    "size_bytes": 9290,
    "line_count": 331,
    "preview": "---\ntitle: Run a local server\nsidebarTitle: Local server\n---\n\n\n\nThis guide shows you how to run a LangGraph application locally.\n\n## Prerequisites\n"
  }
,
  {
    "path": "langgraph/agentic-rag.mdx",
    "filename": "agentic-rag.mdx",
    "size_bytes": 34137,
    "line_count": 1015,
    "preview": "---\ntitle: Build a custom RAG agent with LangGraph\nsidebarTitle: Custom RAG agent\n---\n\n## Overview\n\nIn this tutorial we will build a [retrieval](/oss/langchain/retrieval) agent using LangGraph.\n\nLangChain offers built-in [agent](/oss/langchain/agents) implementations, implemented using [LangGraph](/oss/langgraph/overview) primitives. If deeper customization is required, agents can be implemented directly in LangGraph. This guide demonstrates an example implementation of a retrieval agent. [Retrieval](/oss/langchain/retrieval) agents are useful when you want an LLM to make a decision about whether to retrieve context from a vectorstore or respond to the user directly.\n"
  }
,
  {
    "path": "langgraph/ui.mdx",
    "filename": "ui.mdx",
    "size_bytes": 1093,
    "line_count": 23,
    "preview": "---\ntitle: Agent Chat UI\n---\n\nimport agent_chat_ui from '/snippets/oss/agent-chat-ui.mdx';\n\n<agent_chat_ui />\n\n### Connect to your agent\n\n"
  }
,
  {
    "path": "langgraph/case-studies.mdx",
    "filename": "case-studies.mdx",
    "size_bytes": 8636,
    "line_count": 51,
    "preview": "---\ntitle: Case studies\n---\n\n\n\nThis list of companies using LangGraph and their success stories is compiled from public sources. If your company uses LangGraph, we'd love for you to share your story and add it to the list. You’re also welcome to contribute updates based on publicly available information from other companies, such as blog posts or press releases.\n\n| Company | Industry | Use case | Reference |\n| --- | --- | --- | --- |\n"
  }
,
  {
    "path": "langgraph/use-functional-api.mdx",
    "filename": "use-functional-api.mdx",
    "size_bytes": 49374,
    "line_count": 1675,
    "preview": "---\ntitle: Use the functional API\nsidebarTitle: Use the Functional API\n---\n\n\n\nThe [**Functional API**](/oss/langgraph/functional-api) allows you to add LangGraph's key features — [persistence](/oss/langgraph/persistence), [memory](/oss/langgraph/add-memory), [human-in-the-loop](/oss/langgraph/interrupts), and [streaming](/oss/langgraph/streaming) — to your applications with minimal changes to your existing code.\n\n<Tip>\n"
  }
,
  {
    "path": "langgraph/memory.mdx",
    "filename": "memory.mdx",
    "size_bytes": 22430,
    "line_count": 274,
    "preview": "---\ntitle: Memory overview\n---\n\n\n\n[Memory](/oss/langgraph/add-memory) is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.\n\nThis conceptual guide covers two types of memory, based on their recall scope:\n\n"
  }
,
  {
    "path": "langgraph/durable-execution.mdx",
    "filename": "durable-execution.mdx",
    "size_bytes": 14452,
    "line_count": 284,
    "preview": "---\ntitle: Durable execution\n---\n\n\n\n**Durable execution** is a technique in which a process or workflow saves its progress at key points, allowing it to pause and later resume exactly where it left off. This is particularly useful in scenarios that require [human-in-the-loop](/oss/langgraph/interrupts), where users can inspect, validate, or modify the process before continuing, and in long-running tasks that might encounter interruptions or errors (e.g., calls to an LLM timing out). By preserving completed work, durable execution enables a process to resume without reprocessing previous steps -- even after a significant delay (e.g., a week later).\n\nLangGraph's built-in [persistence](/oss/langgraph/persistence) layer provides durable execution for workflows, ensuring that the state of each execution step is saved to a durable store. This capability guarantees that if a workflow is interrupted -- whether by a system failure or for [human-in-the-loop](/oss/langgraph/interrupts) interactions -- it can be resumed from its last recorded state.\n\n"
  }
,
  {
    "path": "langgraph/quickstart.mdx",
    "filename": "quickstart.mdx",
    "size_bytes": 27539,
    "line_count": 1214,
    "preview": "---\ntitle: Quickstart\n---\n\n\n\nThis quickstart demonstrates how to build a calculator agent using the LangGraph Graph API or the Functional API.\n\n- [Use the Graph API](#use-the-graph-api) if you prefer to define your agent as a graph of nodes and edges.\n- [Use the Functional API](#use-the-functional-api) if you prefer to define your agent as a single function.\n"
  }
,
  {
    "path": "langgraph/interrupts.mdx",
    "filename": "interrupts.mdx",
    "size_bytes": 51275,
    "line_count": 1563,
    "preview": "---\ntitle: Interrupts\n---\n\n\n\nInterrupts allow you to pause graph execution at specific points and wait for external input before continuing. This enables human-in-the-loop patterns where you need external input to proceed. When an interrupt is triggered, LangGraph saves the graph state using its [persistence](/oss/langgraph/persistence) layer and waits indefinitely until you resume execution.\n\nInterrupts work by calling the `interrupt()` function at any point in your graph nodes. The function accepts any JSON-serializable value which is surfaced to the caller. When you're ready to continue, you resume execution by re-invoking the graph using `Command`, which then becomes the return value of the `interrupt()` call from inside the node.\n\n"
  }
,
  {
    "path": "langgraph/use-subgraphs.mdx",
    "filename": "use-subgraphs.mdx",
    "size_bytes": 28392,
    "line_count": 920,
    "preview": "---\ntitle: Subgraphs\nsidebarTitle: Subgraphs\n---\n\n\n\nThis guide explains the mechanics of using subgraphs. A subgraph is a [graph](/oss/langgraph/graph-api#graphs) that is used as a [node](/oss/langgraph/graph-api#nodes) in another graph.\n\nSubgraphs are useful for:\n"
  }
,
  {
    "path": "langgraph/errors/INVALID_GRAPH_NODE_RETURN_VALUE.mdx",
    "filename": "INVALID_GRAPH_NODE_RETURN_VALUE.mdx",
    "size_bytes": 2260,
    "line_count": 85,
    "preview": "---\ntitle: INVALID_GRAPH_NODE_RETURN_VALUE\n---\n\n:::python\nA LangGraph [`StateGraph`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)\nreceived a non-dict return type from a node. Here's an example:\n\n```python\nclass State(TypedDict):\n"
  }
,
  {
    "path": "langgraph/errors/MISSING_CHECKPOINTER.mdx",
    "filename": "MISSING_CHECKPOINTER.mdx",
    "size_bytes": 1708,
    "line_count": 70,
    "preview": "---\ntitle: MISSING_CHECKPOINTER\n---\n\nYou are attempting to use built-in LangGraph persistence without providing a checkpointer.\n\n:::python\nThis happens when a `checkpointer` is missing in the `compile()` method of @[`StateGraph`][StateGraph] or @[`@entrypoint`].\n:::\n\n"
  }
,
  {
    "path": "langgraph/errors/INVALID_CONCURRENT_GRAPH_UPDATE.mdx",
    "filename": "INVALID_CONCURRENT_GRAPH_UPDATE.mdx",
    "size_bytes": 3164,
    "line_count": 103,
    "preview": "---\ntitle: INVALID_CONCURRENT_GRAPH_UPDATE\n---\n\nA LangGraph [`StateGraph`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) received concurrent updates to its state from multiple nodes to a state property that doesn't\nsupport it.\n\nOne way this can occur is if you are using a [fanout](/oss/langgraph/graph-api#map-reduce-and-the-send-api)\nor other parallel execution in your graph and you have defined a graph like this:\n\n"
  }
,
  {
    "path": "langgraph/errors/GRAPH_RECURSION_LIMIT.mdx",
    "filename": "GRAPH_RECURSION_LIMIT.mdx",
    "size_bytes": 1545,
    "line_count": 64,
    "preview": "---\ntitle: GRAPH_RECURSION_LIMIT\n---\n\nYour LangGraph [`StateGraph`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) reached the maximum number of steps before hitting a stop condition.\nThis is often due to an infinite loop caused by code like the example below:\n\n:::python\n```python\nclass State(TypedDict):\n"
  }
,
  {
    "path": "langgraph/errors/INVALID_CHAT_HISTORY.mdx",
    "filename": "INVALID_CHAT_HISTORY.mdx",
    "size_bytes": 3740,
    "line_count": 60,
    "preview": "---\ntitle: INVALID_CHAT_HISTORY\n---\n\n:::python\nThis error is raised in the prebuilt @[`create_agent`] when the `call_model` graph node receives a malformed list of messages. Specifically, it is malformed when there are `AIMessages` with `tool_calls` (LLM requesting to call a tool) that do not have a corresponding @[`ToolMessage`] (result of a tool invocation to return to the LLM).\n:::\n\n:::js\nThis error is raised in the prebuilt @[`createAgent`] when the `callModel` graph node receives a malformed list of messages. Specifically, it is malformed when there are `AIMessage`s with `tool_calls` (LLM requesting to call a tool) that do not have a corresponding @[`ToolMessage`] (result of a tool invocation to return to the LLM).\n"
  }
,
  {
    "path": "langgraph/errors/MULTIPLE_SUBGRAPHS.mdx",
    "filename": "MULTIPLE_SUBGRAPHS.mdx",
    "size_bytes": 778,
    "line_count": 21,
    "preview": "---\ntitle: MULTIPLE_SUBGRAPHS\n---\n\nYou are calling subgraphs multiple times within a single LangGraph node with checkpointing enabled for each subgraph.\n\nThis is currently not allowed due to internal restrictions on how checkpoint namespacing for subgraphs works.\n\n## Troubleshooting\n\n"
  }
,
  {
    "path": "langgraph/use-graph-api.mdx",
    "filename": "use-graph-api.mdx",
    "size_bytes": 110027,
    "line_count": 3569,
    "preview": "---\ntitle: Use the graph API\nsidebarTitle: Use the graph API\n---\n\n\n\nimport ChatModelTabs from '/snippets/chat-model-tabs.mdx';\n\nThis guide demonstrates the basics of LangGraph's Graph API. It walks through [state](#define-and-update-state), as well as composing common graph structures such as [sequences](#create-a-sequence-of-steps), [branches](#create-branches), and [loops](#create-and-control-loops). It also covers LangGraph's control features, including the [Send API](#map-reduce-and-the-send-api) for map-reduce workflows and the [Command API](#combine-control-flow-and-state-updates-with-command) for combining state updates with \"hops\" across nodes.\n"
  }
,
  {
    "path": "langgraph/persistence.mdx",
    "filename": "persistence.mdx",
    "size_bytes": 42396,
    "line_count": 1137,
    "preview": "---\ntitle: Persistence\n---\n\n\n\nLangGraph has a built-in persistence layer, implemented through checkpointers. When you compile a graph with a checkpointer, the checkpointer saves a `checkpoint` of the graph state at every super-step. Those checkpoints are saved to a `thread`, which can be accessed after graph execution. Because `threads` allow access to graph's state after execution, several powerful capabilities including human-in-the-loop, memory, time travel, and fault-tolerance are all possible. Below, we'll discuss each of these concepts in more detail.\n\n![Checkpoints](/oss/images/checkpoints.jpg)\n\n"
  }
,
  {
    "path": "langgraph/workflows-agents.mdx",
    "filename": "workflows-agents.mdx",
    "size_bytes": 58199,
    "line_count": 2179,
    "preview": "---\ntitle: Workflows and agents\nsidebarTitle: Workflows + agents\n---\n\n\n\nThis guide reviews common workflow and agent patterns.\n\n- Workflows have predetermined code paths and are designed to operate in a certain order.\n"
  }
,
  {
    "path": "langgraph/functional-api.mdx",
    "filename": "functional-api.mdx",
    "size_bytes": 39043,
    "line_count": 1129,
    "preview": "---\ntitle: Functional API overview\nsidebarTitle: Functional API\n---\n\n\n\nThe **Functional API** allows you to add LangGraph's key features — [persistence](/oss/langgraph/persistence), [memory](/oss/langgraph/add-memory), [human-in-the-loop](/oss/langgraph/interrupts), and [streaming](/oss/langgraph/streaming) — to your applications with minimal changes to your existing code.\n\nIt is designed to integrate these features into existing code that may use standard language primitives for branching and control flow, such as `if` statements, `for` loops, and function calls. Unlike many data orchestration frameworks that require restructuring code into an explicit pipeline or DAG, the Functional API allows you to incorporate these capabilities without enforcing a rigid execution model.\n"
  }
,
  {
    "path": "langgraph/test.mdx",
    "filename": "test.mdx",
    "size_bytes": 10346,
    "line_count": 313,
    "preview": "---\ntitle: Test\n---\n\n\n\nAfter you've prototyped your LangGraph agent, a natural next step is to add tests. This guide covers some useful patterns you can use when writing unit tests.\n\n:::python\nNote that this guide is LangGraph-specific and covers scenarios around graphs with custom structures - if you are just getting started, check out [this section](/oss/langchain/test/) that uses LangChain's built-in @[`create_agent`] instead.\n"
  }
,
  {
    "path": "common-errors.mdx",
    "filename": "common-errors.mdx",
    "size_bytes": 1695,
    "line_count": 24,
    "preview": "---\ntitle: Errors\nsidebarTitle: Reference\n---\n\nThis page contains guides around resolving common errors you may find while building with LangChain and LangGraph.\n\nErrors referenced below will have an `lc_error_code` property corresponding to one of the below codes when they are thrown in code.\n\n| Error code                                                                                 |\n"
  }
,
  {
    "path": "python/integrations/chat_loaders/langsmith_llm_runs.mdx",
    "filename": "langsmith_llm_runs.mdx",
    "size_bytes": 7191,
    "line_count": 265,
    "preview": "---\ntitle: LangSmith LLM Runs\n---\n\nThis notebook demonstrates how to directly load data from LangSmith's LLM runs and fine-tune a model on that data.\nThe process is simple and comprises 3 steps.\n\n1. Select the LLM runs to train on.\n2. Use the LangSmithRunChatLoader to load runs as chat sessions.\n3. Fine-tune your model.\n"
  }
,
  {
    "path": "python/integrations/chat_loaders/facebook.mdx",
    "filename": "facebook.mdx",
    "size_bytes": 9402,
    "line_count": 297,
    "preview": "---\ntitle: Facebook Messenger\n---\n\nThis notebook shows how to load data from Facebook into a format you can fine-tune on. The overall steps are:\n\n1. Download your messenger data to disk.\n2. Create the Chat Loader and call `loader.load()` (or `loader.lazy_load()`) to perform the conversion.\n3. Optionally use `merge_chat_runs` to combine message from the same sender in sequence, and/or `map_ai_messages` to convert messages from the specified sender to the \"AIMessage\" class. Once you've done this, call `convert_messages_for_finetuning` to prepare your data for fine-tuning.\n\n"
  }
,
  {
    "path": "python/integrations/chat_loaders/twitter.mdx",
    "filename": "twitter.mdx",
    "size_bytes": 967,
    "line_count": 30,
    "preview": "---\ntitle: Twitter (via Apify)\n---\n\nThis notebook shows how to load chat messages from Twitter to fine-tune on. We do this by utilizing Apify.\n\nFirst, use Apify to export tweets. An example\n\n```python\nimport json\n"
  }
,
  {
    "path": "python/integrations/chat_loaders/slack.mdx",
    "filename": "slack.mdx",
    "size_bytes": 3032,
    "line_count": 85,
    "preview": "---\ntitle: Slack\n---\n\nThis notebook shows how to use the Slack chat loader. This class helps map exported slack conversations to LangChain chat messages.\n\nThe process has three steps:\n\n1. Export the desired conversation thread by following the [instructions here](https://slack.com/help/articles/1500001548241-Request-to-export-all-conversations).\n2. Create the `SlackChatLoader` with the file path pointed to the json file or directory of JSON files\n"
  }
,
  {
    "path": "python/integrations/chat_loaders/imessage.mdx",
    "filename": "imessage.mdx",
    "size_bytes": 6802,
    "line_count": 227,
    "preview": "---\ntitle: iMessage\n---\n\nThis notebook shows how to use the iMessage chat loader. This class helps convert iMessage conversations to LangChain chat messages.\n\nOn MacOS, iMessage stores conversations in a sqlite database at `~/Library/Messages/chat.db` (at least for macOS Ventura 13.4).\nThe `IMessageChatLoader` loads from this database file.\n\n1. Create the `IMessageChatLoader` with the file path pointed to `chat.db` database you'd like to process.\n"
  }
,
  {
    "path": "python/integrations/chat_loaders/google_gmail.mdx",
    "filename": "google_gmail.mdx",
    "size_bytes": 2821,
    "line_count": 88,
    "preview": "---\ntitle: Gmail\n---\n\nThis loader goes over how to load data from Gmail. There are many ways you could want to load data from Gmail. This loader is currently fairly opinionated in how to do so. The way it does it is it first looks for all messages that you have sent. It then looks for messages where you are responding to a previous email. It then fetches that previous email, and creates a training example of that email, followed by your email.\n\nNote that there are clear limitations here. For example, all examples created are only looking at the previous email for context.\n\nTo use:\n\n"
  }
,
  {
    "path": "python/integrations/chat_loaders/langsmith_dataset.mdx",
    "filename": "langsmith_dataset.mdx",
    "size_bytes": 3960,
    "line_count": 151,
    "preview": "---\ntitle: LangSmith Chat Datasets\n---\n\nThis notebook demonstrates an easy way to load a LangSmith chat dataset and fine-tune a model on that data.\nThe process is simple and comprises 3 steps.\n\n1. Create the chat dataset.\n2. Use the LangSmithDatasetChatLoader to load examples.\n3. Fine-tune your model.\n"
  }
,
  {
    "path": "python/integrations/middleware/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 440,
    "line_count": 11,
    "preview": "---\ntitle: Provider-specific middleware\nsidebarTitle: Middleware\n---\n\nMiddleware designed for specific providers. Learn more about [middleware](/oss/langchain/middleware/overview).\n\n| Provider | Middleware available |\n|------------|-------------|\n| [Anthropic](/oss/integrations/middleware/anthropic) | Prompt caching, bash tool, text editor, memory, and file search |\n"
  }
,
  {
    "path": "python/integrations/middleware/openai.mdx",
    "filename": "openai.mdx",
    "size_bytes": 5174,
    "line_count": 157,
    "preview": "---\ntitle: OpenAI middleware\n---\n\nMiddleware specifically designed for OpenAI models. Learn more about [middleware](/oss/langchain/middleware/overview).\n\n| Middleware | Description |\n|------------|-------------|\n| [Content moderation](#content-moderation) | Moderate agent traffic using OpenAI's moderation endpoint |\n\n"
  }
,
  {
    "path": "python/integrations/middleware/anthropic.mdx",
    "filename": "anthropic.mdx",
    "size_bytes": 26562,
    "line_count": 808,
    "preview": "---\ntitle: Anthropic middleware\n---\n\nMiddleware specifically designed for Anthropic's Claude models. Learn more about [middleware](/oss/langchain/middleware/overview).\n\n| Middleware | Description |\n|------------|-------------|\n| [Prompt caching](#prompt-caching) | Reduce costs by caching repetitive prompt prefixes |\n| [Bash tool](#bash-tool) | Execute Claude's native bash tool with local command execution |\n"
  }
,
  {
    "path": "python/integrations/text_embedding/cloudflare_workersai.mdx",
    "filename": "cloudflare_workersai.mdx",
    "size_bytes": 1686,
    "line_count": 60,
    "preview": "---\ntitle: Cloudflare Workers AI\n---\n\n>[Cloudflare, Inc. (Wikipedia)](https://en.wikipedia.org/wiki/Cloudflare) is an American company that provides content delivery network services, cloud cybersecurity, DDoS mitigation, and ICANN-accredited domain registration services.\n\n>[Cloudflare Workers AI](https://developers.cloudflare.com/workers-ai/) allows you to run machine learning models, on the `Cloudflare` network, from your code via REST API.\n\n>[Workers AI Developer Docs](https://developers.cloudflare.com/workers-ai/models/text-embeddings/) lists all text embeddings models available.\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/azure_openai.mdx",
    "filename": "azure_openai.mdx",
    "size_bytes": 5043,
    "line_count": 142,
    "preview": "---\ntitle: AzureOpenAIEmbeddings\n---\n\nThis will help you get started with AzureOpenAI embedding models using LangChain. For detailed documentation on `AzureOpenAIEmbeddings` features and configuration options, please refer to the [API reference](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.azure.AzureOpenAIEmbeddings.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/lindorm.mdx",
    "filename": "lindorm.mdx",
    "size_bytes": 4069,
    "line_count": 131,
    "preview": "---\ntitle: Lindorm\n---\n\n\nThis will help you get started with Lindorm embedding models using LangChain.\n\n## Overview\n\n### Integration details\n"
  }
,
  {
    "path": "python/integrations/text_embedding/instruct_embeddings.mdx",
    "filename": "instruct_embeddings.mdx",
    "size_bytes": 702,
    "line_count": 33,
    "preview": "---\ntitle: Instruct Embeddings on Hugging Face\n---\n\n>[Hugging Face sentence-transformers](https://huggingface.co/sentence-transformers) is a Python framework for state-of-the-art sentence, text and image embeddings.\n>One of the instruct embedding models is used in the `HuggingFaceInstructEmbeddings` class.\n\n```python\nfrom langchain_community.embeddings import HuggingFaceInstructEmbeddings\n```\n"
  }
,
  {
    "path": "python/integrations/text_embedding/oracleai.mdx",
    "filename": "oracleai.mdx",
    "size_bytes": 9992,
    "line_count": 180,
    "preview": "---\ntitle: Oracle AI Vector Search Generate Embeddings\n---\n\nOracle AI Vector Search is designed for Artificial Intelligence (AI) workloads that allows you to query data based on semantics, rather than keywords.\nOne of the biggest benefits of Oracle AI Vector Search is that semantic search on unstructured data can be combined with relational search on business data in one single system.\nThis is not only powerful but also significantly more effective because you don't need to add a specialized vector database, eliminating the pain of data fragmentation between multiple systems.\n\nIn addition, your vectors can benefit from all of Oracle Database’s most powerful features, like the following:\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/fastembed.mdx",
    "filename": "fastembed.mdx",
    "size_bytes": 2152,
    "line_count": 74,
    "preview": "---\ntitle: FastEmbed by Qdrant\n---\n\n>[FastEmbed](https://qdrant.github.io/fastembed/) from [Qdrant](https://qdrant.tech) is a lightweight, fast, Python library built for embedding generation.\n>\n>- Quantized model weights\n>- ONNX Runtime, no PyTorch dependency\n>- CPU-first design\n>- Data-parallelism for encoding of large datasets.\n"
  }
,
  {
    "path": "python/integrations/text_embedding/ascend.mdx",
    "filename": "ascend.mdx",
    "size_bytes": 1818,
    "line_count": 81,
    "preview": "---\ntitle: Ascend\n---\n\n```python\nfrom langchain_community.embeddings import AscendEmbeddings\n\nmodel = AscendEmbeddings(\n    model_path=\"/root/.cache/modelscope/hub/yangjhchs/acge_text_embedding\",\n    device_id=0,\n"
  }
,
  {
    "path": "python/integrations/text_embedding/nlp_cloud.mdx",
    "filename": "nlp_cloud.mdx",
    "size_bytes": 952,
    "line_count": 36,
    "preview": "---\ntitle: NLP Cloud\n---\n\n>[NLP Cloud](https://docs.nlpcloud.com/#introduction) is an artificial intelligence platform that allows you to use the most advanced AI engines, and even train your own engines with your own data.\n\nThe [embeddings](https://docs.nlpcloud.com/#embeddings) endpoint offers the following model:\n\n* `paraphrase-multilingual-mpnet-base-v2`: Paraphrase Multilingual MPNet Base V2 is a very fast model based on Sentence Transformers that is perfectly suited for embeddings extraction in more than 50 languages (see the full list here).\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/baseten.mdx",
    "filename": "baseten.mdx",
    "size_bytes": 4974,
    "line_count": 135,
    "preview": "---\ntitle: BasetenEmbeddings\n---\n\nThis will help you get started with Baseten embedding models using LangChain. For detailed documentation on `BasetenEmbeddings` features and configuration options, please refer to the [API reference](https://python.langchain.com/api_reference/baseten/embeddings/langchain_baseten.embeddings.BasetenEmbeddings.html).\n\n## Overview\n\nBaseten provides inference designed for production applications. Built on the Baseten Inference Stack, these APIs deliver enterprise-grade performance and reliability for leading open-source or custom models: https://www.baseten.co/library/.\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/isaacus.mdx",
    "filename": "isaacus.mdx",
    "size_bytes": 5413,
    "line_count": 98,
    "preview": "---\ntitle: Isaacus\n---\n\nThis guide walks you through how to get started generating legal embeddings using [Isaacus'](/oss/integrations/providers/isaacus) LangChain integration.\n\n## 1. Set up your account\n\nHead to the [Isaacus Platform](https://platform.isaacus.com/accounts/signup/) to create a new account.\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/huggingfacehub.mdx",
    "filename": "huggingfacehub.mdx",
    "size_bytes": 2104,
    "line_count": 96,
    "preview": "---\ntitle: Hugging Face\nsidebarTitle: HuggingFaceEmbeddings\n---\n\nLet's load the Hugging Face Embedding class.\n\n```python\npip install -qU  langchain langchain-huggingface sentence_transformers\n```\n"
  }
,
  {
    "path": "python/integrations/text_embedding/ovhcloud.mdx",
    "filename": "ovhcloud.mdx",
    "size_bytes": 11763,
    "line_count": 32,
    "preview": "---\ntitle: OVHcloud\n---\n\n> In order to use this model you need to create a new token on the AI Endpoints website: [endpoints.ai.cloud.ovh.net/](https://endpoints.ai.cloud.ovh.net/).\n\nThis notebook explains how to use OVHCloudEmbeddings, which is included in the langchain_community package, to embed texts in langchain.\n\n```python\nfrom langchain_community.embeddings.ovhcloud import OVHCloudEmbeddings\n"
  }
,
  {
    "path": "python/integrations/text_embedding/sagemaker-endpoint.mdx",
    "filename": "sagemaker-endpoint.mdx",
    "size_bytes": 2945,
    "line_count": 101,
    "preview": "---\ntitle: SageMaker\n---\n\nLet's load the `SageMaker Endpoints Embeddings` class. The class can be used if you host, e.g. your own Hugging Face model on SageMaker.\n\nFor instructions on how to do this, please see [here](https://www.philschmid.de/custom-inference-huggingface-sagemaker).\n\n**Note**: In order to handle batched requests, you will need to adjust the return line in the `predict_fn()` function within the custom `inference.py` script:\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/clova.mdx",
    "filename": "clova.mdx",
    "size_bytes": 735,
    "line_count": 33,
    "preview": "---\ntitle: Clova Embeddings\n---\n\n[Clova](https://api.ncloud-docs.com/docs/ai-naver-clovastudio-summary) offers an embeddings service\n\nThis example goes over how to use LangChain to interact with Clova inference for text embedding.\n\n```python\nimport os\n"
  }
,
  {
    "path": "python/integrations/text_embedding/google_generative_ai.mdx",
    "filename": "google_generative_ai.mdx",
    "size_bytes": 6222,
    "line_count": 173,
    "preview": "---\ntitle: Google Generative AI Embeddings (AI Studio & Gemini API)\nsidebarTitle: GoogleGenerativeAIEmbeddings\n---\n\nConnect to Google's generative AI embeddings service using the `GoogleGenerativeAIEmbeddings` class, found in the [langchain-google-genai](https://pypi.org/project/langchain-google-genai/) package.\n\nThis will help you get started with Google's Generative AI embedding models (like Gemini) using LangChain. For detailed documentation on `GoogleGenerativeAIEmbeddings` features and configuration options, please refer to the [API reference](https://python.langchain.com/v0.2/api_reference/google_genai/embeddings/langchain_google_genai.embeddings.GoogleGenerativeAIEmbeddings.html).\n\n## Overview\n"
  }
,
  {
    "path": "python/integrations/text_embedding/mistralai.mdx",
    "filename": "mistralai.mdx",
    "size_bytes": 4228,
    "line_count": 128,
    "preview": "---\ntitle: MistralAIEmbeddings\n---\n\nThis will help you get started with MistralAI embedding models using LangChain. For detailed documentation on `MistralAIEmbeddings` features and configuration options, please refer to the [API reference](https://python.langchain.com/api_reference/mistralai/embeddings/langchain_mistralai.embeddings.MistralAIEmbeddings.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/tensorflowhub.mdx",
    "filename": "tensorflowhub.mdx",
    "size_bytes": 1361,
    "line_count": 43,
    "preview": "---\ntitle: TensorFlow Hub\n---\n\n>[TensorFlow Hub](https://www.tensorflow.org/hub) is a repository of trained machine learning models ready for fine-tuning and deployable anywhere. Reuse trained models like `BERT` and `Faster R-CNN` with just a few lines of code.\n>\n>\nLet's load the TensorflowHub Embedding class.\n\n```python\n"
  }
,
  {
    "path": "python/integrations/text_embedding/naver.mdx",
    "filename": "naver.mdx",
    "size_bytes": 4750,
    "line_count": 130,
    "preview": "---\ntitle: Naver\n---\n\nThis notebook covers how to get started with embedding models provided by CLOVA Studio. For detailed documentation on `ClovaXEmbeddings` features and configuration options, please refer to the [API reference](https://guide.ncloud-docs.com/docs/clovastudio-dev-langchain#%EC%9E%84%EB%B2%A0%EB%94%A9%EB%8F%84%EA%B5%AC%EC%9D%B4%EC%9A%A9).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/nomic.mdx",
    "filename": "nomic.mdx",
    "size_bytes": 4939,
    "line_count": 141,
    "preview": "---\ntitle: NomicEmbeddings\n---\n\nThis will help you get started with Nomic embedding models using LangChain. For detailed documentation on `NomicEmbeddings` features and configuration options, please refer to the [API reference](https://python.langchain.com/api_reference/nomic/embeddings/langchain_nomic.embeddings.NomicEmbeddings.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/oci_generative_ai.mdx",
    "filename": "oci_generative_ai.mdx",
    "size_bytes": 2578,
    "line_count": 73,
    "preview": "---\ntitle: Oracle Cloud Infrastructure Generative AI\n---\n\nOracle Cloud Infrastructure (OCI) Generative AI is a fully managed service that provides a set of state-of-the-art, customizable large language models (LLMs), that cover a wide range of use cases, and which are available through a single API.\nUsing the OCI Generative AI service you can access ready-to-use pretrained models, or create and host your own fine-tuned custom models based on your own data on dedicated AI clusters. Detailed documentation of the service and API is available __[here](https://docs.oracle.com/en-us/iaas/Content/generative-ai/home.htm)__ and __[here](https://docs.oracle.com/en-us/iaas/api/#/en/generative-ai/20231130/)__.\n\nThis notebook explains how to use OCI's Genrative AI models with LangChain.\n\n### Prerequisite\n"
  }
,
  {
    "path": "python/integrations/text_embedding/spacy_embedding.mdx",
    "filename": "spacy_embedding.mdx",
    "size_bytes": 1790,
    "line_count": 52,
    "preview": "---\ntitle: SpaCy\n---\n\n>[spaCy](https://spacy.io/) is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython.\n\n## Installation and setup\n\n```python\npip install -qU  spacy\n"
  }
,
  {
    "path": "python/integrations/text_embedding/nebius.mdx",
    "filename": "nebius.mdx",
    "size_bytes": 7532,
    "line_count": 247,
    "preview": "---\ntitle: Nebius\n---\n\n[Nebius AI Studio](https://studio.nebius.ai/) provides API access to high-quality embedding models through a unified interface. The Nebius embedding models convert text into numerical vectors that capture semantic meaning, making them useful for various applications like semantic search, clustering, and recommendations.\n\n## Overview\n\nThe `NebiusEmbeddings` class provides access to Nebius AI Studio's embedding models through LangChain. These embeddings can be used for semantic search, document similarity, and other NLP tasks requiring vector representations of text.\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/laser.mdx",
    "filename": "laser.mdx",
    "size_bytes": 1544,
    "line_count": 53,
    "preview": "---\ntitle: LASER Language-Agnostic SEntence Representations Embeddings by Meta AI\n---\n\n>[LASER](https://github.com/facebookresearch/LASER/) is a Python library developed by the Meta AI Research team and used for creating multilingual sentence embeddings for over 147 languages as of 2/25/2024\n>\n>- List of supported languages at [github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200)\n\n## Dependencies\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/textembed.mdx",
    "filename": "textembed.mdx",
    "size_bytes": 3038,
    "line_count": 95,
    "preview": "---\ntitle: TextEmbed - Embedding Inference Server\n---\n\nTextEmbed is a high-throughput, low-latency REST API designed for serving vector embeddings. It supports a wide range of sentence-transformer models and frameworks, making it suitable for various applications in natural language processing.\n\n## Features\n\n- **High Throughput & Low Latency:** Designed to handle a large number of requests efficiently.\n- **Flexible Model Support:** Works with various sentence-transformer models.\n"
  }
,
  {
    "path": "python/integrations/text_embedding/ipex_llm.mdx",
    "filename": "ipex_llm.mdx",
    "size_bytes": 1832,
    "line_count": 54,
    "preview": "---\ntitle: IPEX-LLM - Local BGE Embeddings on Intel CPU\n---\n\n> [IPEX-LLM](https://github.com/intel-analytics/ipex-llm) is a PyTorch library for running LLM on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max) with very low latency.\n\nThis example goes over how to use LangChain to conduct embedding tasks with `ipex-llm` optimizations on Intel CPU. This would be helpful in applications such as RAG, document QA, etc.\n\n## Setup\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/aimlapi.mdx",
    "filename": "aimlapi.mdx",
    "size_bytes": 3555,
    "line_count": 107,
    "preview": "---\ntitle: AimlapiEmbeddings\n---\n\nThis guide helps you get started with AI/ML API embedding models using LangChain. For detailed documentation on `AimlapiEmbeddings` features and configuration options, please refer to the [API reference](https://python.langchain.com/api_reference/aimlapi/embeddings/langchain_aimlapi.embeddings.AimlapiEmbeddings.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/llm_rails.mdx",
    "filename": "llm_rails.mdx",
    "size_bytes": 1171,
    "line_count": 48,
    "preview": "---\ntitle: LLMRails\n---\n\nLet's load the LLMRails Embeddings class.\n\nTo use LLMRails embedding you need to pass api key by argument or set it in environment with `LLM_RAILS_API_KEY` key.\nTo gey API Key you need to sign up in [console.llmrails.com/signup](https://console.llmrails.com/signup) and then go to [console.llmrails.com/api-keys](https://console.llmrails.com/api-keys) and copy key from there after creating one key in platform.\n\n```python\n"
  }
,
  {
    "path": "python/integrations/text_embedding/premai.mdx",
    "filename": "premai.mdx",
    "size_bytes": 2817,
    "line_count": 74,
    "preview": "---\ntitle: PremAI\n---\n\n[PremAI](https://premai.io/) is an all-in-one platform that simplifies the creation of robust, production-ready applications powered by Generative AI. By streamlining the development process, PremAI allows you to concentrate on enhancing user experience and driving overall growth for your application. You can quickly start using our platform [here](https://docs.premai.io/quick-start).\n\n### Installation and setup\n\nWe start by installing `langchain` and `premai-sdk`. You can type the following command to install:\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/sparkllm.mdx",
    "filename": "sparkllm.mdx",
    "size_bytes": 3128,
    "line_count": 59,
    "preview": "---\ntitle: SparkLLM Text Embeddings\n---\n\nOfficial Website: [www.xfyun.cn/doc/spark/Embedding_new_api.html](https://www.xfyun.cn/doc/spark/Embedding_new_api.html)\n\nAn API key is required to use this embedding model. You can get one by registering at [platform.SparkLLM-ai.com/docs/text-Embedding](https://platform.SparkLLM-ai.com/docs/text-Embedding).\n\nSparkLLMTextEmbeddings support 2K token window and preduces vectors with 2560 dimensions.\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/baidu_qianfan_endpoint.mdx",
    "filename": "baidu_qianfan_endpoint.mdx",
    "size_bytes": 3751,
    "line_count": 94,
    "preview": "---\ntitle: Baidu Qianfan\n---\n\nBaidu AI Cloud Qianfan Platform is a one-stop large model development and service operation platform for enterprise developers. Qianfan not only provides including the model of Wenxin Yiyan (ERNIE-Bot) and the third-party open-source models, but also provides various AI development tools and the whole set of development environment, which facilitates customers to use and develop large model applications easily.\n\nBasically, those model are split into the following type:\n\n- Embedding\n- Chat\n"
  }
,
  {
    "path": "python/integrations/text_embedding/awadb.mdx",
    "filename": "awadb.mdx",
    "size_bytes": 977,
    "line_count": 40,
    "preview": "---\ntitle: AwaDB\n---\n\n>[AwaDB](https://github.com/awa-ai/awadb) is an AI Native database for the search and storage of embedding vectors used by LLM Applications.\n\nThis notebook explains how to use `AwaEmbeddings` in LangChain.\n\n```python\n# pip install awadb\n"
  }
,
  {
    "path": "python/integrations/text_embedding/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 19075,
    "line_count": 215,
    "preview": "---\ntitle: \"Embedding models\"\n---\n\n## Overview\n\n<Note>\nThis overview covers **text-based embedding models**. LangChain does not currently support multimodal embeddings.\n\nSee [top embedding models](#top-integrations).\n"
  }
,
  {
    "path": "python/integrations/text_embedding/openai.mdx",
    "filename": "openai.mdx",
    "size_bytes": 7622,
    "line_count": 227,
    "preview": "---\ntitle: OpenAIEmbeddings\n---\n\nThis will help you get started with OpenAI embedding models using LangChain. For detailed documentation on `OpenAIEmbeddings` features and configuration options, please refer to the [API reference](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/model2vec.mdx",
    "filename": "model2vec.mdx",
    "size_bytes": 1404,
    "line_count": 63,
    "preview": "---\ntitle: Model2Vec\n---\n\nModel2Vec is a technique to turn any sentence transformer into a really small static model\n[model2vec](https://github.com/MinishLab/model2vec) can be used to generate embeddings.\n\n## Setup\n\n```bash\n"
  }
,
  {
    "path": "python/integrations/text_embedding/baichuan.mdx",
    "filename": "baichuan.mdx",
    "size_bytes": 1305,
    "line_count": 42,
    "preview": "---\ntitle: Baichuan Text Embeddings\n---\n\nAs of today (Jan 25th, 2024) BaichuanTextEmbeddings ranks #1 in C-MTEB (Chinese Multi-Task Embedding Benchmark) leaderboard.\n\nLeaderboard (Under Overall -> Chinese section): [huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n\nOfficial Website: [platform.baichuan-ai.com/docs/text-Embedding](https://platform.baichuan-ai.com/docs/text-Embedding)\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/databricks.mdx",
    "filename": "databricks.mdx",
    "size_bytes": 5824,
    "line_count": 147,
    "preview": "---\ntitle: DatabricksEmbeddings\n---\n\n> [Databricks](https://www.databricks.com/) Lakehouse Platform unifies data, analytics, and AI on one platform.\n\nThis guide provides a quick overview for getting started with Databricks [embedding models](/oss/integrations/text_embedding). For detailed documentation of all `DatabricksEmbeddings` features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/embeddings/langchain_community.embeddings.databricks.DatabricksEmbeddings.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/optimum_intel.mdx",
    "filename": "optimum_intel.mdx",
    "size_bytes": 2212,
    "line_count": 90,
    "preview": "---\ntitle: Embedding Documents using Optimized and Quantized Embedders\n---\n\nEmbedding all documents using Quantized Embedders.\n\nThe embedders are based on optimized models, created by using [optimum-intel](https://github.com/huggingface/optimum-intel.git) and [IPEX](https://github.com/intel/intel-extension-for-pytorch).\n\nExample text is based on [SBERT](https://www.sbert.net/docs/pretrained_cross-encoders.html).\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/zhipuai.mdx",
    "filename": "zhipuai.mdx",
    "size_bytes": 4432,
    "line_count": 134,
    "preview": "---\ntitle: ZhipuAIEmbeddings\n---\n\nThis will help you get started with ZhipuAI embedding models using LangChain. For detailed documentation on `ZhipuAIEmbeddings` features and configuration options, please refer to the [API reference](https://bigmodel.cn/dev/api#vector).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/bedrock.mdx",
    "filename": "bedrock.mdx",
    "size_bytes": 1613,
    "line_count": 47,
    "preview": "---\ntitle: BedrockEmbeddings\n---\n\n>[Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that offers a choice of\n> high-performing foundation models (FMs) from leading AI companies like `AI21 Labs`, `Anthropic`, `Cohere`,\n> `Meta`, `Stability AI`, and `Amazon` via a single API, along with a broad set of capabilities you need to\n> build generative AI applications with security, privacy, and responsible AI. Using `Amazon Bedrock`,\n> you can easily experiment with and evaluate top FMs for your use case, privately customize them with\n> your data using techniques such as fine-tuning and `Retrieval Augmented Generation` (`RAG`), and build\n"
  }
,
  {
    "path": "python/integrations/text_embedding/upstage.mdx",
    "filename": "upstage.mdx",
    "size_bytes": 1759,
    "line_count": 81,
    "preview": "---\ntitle: UpstageEmbeddings\n---\n\nThis notebook covers how to get started with Upstage embedding models.\n\n## Installation\n\nInstall `langchain-upstage` package.\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/localai.mdx",
    "filename": "localai.mdx",
    "size_bytes": 1812,
    "line_count": 56,
    "preview": "---\ntitle: LocalAI Embeddings\n---\n\n<Info>\n**`langchain-localai` is a 3rd party integration package for LocalAI. It provides a simple way to use LocalAI services in LangChain.**\n\nThe source code is available on [GitHub](https://github.com/mkhludnev/langchain-localai)\n\n</Info>\n"
  }
,
  {
    "path": "python/integrations/text_embedding/mosaicml.mdx",
    "filename": "mosaicml.mdx",
    "size_bytes": 1271,
    "line_count": 52,
    "preview": "---\ntitle: MosaicML\n---\n\n>[MosaicML](https://docs.mosaicml.com/en/latest/inference.html) offers a managed inference service. You can either use a variety of open-source models, or deploy your own.\n\nThis example goes over how to use LangChain to interact with `MosaicML` Inference for text embedding.\n\n```python\n# sign up for an account: https://forms.mosaicml.com/demo?utm_source=langchain\n"
  }
,
  {
    "path": "python/integrations/text_embedding/ibm_watsonx.mdx",
    "filename": "ibm_watsonx.mdx",
    "size_bytes": 5792,
    "line_count": 188,
    "preview": "---\ntitle: WatsonxEmbeddings\n---\n\n>`WatsonxEmbeddings` is a wrapper for IBM [watsonx.ai](https://www.ibm.com/products/watsonx-ai) foundation models.\n\nThis example shows how to communicate with `watsonx.ai` models using `LangChain`.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/sambanova.mdx",
    "filename": "sambanova.mdx",
    "size_bytes": 4124,
    "line_count": 124,
    "preview": "---\ntitle: SambaNovaEmbeddings\n---\n\nThis will help you get started with SambaNova embedding models using LangChain. For detailed documentation on `SambaNovaEmbeddings` features and configuration options, please refer to the [API reference](https://docs.sambanova.ai/cloud/docs/get-started/overview).\n\n**[SambaNova](https://sambanova.ai/)'s** [SambaCloud](https://cloud.sambanova.ai/) is a platform for performing inference with open-source models\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/gpt4all.mdx",
    "filename": "gpt4all.mdx",
    "size_bytes": 1101,
    "line_count": 41,
    "preview": "---\ntitle: GPT4All\n---\n\n[GPT4All](https://gpt4all.io/index.html) is a free-to-use, locally running, privacy-aware chatbot. There is no GPU or internet required. It features popular models and its own models such as GPT4All Falcon, Wizard, etc.\n\nThis notebook explains how to use [GPT4All embeddings](https://docs.gpt4all.io/gpt4all_python_embedding.html#gpt4all.gpt4all.Embed4All) with LangChain.\n\n## Install GPT4All's Python bindings\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/fireworks.mdx",
    "filename": "fireworks.mdx",
    "size_bytes": 4211,
    "line_count": 128,
    "preview": "---\ntitle: FireworksEmbeddings\n---\n\nThis will help you get started with Fireworks embedding models using LangChain. For detailed documentation on `FireworksEmbeddings` features and configuration options, please refer to the [API reference](https://python.langchain.com/api_reference/fireworks/embeddings/langchain_fireworks.embeddings.FireworksEmbeddings.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/predictionguard.mdx",
    "filename": "predictionguard.mdx",
    "size_bytes": 5709,
    "line_count": 202,
    "preview": "---\ntitle: PredictionGuardEmbeddings\n---\n\n>[Prediction Guard](https://predictionguard.com) is a secure, scalable GenAI platform that safeguards sensitive data, prevents common AI malfunctions, and runs on affordable hardware.\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/elasticsearch.mdx",
    "filename": "elasticsearch.mdx",
    "size_bytes": 2549,
    "line_count": 110,
    "preview": "---\ntitle: Elasticsearch\n---\n\nWalkthrough of how to generate embeddings using a hosted embedding model in Elasticsearch\n\nThe easiest way to instantiate the `ElasticsearchEmbeddings` class it either\n\n- using the `from_credentials` constructor if you are using Elastic Cloud\n- or using the `from_es_connection` constructor with any Elasticsearch cluster\n"
  }
,
  {
    "path": "python/integrations/text_embedding/gradient.mdx",
    "filename": "gradient.mdx",
    "size_bytes": 1750,
    "line_count": 66,
    "preview": "---\ntitle: Gradient\n---\n\n`Gradient` allows to create @[`Embeddings`] as well fine tune and get completions on LLMs with a simple web API.\n\nThis notebook goes over how to use LangChain with Embeddings of [Gradient](https://gradient.ai/).\n\n## Imports\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/edenai.mdx",
    "filename": "edenai.mdx",
    "size_bytes": 2310,
    "line_count": 68,
    "preview": "---\ntitle: EdenAI\n---\n\nEden AI is revolutionizing the AI landscape by uniting the best AI providers, empowering users to unlock limitless possibilities and tap into the true potential of artificial intelligence. With an all-in-one comprehensive and hassle-free platform, it allows users to deploy AI features to production lightning fast, enabling effortless access to the full breadth of AI capabilities via a single API. (website: [edenai.co/](https://edenai.co/))\n\nThis example goes over how to use LangChain to interact with Eden AI embedding models\n\n-----------------------------------------------------------------------------------\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/voyageai.mdx",
    "filename": "voyageai.mdx",
    "size_bytes": 3310,
    "line_count": 103,
    "preview": "---\ntitle: Voyage AI\n---\n\n>[Voyage AI](https://www.voyageai.com/) provides cutting-edge embedding/vectorizations models.\n\nLet's load the Voyage AI Embedding class. (Install the LangChain partner package with `pip install langchain-voyageai`)\n\n```python\nfrom langchain_voyageai import VoyageAIEmbeddings\n"
  }
,
  {
    "path": "python/integrations/text_embedding/pinecone.mdx",
    "filename": "pinecone.mdx",
    "size_bytes": 1571,
    "line_count": 51,
    "preview": "---\ntitle: Pinecone Embeddings\n---\n\nPinecone's inference API can be accessed via `PineconeEmbeddings`. Providing text embeddings via the Pinecone service. We start by installing prerequisite libraries:\n\n```python\n!pip install -qU \"langchain-pinecone>=0.2.0\"\n```\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/minimax.mdx",
    "filename": "minimax.mdx",
    "size_bytes": 1120,
    "line_count": 51,
    "preview": "---\ntitle: MiniMax\n---\n\n[MiniMax](https://api.minimax.chat/document/guides/embeddings?id=6464722084cdc277dfaa966a) offers an embeddings service.\n\nThis example goes over how to use LangChain to interact with MiniMax Inference for text embedding.\n\n```python\nimport os\n"
  }
,
  {
    "path": "python/integrations/text_embedding/bookend.mdx",
    "filename": "bookend.mdx",
    "size_bytes": 466,
    "line_count": 29,
    "preview": "---\ntitle: Bookend AI\n---\n\nLet's load the Bookend AI Embeddings class.\n\n```python\nfrom langchain_community.embeddings import BookendEmbeddings\n```\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/openvino.mdx",
    "filename": "openvino.mdx",
    "size_bytes": 2992,
    "line_count": 107,
    "preview": "---\ntitle: OpenVINOEmbeddings\n---\n\n[OpenVINO™](https://github.com/openvinotoolkit/openvino) is an open-source toolkit for optimizing and deploying AI inference. The OpenVINO™ Runtime supports various hardware [devices](https://github.com/openvinotoolkit/openvino?tab=readme-ov-file#supported-hardware-matrix) including x86 and ARM CPUs, and Intel GPUs. It can help to boost deep learning performance in Computer Vision, Automatic Speech Recognition, Natural Language Processing and other common tasks.\n\nHugging Face embedding model can be supported by OpenVINO through `OpenVINOEmbeddings` class. If you have an Intel GPU, you can specify `model_kwargs={\"device\": \"GPU\"}` to run inference on it.\n\n```python\npip install -U-strategy eager \"optimum[openvino,nncf]\" --quiet\n"
  }
,
  {
    "path": "python/integrations/text_embedding/dashscope.mdx",
    "filename": "dashscope.mdx",
    "size_bytes": 483,
    "line_count": 29,
    "preview": "---\ntitle: DashScope\n---\n\nLet's load the DashScope Embedding class.\n\n```python\nfrom langchain_community.embeddings import DashScopeEmbeddings\n```\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/google_vertex_ai.mdx",
    "filename": "google_vertex_ai.mdx",
    "size_bytes": 6152,
    "line_count": 165,
    "preview": "---\ntitle: VertexAIEmbeddings\n---\n\n<Danger>\n    **Deprecated**\n\n    This integration is deprecated and will be removed in a future release. Please use [`GoogleGenerativeAIEmbeddings`](/oss/integrations/text_embedding/google_generative_ai) instead. See the full [release notes and migration guide](https://github.com/langchain-ai/langchain-google/discussions/1422).\n</Danger>\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/xinference.mdx",
    "filename": "xinference.mdx",
    "size_bytes": 1613,
    "line_count": 59,
    "preview": "---\ntitle: Xorbits inference (Xinference)\n---\n\nThis notebook goes over how to use Xinference embeddings within LangChain\n\n## Installation\n\nInstall `Xinference` through PyPI:\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/aleph_alpha.mdx",
    "filename": "aleph_alpha.mdx",
    "size_bytes": 1163,
    "line_count": 54,
    "preview": "---\ntitle: Aleph Alpha\n---\n\nThere are two possible ways to use Aleph Alpha's semantic embeddings. If you have texts with a dissimilar structure (e.g. a Document and a Query) you would want to use asymmetric embeddings. Conversely, for texts with comparable structures, symmetric embeddings are the suggested approach.\n\n## Asymmetric\n\n```python\nfrom langchain_community.embeddings import AlephAlphaAsymmetricSemanticEmbedding\n"
  }
,
  {
    "path": "python/integrations/text_embedding/bge_huggingface.mdx",
    "filename": "bge_huggingface.mdx",
    "size_bytes": 1147,
    "line_count": 38,
    "preview": "---\ntitle: BGE on Hugging Face\n---\n\n>[BGE models on the HuggingFace](https://huggingface.co/BAAI/bge-large-en-v1.5) are one of [the best open-source embedding models](https://huggingface.co/spaces/mteb/leaderboard).\n>BGE model is created by the [Beijing Academy of Artificial Intelligence (BAAI)](https://en.wikipedia.org/wiki/Beijing_Academy_of_Artificial_Intelligence). `BAAI` is a private non-profit organization engaged in AI research and development.\n\nThis notebook shows how to use `BGE Embeddings` through `Hugging Face`\n\n```python\n"
  }
,
  {
    "path": "python/integrations/text_embedding/llamacpp.mdx",
    "filename": "llamacpp.mdx",
    "size_bytes": 934,
    "line_count": 43,
    "preview": "---\ntitle: Llama.cpp\n---\n\n>[llama.cpp python](https://github.com/abetlen/llama-cpp-python) library is a simple Python bindings for `@ggerganov`\n>[llama.cpp](https://github.com/ggerganov/llama.cpp).\n>\n>This package provides:\n>\n> - Low-level access to C API via ctypes interface.\n"
  }
,
  {
    "path": "python/integrations/text_embedding/fake.mdx",
    "filename": "fake.mdx",
    "size_bytes": 372,
    "line_count": 21,
    "preview": "---\ntitle: FakeEmbeddings\n---\n\nLangChain also provides a fake embedding class. You can use this to test your pipelines.\n\n```python\nfrom langchain_community.embeddings import FakeEmbeddings\n```\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/anyscale.mdx",
    "filename": "anyscale.mdx",
    "size_bytes": 45902,
    "line_count": 37,
    "preview": "---\ntitle: Anyscale\n---\n\nLet's load the Anyscale Embedding class.\n\n```python\nfrom langchain_community.embeddings import AnyscaleEmbeddings\n```\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/ipex_llm_gpu.mdx",
    "filename": "ipex_llm_gpu.mdx",
    "size_bytes": 4368,
    "line_count": 97,
    "preview": "---\ntitle: IPEX-LLM - Local BGE Embeddings on Intel GPU\n---\n\n> [IPEX-LLM](https://github.com/intel-analytics/ipex-llm) is a PyTorch library for running LLM on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max) with very low latency.\n\nThis example goes over how to use LangChain to conduct embedding tasks with `ipex-llm` optimizations on Intel GPU. This would be helpful in applications such as RAG, document QA, etc.\n\n> **Note**\n>\n"
  }
,
  {
    "path": "python/integrations/text_embedding/infinity.mdx",
    "filename": "infinity.mdx",
    "size_bytes": 5665,
    "line_count": 164,
    "preview": "---\ntitle: Infinity\n---\n\n`Infinity` allows to create @[`Embeddings`] using a MIT-licensed Embedding Server.\n\nThis notebook goes over how to use LangChain with Embeddings with the [Infinity GitHub Project](https://github.com/michaelfeil/infinity).\n\n## Imports\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/yandex.mdx",
    "filename": "yandex.mdx",
    "size_bytes": 1781,
    "line_count": 68,
    "preview": "---\ntitle: YandexGPT\n---\n\nThis notebook goes over how to use LangChain with [YandexGPT](https://cloud.yandex.com/en/services/yandexgpt) embeddings models.\n\nTo use, you should have the `yandexcloud` python package installed.\n\n```python\npip install -qU  yandexcloud\n"
  }
,
  {
    "path": "python/integrations/text_embedding/modelscope_embedding.mdx",
    "filename": "modelscope_embedding.mdx",
    "size_bytes": 6356,
    "line_count": 150,
    "preview": "---\ntitle: ModelScope\n---\n\nModelScope ([Home](https://www.modelscope.cn/) | [GitHub](https://github.com/modelscope/modelscope)) is built upon the notion of “Model-as-a-Service” (MaaS). It seeks to bring together most advanced machine learning models from the AI community, and streamlines the process of leveraging AI models in real-world applications. The core ModelScope library open-sourced in this repository provides the interfaces and implementations that allow developers to perform model inference, training and evaluation.\n\nThis will help you get started with ModelScope embedding models using LangChain.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/text_embeddings_inference.mdx",
    "filename": "text_embeddings_inference.mdx",
    "size_bytes": 1730,
    "line_count": 60,
    "preview": "---\ntitle: Text Embeddings Inference\n---\n\n>[Hugging Face Text Embeddings Inference (TEI)](https://huggingface.co/docs/text-embeddings-inference/index) is a toolkit for deploying and serving open-source\n> text embeddings and sequence classification models. `TEI` enables high-performance extraction for the most popular models,\n>including `FlagEmbedding`, `Ember`, `GTE` and `E5`.\n\nTo use it within langchain, first install `huggingface-hub`.\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/johnsnowlabs_embedding.mdx",
    "filename": "johnsnowlabs_embedding.mdx",
    "size_bytes": 2191,
    "line_count": 54,
    "preview": "---\ntitle: John Snow Labs\n---\n\n>[John Snow Labs](https://nlp.johnsnowlabs.com/) NLP & LLM ecosystem includes software libraries for state-of-the-art AI at scale, Responsible AI, No-Code AI, and access to over 20,000 models for Healthcare, Legal, Finance, etc.\n>\n>Models are loaded with [nlp.load](https://nlp.johnsnowlabs.com/docs/en/jsl/load_api) and spark session is started >with [nlp.start()](https://nlp.johnsnowlabs.com/docs/en/jsl/start-a-sparksession) under the hood.\n>For all 24.000+ models, see the [John Snow Labs Model Models Hub](https://nlp.johnsnowlabs.com/models)\n\n## Setting up\n"
  }
,
  {
    "path": "python/integrations/text_embedding/volcengine.mdx",
    "filename": "volcengine.mdx",
    "size_bytes": 1496,
    "line_count": 53,
    "preview": "---\ntitle: Volc Engine\n---\n\nThis guide provides you with a guide on how to load the Volcano Embedding class.\n\n## API initialization\n\nTo use the LLM services based on [VolcEngine](https://www.volcengine.com/docs/82379/1099455), you have to initialize these parameters:\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/itrex.mdx",
    "filename": "itrex.mdx",
    "size_bytes": 2029,
    "line_count": 37,
    "preview": "---\ntitle: Intel® Extension for Transformers Quantized Text Embeddings\n---\n\nLoad quantized BGE embedding models generated by [Intel® Extension for Transformers](https://github.com/intel/intel-extension-for-transformers) (ITREX) and use ITREX [Neural Engine](https://github.com/intel/intel-extension-for-transformers/blob/main/intel_extension_for_transformers/llm/runtime/deprecated/docs/Installation.md), a high-performance NLP backend, to accelerate the inference of models without compromising accuracy.\n\nRefer to our blog of [Efficient Natural Language Embedding Models with Intel Extension for Transformers](https://medium.com/intel-analytics-software/efficient-natural-language-embedding-models-with-intel-extension-for-transformers-2b6fcd0f8f34) and [BGE optimization example](https://github.com/intel/intel-extension-for-transformers/tree/main/examples/huggingface/pytorch/text-embedding/deployment/mteb/bge) for more details.\n\n```python\nfrom langchain_community.embeddings import QuantizedBgeEmbeddings\n"
  }
,
  {
    "path": "python/integrations/text_embedding/deepinfra.mdx",
    "filename": "deepinfra.mdx",
    "size_bytes": 1695,
    "line_count": 62,
    "preview": "---\ntitle: DeepInfra\n---\n\n[DeepInfra](https://deepinfra.com/?utm_source=langchain) is a serverless inference as a service that provides access to a [variety of LLMs](https://deepinfra.com/models?utm_source=langchain) and [embeddings models](https://deepinfra.com/models?type=embeddings&utm_source=langchain). This notebook goes over how to use LangChain with DeepInfra for text embeddings.\n\n```python\n# sign up for an account: https://deepinfra.com/login?utm_source=langchain\n\nfrom getpass import getpass\n"
  }
,
  {
    "path": "python/integrations/text_embedding/jina.mdx",
    "filename": "jina.mdx",
    "size_bytes": 1641,
    "line_count": 93,
    "preview": "---\ntitle: Jina\n---\n\nYou can check the list of available models from [here](https://jina.ai/embeddings/).\n\n## Installation and setup\n\nInstall requirements\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/ernie.mdx",
    "filename": "ernie.mdx",
    "size_bytes": 1336,
    "line_count": 47,
    "preview": "---\ntitle: ERNIE\n---\n\n[ERNIE Embedding-V1](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/alj562vvu) is a text representation model based on `Baidu Wenxin` large-scale model technology,\nwhich converts text into a vector form represented by numerical values, and is used in text retrieval, information recommendation, knowledge mining and other scenarios.\n\n**Deprecated Warning**\n\nWe recommend users using `langchain_community.embeddings.ErnieEmbeddings`\n"
  }
,
  {
    "path": "python/integrations/text_embedding/together.mdx",
    "filename": "together.mdx",
    "size_bytes": 4225,
    "line_count": 128,
    "preview": "---\ntitle: TogetherEmbeddings\n---\n\nThis will help you get started with Together embedding models using LangChain. For detailed documentation on `TogetherEmbeddings` features and configuration options, please refer to the [API reference](https://python.langchain.com/api_reference/together/embeddings/langchain_together.embeddings.TogetherEmbeddings.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/greennode.mdx",
    "filename": "greennode.mdx",
    "size_bytes": 6815,
    "line_count": 203,
    "preview": "---\ntitle: GreenNodeEmbeddings\n---\n\n>[GreenNode](https://greennode.ai/) is a global AI solutions provider and a **NVIDIA Preferred Partner**, delivering full-stack AI capabilities—from infrastructure to application—for enterprises across the US, MENA, and APAC regions. Operating on **world-class infrastructure** (LEED Gold, TIA‑942, Uptime Tier III), GreenNode empowers enterprises, startups, and researchers with a comprehensive suite of AI services\n\nThis guide provides a guide to getting started with `GreenNodeEmbeddings`. It enables you to perform semantic document search using various built-in connectors or your own custom data sources by generating high-quality vector representations of text.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/self-hosted.mdx",
    "filename": "self-hosted.mdx",
    "size_bytes": 2045,
    "line_count": 85,
    "preview": "---\ntitle: Self Hosted\n---\n\nLet's load the `SelfHostedEmbeddings`, `SelfHostedHuggingFaceEmbeddings`, and `SelfHostedHuggingFaceInstructEmbeddings` classes.\n\n```python\nimport runhouse as rh\nfrom langchain_community.embeddings import (\n    SelfHostedEmbeddings,\n"
  }
,
  {
    "path": "python/integrations/text_embedding/cohere.mdx",
    "filename": "cohere.mdx",
    "size_bytes": 4134,
    "line_count": 128,
    "preview": "---\ntitle: CohereEmbeddings\n---\n\nThis will help you get started with Cohere embedding models using LangChain. For detailed documentation on `CohereEmbeddings` features and configuration options, please refer to the [API reference](https://python.langchain.com/api_reference/cohere/embeddings/langchain_cohere.embeddings.CohereEmbeddings.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/ollama.mdx",
    "filename": "ollama.mdx",
    "size_bytes": 5033,
    "line_count": 132,
    "preview": "---\ntitle: OllamaEmbeddings\n---\n\nThis will help you get started with Ollama embedding models using LangChain. For detailed documentation on `OllamaEmbeddings` features and configuration options, please refer to the [API reference](https://python.langchain.com/api_reference/ollama/embeddings/langchain_ollama.embeddings.OllamaEmbeddings.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/embaas.mdx",
    "filename": "embaas.mdx",
    "size_bytes": 1777,
    "line_count": 61,
    "preview": "---\ntitle: Embaas\n---\n\n[embaas](https://embaas.io) is a fully managed NLP API service that offers features like embedding generation, document text extraction, document to embeddings and more. You can choose a [variety of pre-trained models](https://embaas.io/docs/models/embeddings).\n\nIn this tutorial, we will show you how to use the embaas Embeddings API to generate embeddings for a given text.\n\n### Prerequisites\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/netmind.mdx",
    "filename": "netmind.mdx",
    "size_bytes": 4276,
    "line_count": 134,
    "preview": "---\ntitle: Netmind\n---\n\nThis will help you get started with Netmind embedding models using LangChain. For detailed documentation on `NetmindEmbeddings` features and configuration options, please refer to the [API reference](https://python.langchain.com/api_reference/).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/titan_takeoff.mdx",
    "filename": "titan_takeoff.mdx",
    "size_bytes": 2502,
    "line_count": 56,
    "preview": "---\ntitle: Titan Takeoff\n---\n\n`TitanML` helps businesses build and deploy better, smaller, cheaper, and faster NLP models through our training, compression, and inference optimization platform.\n\nOur inference server, [Titan Takeoff](https://docs.titanml.co/docs/intro) enables deployment of LLMs locally on your hardware in a single command. Most embedding models are supported out of the box, if you experience trouble with a specific model, please let us know at [hello@titanml.co](mailto:hello@titanml.co).\n\n## Example usage\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/open_clip.mdx",
    "filename": "open_clip.mdx",
    "size_bytes": 4003195,
    "line_count": 168,
    "preview": "---\ntitle: OpenClip\n---\n\n[OpenClip](https://github.com/mlfoundations/open_clip/tree/main) is an source implementation of OpenAI's CLIP.\n\nThese multi-modal embeddings can be used to embed images or text.\n\n```python\npip install -qU  langchain-experimental\n"
  }
,
  {
    "path": "python/integrations/text_embedding/sentence_transformers.mdx",
    "filename": "sentence_transformers.mdx",
    "size_bytes": 1498,
    "line_count": 54,
    "preview": "---\ntitle: Sentence Transformers on Hugging Face\n---\n\n>[Hugging Face sentence-transformers](https://huggingface.co/sentence-transformers) is a Python framework for state-of-the-art sentence, text and image embeddings.\n>You can use these embedding models from the `HuggingFaceEmbeddings` class.\n\n<Warning>\n**Running sentence-transformers locally can be affected by your operating system and other global factors. It is recommended for experienced users only.**\n\n"
  }
,
  {
    "path": "python/integrations/text_embedding/clarifai.mdx",
    "filename": "clarifai.mdx",
    "size_bytes": 2850,
    "line_count": 90,
    "preview": "---\ntitle: Clarifai\n---\n\n>[Clarifai](https://www.clarifai.com/) is an AI Platform that provides the full AI lifecycle ranging from data exploration, data labeling, model training, evaluation, and inference.\n\nThis example goes over how to use LangChain to interact with `Clarifai` [models](https://clarifai.com/explore/models). Text embedding models in particular can be found [here](https://clarifai.com/explore/models?page=1&perPage=24&filterData=%5B%7B%22field%22%3A%22model_type_id%22%2C%22value%22%3A%5B%22text-embedder%22%5D%7D%5D).\n\nTo use Clarifai, you must have an account and a Personal Access Token (PAT) key.\n[Check here](https://clarifai.com/settings/security) to get or create a PAT.\n"
  }
,
  {
    "path": "python/integrations/text_embedding/solar.mdx",
    "filename": "solar.mdx",
    "size_bytes": 49011,
    "line_count": 2062,
    "preview": "---\ntitle: Solar\n---\n\n[Solar](https://console.upstage.ai/services/embedding) offers an embeddings service.\n\nThis example goes over how to use LangChain to interact with Solar Inference for text embedding.\n\n```python\nimport os\n"
  }
,
  {
    "path": "python/integrations/text_embedding/nvidia_ai_endpoints.mdx",
    "filename": "nvidia_ai_endpoints.mdx",
    "size_bytes": 11927,
    "line_count": 309,
    "preview": "---\ntitle: NVIDIAEmbeddings\n---\n\nThe `langchain-nvidia-ai-endpoints` package contains LangChain integrations for chat models and embeddings powered by [NVIDIA AI Foundation Models](https://www.nvidia.com/en-us/ai-data-science/foundation-models/), and hosted on the [NVIDIA API Catalog](https://build.nvidia.com/).\n\nNVIDIA AI Foundation models are community- and NVIDIA-built models that are optimized to deliver the best performance on NVIDIA-accelerated infrastructure. You can use the API to query live endpoints that are available on the NVIDIA API Catalog to get quick results from a DGX-hosted cloud compute environment, or you can download models from NVIDIA's API catalog with NVIDIA NIM, which is included with the NVIDIA AI Enterprise license. The ability to run models on-premises gives your enterprise ownership of your customizations and full control of your IP and AI application.\n\nNIM microservices are packaged as container images on a per model/model family basis and are distributed as NGC container images through the [NVIDIA NGC Catalog](https://catalog.ngc.nvidia.com/). At their core, NIM microservices are containers that provide interactive APIs for running inference on an AI Model.\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/dappier.mdx",
    "filename": "dappier.mdx",
    "size_bytes": 6954,
    "line_count": 88,
    "preview": "---\ntitle: Dappier\n---\n\n[Dappier](https://dappier.com) connects any LLM or your Agentic AI to real-time, rights-cleared, proprietary data from trusted sources, making your AI an expert in anything. Our specialized models include Real-Time Web Search, News, Sports, Financial Stock Market Data, Crypto Data, and exclusive content from premium publishers. Explore a wide range of data models in our marketplace at [marketplace.dappier.com](https://marketplace.dappier.com).\n\n[Dappier](https://dappier.com) delivers enriched, prompt-ready, and contextually relevant data strings, optimized for seamless integration with LangChain. Whether you're building conversational AI, recommendation engines, or intelligent search, Dappier's LLM-agnostic RAG models ensure your AI has access to verified, up-to-date data—without the complexity of building and managing your own retrieval pipeline.\n\n# DappierRetriever\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/zep_memorystore.mdx",
    "filename": "zep_memorystore.mdx",
    "size_bytes": 25290,
    "line_count": 308,
    "preview": "---\ntitle: Zep Open Source\n---\n\n## Retriever example for [Zep](https://docs.getzep.com/)\n\n> Recall, understand, and extract data from chat histories. Power personalized AI experiences.\n\n> [Zep](https://www.getzep.com) is a long-term memory service for AI Assistant apps.\n> With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant,\n"
  }
,
  {
    "path": "python/integrations/retrievers/google_drive.mdx",
    "filename": "google_drive.mdx",
    "size_bytes": 5644,
    "line_count": 136,
    "preview": "---\ntitle: Google Drive\n---\n\nThis notebook covers how to retrieve documents from `Google Drive`.\n\n## Prerequisites\n\n1. Create a Google Cloud project or use an existing project\n1. Enable the [Google Drive API](https://console.cloud.google.com/flows/enableapi?apiid=drive.googleapis.com)\n"
  }
,
  {
    "path": "python/integrations/retrievers/elastic_search_bm25.mdx",
    "filename": "elastic_search_bm25.mdx",
    "size_bytes": 2525,
    "line_count": 74,
    "preview": "---\ntitle: ElasticSearch BM25\n---\n\n>[Elasticsearch](https://www.elastic.co/elasticsearch/) is a distributed, RESTful search and analytics engine. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents.\n\n>In information retrieval, [Okapi BM25](https://en.wikipedia.org/wiki/Okapi_BM25) (BM is an abbreviation of best matching) is a ranking function used by search engines to estimate the relevance of documents to a given search query. It is based on the probabilistic retrieval framework developed in the 1970s and 1980s by Stephen E. Robertson, Karen Spärck Jones, and others.\n\n>The name of the actual ranking function is BM25. The fuller name, Okapi BM25, includes the name of the first system to use it, which was the Okapi information retrieval system, implemented at London's City University in the 1980s and 1990s. BM25 and its newer variants, e.g. BM25F (a version of BM25 that can take document structure and anchor text into account), represent TF-IDF-like retrieval functions used in document retrieval.\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/perigon.mdx",
    "filename": "perigon.mdx",
    "size_bytes": 10275,
    "line_count": 327,
    "preview": "---\ntitle: Perigon\n---\n\nThe Perigon API suite provides fast, structured access to global news and events, helping you build real-time, data-driven products. Whether you're tracking emerging risks, surfacing relevant articles, or uncovering key insights, Perigon gives you the tools to do it programmatically.\n\nUnlike traditional keyword-based search, Perigon's semantic search capabilities allow it to understand queries contextually and return relevant documents.\n\nThis notebook demonstrates how to use Perigon's retrievers with LangChain for both news articles and Wikipedia content.\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/pinecone_rerank.mdx",
    "filename": "pinecone_rerank.mdx",
    "size_bytes": 4905,
    "line_count": 174,
    "preview": "---\ntitle: Pinecone Rerank\n---\n\n> This notebook shows how to use **PineconeRerank** for two-stage vector retrieval reranking using Pinecone's hosted reranking API as demonstrated in `langchain_pinecone/libs/pinecone/rerank.py`.\n\n## Setup\n\nInstall the `langchain-pinecone` package.\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/you-retriever.mdx",
    "filename": "you-retriever.mdx",
    "size_bytes": 31684,
    "line_count": 233,
    "preview": "---\ntitle: You.com\n---\n\n>[you.com API](https://api.you.com) is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.\n\n## Setup\n\nThe retriever lives in the `langchain-community` package.\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/nebius.mdx",
    "filename": "nebius.mdx",
    "size_bytes": 8304,
    "line_count": 309,
    "preview": "---\ntitle: Nebius\n---\n\nThe `NebiusRetriever` enables efficient similarity search using embeddings from [Nebius AI Studio](https://studio.nebius.ai/). It leverages high-quality embedding models to enable semantic search over documents.\n\nThis retriever is optimized for scenarios where you need to perform similarity search over a collection of documents, but don't need to persist the vectors to a vector database. It performs vector similarity search in-memory using matrix operations, making it efficient for medium-sized document collections.\n\n## Setup\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/knn.mdx",
    "filename": "knn.mdx",
    "size_bytes": 1120,
    "line_count": 41,
    "preview": "---\ntitle: kNN\n---\n\n>In statistics, the [k-nearest neighbours algorithm (k-NN)](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) is a non-parametric supervised learning method first developed by `Evelyn Fix` and `Joseph Hodges` in 1951, and later expanded by `Thomas Cover`. It is used for classification and regression.\n\nThis notebook goes over how to use a retriever that under the hood uses a kNN.\n\nLargely based on the code of [Andrej Karpathy](https://github.com/karpathy/randomfun/blob/master/knn_vs_svm.html).\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/zotero.mdx",
    "filename": "zotero.mdx",
    "size_bytes": 6391,
    "line_count": 147,
    "preview": "---\ntitle: Zotero\n---\n\nThis will help you get started with the Zotero [retriever](/oss/langchain/retrieval). For detailed documentation of all ZoteroRetriever features and configurations head to the [GitHub page](https://github.com/TimBMK/langchain-zotero-retriever).\n\n### Integration details\n\n| Retriever | Source | Package |\n| :--- | :--- | :---: |\n"
  }
,
  {
    "path": "python/integrations/retrievers/valyu.mdx",
    "filename": "valyu.mdx",
    "size_bytes": 3156,
    "line_count": 119,
    "preview": "---\ntitle: ValyuContext\n---\n\n>[Valyu](https://www.valyu.network/) allows AI applications and agents to search the internet and proprietary data sources for relevant LLM ready information.\n\nThis notebook goes over how to use Valyu deep search tool in LangChain.\n\nFirst, get an Valyu API key and add it as an environment variable. Get $10 free credit  by [signing up here](https://platform.valyu.network/).\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/wikipedia.mdx",
    "filename": "wikipedia.mdx",
    "size_bytes": 2861,
    "line_count": 66,
    "preview": "---\ntitle: Wikipedia\n---\n\n>[Wikipedia](https://wikipedia.org/) is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. `Wikipedia` is the largest and most-read reference work in history.\n\nThis notebook shows how to retrieve wiki pages from `wikipedia.org` into the [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) format that is used downstream.\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/cognee.mdx",
    "filename": "cognee.mdx",
    "size_bytes": 2172,
    "line_count": 78,
    "preview": "---\ntitle: Cognee\n---\n\n# CogneeRetriever\n\nThis will help you get started with the Cognee [retriever](/oss/langchain/retrieval). For detailed documentation of all CogneeRetriever features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.cognee.CogneeRetriever.html).\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/activeloop.mdx",
    "filename": "activeloop.mdx",
    "size_bytes": 14306,
    "line_count": 406,
    "preview": "---\ntitle: Activeloop Deep Memory\n---\n\n>[Activeloop Deep Memory](https://docs.activeloop.ai/performance-features/deep-memory) is a suite of tools that enables you to optimize your Vector Store for your use-case and achieve higher accuracy in your LLM apps.\n\n`Retrieval-Augmented Generatation` (`RAG`) has recently gained significant attention. As advanced RAG techniques and agents emerge, they expand the potential of what RAGs can accomplish. However, several challenges may limit the integration of RAGs into production. The primary factors to consider when implementing RAGs in production settings are accuracy (recall), cost, and latency. For basic use cases, OpenAI's Ada model paired with a naive similarity search can produce satisfactory results. Yet, for higher accuracy or recall during searches, one might need to employ advanced retrieval techniques. These methods might involve varying data chunk sizes, rewriting queries multiple times, and more, potentially increasing latency and costs.  Activeloop's [Deep Memory](https://www.activeloop.ai/resources/use-deep-memory-to-boost-rag-apps-accuracy-by-up-to-22/) a feature available to `Activeloop Deep Lake` users, addresses these issuea by introducing a tiny neural network layer trained to match user queries with relevant data from a corpus. While this addition incurs minimal latency during search, it can boost retrieval accuracy by up to 27\n% and remains cost-effective and simple to use, without requiring any additional advanced rag techniques.\n\nFor this tutorial we will parse `DeepLake` documentation, and create a RAG system that could answer the question from the docs.\n"
  }
,
  {
    "path": "python/integrations/retrievers/svm.mdx",
    "filename": "svm.mdx",
    "size_bytes": 1446,
    "line_count": 67,
    "preview": "---\ntitle: SVM\n---\n\n>[Support vector machines (SVMs)](https://scikit-learn.org/stable/modules/svm.html#support-vector-machines) are a set of supervised learning methods used for classification, regression and outliers detection.\n\nThis notebook goes over how to use a retriever that under the hood uses an `SVM` using `scikit-learn` package.\n\nLargely based on [github.com/karpathy/randomfun/blob/master/knn_vs_svm.html](https://github.com/karpathy/randomfun/blob/master/knn_vs_svm.html)\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 10819,
    "line_count": 106,
    "preview": "---\ntitle: \"Retrievers\"\n---\n\nA [retriever](/oss/langchain/retrieval#building-blocks) is an interface that returns documents given an unstructured query.\nIt is more general than a vector store.\nA retriever does not need to be able to store documents, only to return (or retrieve) them.\nRetrievers can be created from vector stores, but are also broad enough to include [Wikipedia search](/oss/integrations/retrievers/wikipedia/) and [Amazon Kendra](/oss/integrations/retrievers/amazon_kendra_retriever/).\n\nRetrievers accept a string query as input and return a list of @[`Document`] objects as output.\n"
  }
,
  {
    "path": "python/integrations/retrievers/pinecone_hybrid_search.mdx",
    "filename": "pinecone_hybrid_search.mdx",
    "size_bytes": 3550,
    "line_count": 152,
    "preview": "---\ntitle: Pinecone Hybrid Search\n---\n\n>[Pinecone](https://docs.pinecone.io/docs/overview) is a vector database with broad functionality.\n\nThis notebook goes over how to use a retriever that under the hood uses Pinecone and Hybrid Search.\n\nThe logic of this retriever is taken from [this documentation](https://docs.pinecone.io/docs/hybrid-search)\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/qdrant-sparse.mdx",
    "filename": "qdrant-sparse.mdx",
    "size_bytes": 5947,
    "line_count": 135,
    "preview": "---\ntitle: Qdrant Sparse Vector\n---\n\n>[Qdrant](https://qdrant.tech/) is an open-source, high-performance vector search engine/database.\n\n>`QdrantSparseVectorRetriever` uses [sparse vectors](https://qdrant.tech/articles/sparse-vectors/) introduced in `Qdrant` [v1.7.0](https://qdrant.tech/articles/qdrant-1.7.x/) for document retrieval.\n\nInstall the 'qdrant_client' package:\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/needle.mdx",
    "filename": "needle.mdx",
    "size_bytes": 10567,
    "line_count": 138,
    "preview": "---\ntitle: Needle\n---\n\n[Needle](https://needle-ai.com) makes it easy to create your RAG pipelines with minimal effort.\n\nFor more details, refer to our [API documentation](https://docs.needle-ai.com/docs/api-reference/needle-api)\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/metal.mdx",
    "filename": "metal.mdx",
    "size_bytes": 1386,
    "line_count": 63,
    "preview": "---\ntitle: Metal\n---\n\n>[Metal](https://github.com/getmetal/metal-python) is a managed service for ML Embeddings.\n\nThis notebook shows how to use [Metal's](https://docs.getmetal.io/introduction) retriever.\n\nFirst, you will need to sign up for Metal and get an API key. You can do so [here](https://docs.getmetal.io/misc-create-app)\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/zep_cloud_memorystore.mdx",
    "filename": "zep_cloud_memorystore.mdx",
    "size_bytes": 14557,
    "line_count": 309,
    "preview": "---\ntitle: Zep Cloud\n---\n\n## Retriever example for [Zep cloud](https://docs.getzep.com/)\n\n> Recall, understand, and extract data from chat histories. Power personalized AI experiences.\n\n> [Zep](https://www.getzep.com) is a long-term memory service for AI Assistant apps.\n> With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant,\n"
  }
,
  {
    "path": "python/integrations/retrievers/vectorize.mdx",
    "filename": "vectorize.mdx",
    "size_bytes": 5091,
    "line_count": 178,
    "preview": "---\ntitle: VectorizeRetriever\n---\n\nThis notebook shows how to use the LangChain Vectorize retriever.\n\n> [Vectorize](https://vectorize.io/) helps you build AI apps faster and with less hassle.\n> It automates data extraction, finds the best vectorization strategy using RAG evaluation,\n> and lets you quickly deploy real-time RAG pipelines for your unstructured data.\n> Your vector search indexes stay up-to-date, and it integrates with your existing vector database,\n"
  }
,
  {
    "path": "python/integrations/retrievers/bedrock.mdx",
    "filename": "bedrock.mdx",
    "size_bytes": 3074,
    "line_count": 79,
    "preview": "---\ntitle: Bedrock (Knowledge Bases)\n---\n\nThis guide will help you get started with the AWS Knowledge Bases [retriever](/oss/langchain/retrieval).\n\n[Knowledge Bases for Amazon Bedrock](https://aws.amazon.com/bedrock/knowledge-bases/) is an Amazon Web Services (AWS) offering which lets you quickly build RAG applications by using your private data to customize FM response.\n\nImplementing `RAG` requires organizations to perform several cumbersome steps to convert data into embeddings (vectors), store the embeddings in a specialized vector database, and build custom integrations into the database to search and retrieve text relevant to the user’s query. This can be time-consuming and inefficient.\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/dria_index.mdx",
    "filename": "dria_index.mdx",
    "size_bytes": 1613,
    "line_count": 73,
    "preview": "---\ntitle: Dria\n---\n\n>[Dria](https://dria.co/) is a hub of public RAG models for developers to both contribute and utilize a shared embedding lake. This notebook demonstrates how to use the `Dria API` for data retrieval tasks.\n\n# Installation\n\nEnsure you have the `dria` package installed. You can install it using pip:\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/linkup_search.mdx",
    "filename": "linkup_search.mdx",
    "size_bytes": 8646,
    "line_count": 79,
    "preview": "---\ntitle: LinkupSearchRetriever\n---\n\n> [Linkup](https://www.linkup.so/) provides an API to connect LLMs to the web and the Linkup Premium Partner sources.\n\nThis will help you get started with the LinkupSearchRetriever [retriever](/oss/langchain/retrieval/). For detailed documentation of all LinkupSearchRetriever features and configurations head to the [API reference](https://python.langchain.com/api_reference/linkup/retrievers/linkup_langchain.search_retriever.LinkupSearchRetriever.html).\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/nanopq.mdx",
    "filename": "nanopq.mdx",
    "size_bytes": 1684,
    "line_count": 55,
    "preview": "---\ntitle: NanoPQ (Product Quantization)\n---\n\n>[Product Quantization algorithm (k-NN)](https://towardsdatascience.com/similarity-search-product-quantization-b2a1a6397701) in brief is a quantization algorithm that helps in compression of database vectors which helps in semantic search when large datasets are involved. In a nutshell, the embedding is split into M subspaces which further goes through clustering. Upon clustering the vectors the centroid vector gets mapped to the vectors present in the each of the clusters of the subspace.\n\nThis notebook goes over how to use a retriever that under the hood uses a Product Quantization which has been implemented by the [nanopq](https://github.com/matsui528/nanopq) package.\n\n```python\npip install -qU langchain-community langchain-openai nanopq\n"
  }
,
  {
    "path": "python/integrations/retrievers/flashrank-reranker.mdx",
    "filename": "flashrank-reranker.mdx",
    "size_bytes": 15058,
    "line_count": 348,
    "preview": "---\ntitle: FlashRank reranker\n---\n\n>[FlashRank](https://github.com/PrithivirajDamodaran/FlashRank) is the Ultra-lite & Super-fast Python library to add re-ranking to your existing search & retrieval pipelines. It is based on SoTA cross-encoders, with gratitude to all the model owners.\n\nThis notebook shows how to use [flashrank](https://github.com/PrithivirajDamodaran/FlashRank) for document compression and retrieval.\n\n```python\npip install -qU  flashrank\n"
  }
,
  {
    "path": "python/integrations/retrievers/thirdai_neuraldb.mdx",
    "filename": "thirdai_neuraldb.mdx",
    "size_bytes": 3721,
    "line_count": 98,
    "preview": "---\ntitle: NeuralDB\n---\n\nNeuralDB is a CPU-friendly and fine-tunable retrieval engine developed by ThirdAI.\n\n### **Initialization**\n\nThere are two initialization methods:\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/pubmed.mdx",
    "filename": "pubmed.mdx",
    "size_bytes": 3358,
    "line_count": 29,
    "preview": "---\ntitle: PubMed\n---\n\n>[PubMed®](https://pubmed.ncbi.nlm.nih.gov/) by `The National Center for Biotechnology Information, National Library of Medicine` comprises more than 35 million citations for biomedical literature from `MEDLINE`, life science journals, and online books. Citations may include links to full text content from `PubMed Central` and publisher web sites.\n\nThis notebook goes over how to use `PubMed` as a retriever\n\n```python\nfrom langchain_community.retrievers import PubMedRetriever\n"
  }
,
  {
    "path": "python/integrations/retrievers/chatgpt-plugin.mdx",
    "filename": "chatgpt-plugin.mdx",
    "size_bytes": 3362,
    "line_count": 92,
    "preview": "---\ntitle: ChatGPT plugin\n---\n\n>[OpenAI plugins](https://platform.openai.com/docs/plugins/introduction) connect `ChatGPT` to third-party applications. These plugins enable `ChatGPT` to interact with APIs defined by developers, enhancing `ChatGPT's` capabilities and allowing it to perform a wide range of actions.\n\n>Plugins allow `ChatGPT` to do things like:\n>\n>- Retrieve real-time information; e.g., sports scores, stock prices, the latest news, etc.\n>- Retrieve knowledge-base information; e.g., company docs, personal notes, etc.\n"
  }
,
  {
    "path": "python/integrations/retrievers/jaguar.mdx",
    "filename": "jaguar.mdx",
    "size_bytes": 5240,
    "line_count": 190,
    "preview": "---\ntitle: JaguarDB Vector Database\n---\n\n>[JaguarDB Vector Database]([www.jaguardb.com/windex.html](http://www.jaguardb.com/windex.html)\n>\n>1. It is a distributed vector database\n>2. The “ZeroMove” feature of JaguarDB enables instant horizontal scalability\n>3. Multimodal: embeddings, text, images, videos, PDFs, audio, time series, and geospatial\n>4. All-masters: allows both parallel reads and writes\n"
  }
,
  {
    "path": "python/integrations/retrievers/box.mdx",
    "filename": "box.mdx",
    "size_bytes": 10015,
    "line_count": 246,
    "preview": "---\ntitle: BoxRetriever\n---\n\nThis will help you get started with the Box [retriever](/oss/langchain/retrieval). For detailed documentation of all BoxRetriever features and configurations head to the [API reference](https://python.langchain.com/api_reference/box/retrievers/langchain_box.retrievers.box.BoxRetriever.html).\n\n# Overview\n\nThe `BoxRetriever` class helps you get your unstructured content from Box in LangChain's @[`Document`] format. You can do this by searching for files based on a full-text search or using Box AI to retrieve a @[`Document`] containing the result of an AI query against files. This requires including a `List[str]` containing Box file ids, i.e. `[\"12345\",\"67890\"]`\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/imap.mdx",
    "filename": "imap.mdx",
    "size_bytes": 6902,
    "line_count": 276,
    "preview": "---\ntitle: Imap\n---\n\n# ImapRetriever\n\nThis guide will help you get started with the IMAP [retriever](/oss/integrations/retrievers). The `ImapRetriever` enables search and retrieval of emails from IMAP servers as LangChain `Document` objects.\n\n## Integration details\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/azure_ai_search.mdx",
    "filename": "azure_ai_search.mdx",
    "size_bytes": 8549,
    "line_count": 199,
    "preview": "---\ntitle: Azure AI Search\n---\n\n[Azure AI Search](https://learn.microsoft.com/azure/search/search-what-is-azure-search) (formerly known as `Azure Cognitive Search`) is a Microsoft cloud search service that gives developers infrastructure, APIs, and tools for information retrieval of vector, keyword, and hybrid queries at scale.\n\n`AzureAISearchRetriever` is an integration module that returns documents from an unstructured query. It's based on the BaseRetriever class and it targets the 2023-11-01 stable REST API version of Azure AI Search, which means it supports vector indexing and queries.\n\nThis guide will help you get started with the Azure AI Search [retriever](/oss/langchain/retrieval). For detailed documentation of all `AzureAISearchRetriever` features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.azure_ai_search.AzureAISearchRetriever.html).\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/tf_idf.mdx",
    "filename": "tf_idf.mdx",
    "size_bytes": 1978,
    "line_count": 87,
    "preview": "---\ntitle: TF-IDF\n---\n\n>[TF-IDF](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting) means term-frequency times inverse document-frequency.\n\nThis notebook goes over how to use a retriever that under the hood uses [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) using `scikit-learn` package.\n\nFor more information on the details of TF-IDF see [this blog post](https://medium.com/data-science-bootcamp/tf-idf-basics-of-information-retrieval-48de122b2a4c).\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/arxiv.mdx",
    "filename": "arxiv.mdx",
    "size_bytes": 3897,
    "line_count": 99,
    "preview": "---\ntitle: Arxiv\n---\n\n>[arXiv](https://arxiv.org/) is an open-access archive for 2 million scholarly articles in the fields of physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems science, and economics.\n\nThis notebook shows how to retrieve scientific articles from Arxiv.org into the [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) format that is used downstream.\n\nFor detailed documentation of all `ArxivRetriever` features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.arxiv.ArxivRetriever.html).\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/fleet_context.mdx",
    "filename": "fleet_context.mdx",
    "size_bytes": 6138,
    "line_count": 171,
    "preview": "---\ntitle: Fleet AI Context\n---\n\n>[Fleet AI Context](https://www.fleet.so/context) is a dataset of high-quality embeddings of the top 1200 most popular & permissive Python Libraries & their documentation.\n>\n>The `Fleet AI` team is on a mission to embed the world's most important data. They've started by embedding the top 1200 Python libraries to enable code generation with up-to-date knowledge.\n\nLet's take a look at how we can use these embeddings to power a docs retrieval system and ultimately a simple code-generating chain!\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/rememberizer.mdx",
    "filename": "rememberizer.mdx",
    "size_bytes": 4338,
    "line_count": 114,
    "preview": "---\ntitle: Rememberizer\n---\n\n>[Rememberizer](https://rememberizer.ai/) is a knowledge enhancement service for AI applications created by  SkyDeck AI Inc.\n\nThis notebook shows how to retrieve documents from `Rememberizer` into the Document format that is used downstream.\n\n# Preparation\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/ragatouille.mdx",
    "filename": "ragatouille.mdx",
    "size_bytes": 16733,
    "line_count": 295,
    "preview": "---\ntitle: RAGatouille\n---\n\n>[RAGatouille](https://github.com/bclavie/RAGatouille) makes it as simple as can be to use `ColBERT`!\n>\n>[ColBERT](https://github.com/stanford-futuredata/ColBERT) is a fast and accurate retrieval model, enabling scalable BERT-based search over large text collections in tens of milliseconds.\n>\n>See the [ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction](https://arxiv.org/abs/2112.01488) paper.\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/docarray_retriever.mdx",
    "filename": "docarray_retriever.mdx",
    "size_bytes": 14605,
    "line_count": 470,
    "preview": "---\ntitle: DocArray\n---\n\n>[DocArray](https://github.com/docarray/docarray) is a versatile, open-source tool for managing your multi-modal data. It lets you shape your data however you want, and offers the flexibility to store and search it using various document index backends. Plus, it gets even better - you can utilize your `DocArray` document index to create a `DocArrayRetriever`, and build awesome LangChain apps!\n\nThis notebook is split into two sections. The [first section](#document-index-backends) offers an introduction to all five supported document index backends. It provides guidance on setting up and indexing each backend and also instructs you on how to build a `DocArrayRetriever` for finding relevant documents.\nIn the [second section](#movie-retrieval-using-hnswdocumentindex), we'll select one of these backends and illustrate how to use it through a basic example.\n\n## Document index backends\n"
  }
,
  {
    "path": "python/integrations/retrievers/llmlingua.mdx",
    "filename": "llmlingua.mdx",
    "size_bytes": 14526,
    "line_count": 315,
    "preview": "---\ntitle: LLMLingua Document Compressor\n---\n\n>[LLMLingua](https://github.com/microsoft/LLMLingua) utilizes a compact, well-trained language model (e.g., GPT2-small, LLaMA-7B) to identify and remove non-essential tokens in prompts. This approach enables efficient inference with large language models (LLMs), achieving up to 20x compression with minimal performance loss.\n\nThis notebook shows how to use LLMLingua as a document compressor.\n\n```python\npip install -qU  llmlingua accelerate\n"
  }
,
  {
    "path": "python/integrations/retrievers/galaxia-retriever.mdx",
    "filename": "galaxia-retriever.mdx",
    "size_bytes": 2758,
    "line_count": 102,
    "preview": "---\ntitle: Galaxia\n---\n\nGalaxia is GraphRAG solution, which automates document processing, knowledge base (Graph Language Model) creation and retrieval:\n[galaxia-rag](https://smabbler.gitbook.io/smabbler/api-rag/smabblers-api-rag)\n\nTo use Galaxia first upload your texts and create a Graph Language Model here: [smabbler-cloud](https://beta.cloud.smabbler.com)\n\nAfter the model is built and activated, you will be able to use this integration to retrieve what you need.\n"
  }
,
  {
    "path": "python/integrations/retrievers/re_phrase.mdx",
    "filename": "re_phrase.mdx",
    "size_bytes": 3124,
    "line_count": 110,
    "preview": "---\ntitle: RePhraseQuery\n---\n\n`RePhraseQuery` is a simple retriever that applies an LLM between the user input and the query passed by the retriever.\n\nIt can be used to pre-process the user input in any way.\n\n## Example\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/arcee.mdx",
    "filename": "arcee.mdx",
    "size_bytes": 2341,
    "line_count": 68,
    "preview": "---\ntitle: Arcee\n---\n\n>[Arcee](https://www.arcee.ai/about/about-us) helps with the development of the SLMs—small, specialized, secure, and scalable language models.\n\nThis notebook demonstrates how to use the `ArceeRetriever` class to retrieve relevant document(s) for Arcee's `Domain Adapted Language Models` (`DALMs`).\n\n### Setup\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/ibm_watsonx_ranker.mdx",
    "filename": "ibm_watsonx_ranker.mdx",
    "size_bytes": 11929,
    "line_count": 272,
    "preview": "---\ntitle: IBM watsonx.ai\n---\n\n>`WatsonxRerank` is a wrapper for IBM [watsonx.ai](https://www.ibm.com/products/watsonx-ai) foundation models.\n\nThis notebook shows how to use [watsonx's rerank endpoint](https://cloud.ibm.com/apidocs/watsonx-ai#text-rerank) in a retriever.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/chaindesk.mdx",
    "filename": "chaindesk.mdx",
    "size_bytes": 3721,
    "line_count": 39,
    "preview": "---\ntitle: Chaindesk\n---\n\n>[Chaindesk platform](https://docs.chaindesk.ai/introduction) brings data from anywhere (Datsources: Text, PDF, Word, PowerPpoint, Excel, Notion, Airtable, Google Sheets, etc..) into Datastores (container of multiple Datasources).\nThen your Datastores can be connected to ChatGPT via Plugins or any other Large Langue Model (LLM) via the `Chaindesk API`.\n\nThis notebook shows how to use [Chaindesk's](https://www.chaindesk.ai/) retriever.\n\nFirst, you will need to sign up for Chaindesk, create a datastore, add some data and get your datastore api endpoint url. You need the [API Key](https://docs.chaindesk.ai/api-reference/authentication).\n"
  }
,
  {
    "path": "python/integrations/retrievers/outline.mdx",
    "filename": "outline.mdx",
    "size_bytes": 15663,
    "line_count": 80,
    "preview": "---\ntitle: Outline\n---\n\n>[Outline](https://www.getoutline.com/) is an open-source collaborative knowledge base platform designed for team information sharing.\n\nThis notebook shows how to retrieve documents from your Outline instance into the Document format that is used downstream.\n\n## Setup\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/breebs.mdx",
    "filename": "breebs.mdx",
    "size_bytes": 6090,
    "line_count": 37,
    "preview": "---\ntitle: Breebs\n---\n\n>[BREEBS](https://www.breebs.com/) is an open collaborative knowledge platform.\nAnybody can create a Breeb, a knowledge capsule, based on PDFs stored on a Google Drive folder.\nA breeb can be used by any LLM/chatbot to improve its expertise, reduce hallucinations and give access to sources.\nBehind the scenes, Breebs implements several Retrieval Augmented Generation (RAG) models to seamlessly provide useful context at each iteration.\n\n## List of available breebs\n"
  }
,
  {
    "path": "python/integrations/retrievers/tavily.mdx",
    "filename": "tavily.mdx",
    "size_bytes": 10055,
    "line_count": 108,
    "preview": "---\ntitle: TavilySearchAPI\n---\n\n>[Tavily's Search API](https://tavily.com) is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.\n\nWe can use this as a [retriever](/oss/langchain/retrieval). It will show functionality specific to this integration. After going through, it may be useful to explore [relevant use-case pages](/oss/langchain/rag) to learn how to use this vectorstore as part of a larger chain.\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/google_vertex_ai_search.mdx",
    "filename": "google_vertex_ai_search.mdx",
    "size_bytes": 12760,
    "line_count": 253,
    "preview": "---\ntitle: Google Vertex AI Search\n---\n\n>[Google Vertex AI Search](https://cloud.google.com/enterprise-search) (formerly known as `Enterprise Search` on `Generative AI App Builder`) is a part of the [Vertex AI](https://cloud.google.com/vertex-ai) machine learning platform offered by `Google Cloud`.\n>\n>`Vertex AI Search` lets organizations quickly build generative AI-powered search engines for customers and employees. It's underpinned by a variety of `Google Search` technologies, including semantic search, which helps deliver more relevant results than traditional keyword-based search techniques by using natural language processing and machine learning techniques to infer relationships within the content and intent from the user’s query input. Vertex AI Search also benefits from Google’s expertise in understanding how users search and factors in content relevance to order displayed results.\n\n>`Vertex AI Search` is available in the `Google Cloud Console` and via an API for enterprise workflow integration.\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/merger_retriever.mdx",
    "filename": "merger_retriever.mdx",
    "size_bytes": 4814,
    "line_count": 121,
    "preview": "---\ntitle: LOTR (Merger Retriever)\n---\n\n>`Lord of the Retrievers (LOTR)`, also known as `MergerRetriever`, takes a list of retrievers as input and merges the results of their get_relevant_documents() methods into a single list. The merged results will be a list of documents that are relevant to the query and that have been ranked by the different retrievers.\n\nThe `MergerRetriever` class can be used to improve the accuracy of document retrieval in a number of ways. First, it can combine the results of multiple retrievers, which can help to reduce the risk of bias in the results. Second, it can rank the results of the different retrievers, which can help to ensure that the most relevant documents are returned first.\n\n```python\nimport os\n"
  }
,
  {
    "path": "python/integrations/retrievers/vespa.mdx",
    "filename": "vespa.mdx",
    "size_bytes": 1874,
    "line_count": 55,
    "preview": "---\ntitle: Vespa\n---\n\n>[Vespa](https://vespa.ai/) is a fully featured search engine and vector database. It supports vector search (ANN), lexical search, and search in structured data, all in the same query.\n\nThis notebook shows how to use `Vespa.ai` as a LangChain retriever.\n\nIn order to create a retriever, we use [pyvespa](https://pyvespa.readthedocs.io/en/latest/index.html) to\ncreate a connection a `Vespa` service.\n"
  }
,
  {
    "path": "python/integrations/retrievers/amazon_kendra_retriever.mdx",
    "filename": "amazon_kendra_retriever.mdx",
    "size_bytes": 1176,
    "line_count": 29,
    "preview": "---\ntitle: Amazon Kendra\n---\n\n> [Amazon Kendra](https://docs.aws.amazon.com/kendra/latest/dg/what-is-kendra.html) is an intelligent search service provided by `Amazon Web Services` (`AWS`). It utilizes advanced natural language processing (NLP) and machine learning algorithms to enable powerful search capabilities across various data sources within an organization. `Kendra` is designed to help users find the information they need quickly and accurately, improving productivity and decision-making.\n\n> With `Kendra`, users can search across a wide range of content types, including documents, FAQs, knowledge bases, manuals, and websites. It supports multiple languages and can understand complex queries, synonyms, and contextual meanings to provide highly relevant search results.\n\n## Using the Amazon kendra index retriever\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/contextual.mdx",
    "filename": "contextual.mdx",
    "size_bytes": 5864,
    "line_count": 132,
    "preview": "---\ntitle: Contextual AI Reranker\n---\n\nContextual AI's Instruction-Following Reranker is the world's first reranker designed to follow custom instructions about how to prioritize documents based on specific criteria like recency, source, and metadata. With superior performance on the BEIR benchmark (scoring 61.2 and outperforming competitors by significant margins), it delivers unprecedented control and accuracy for enterprise RAG applications.\n\n## Key capabilities\n\n- **Instruction Following**: Dynamically control document ranking through natural language commands\n- **Conflict Resolution**: Intelligently handle contradictory information from multiple knowledge sources\n"
  }
,
  {
    "path": "python/integrations/retrievers/kay.mdx",
    "filename": "kay.mdx",
    "size_bytes": 10315,
    "line_count": 104,
    "preview": "---\ntitle: Kay.ai\n---\n\n>[Kai Data API](https://www.kay.ai/) built for RAG 🕵️ We are curating the world's largest datasets as high-quality embeddings so your AI agents can retrieve context on the fly. Latest models, fast retrieval, and zero infra.\n\nThis notebook shows you how to retrieve datasets supported by [Kay](https://kay.ai/). You can currently search `SEC Filings` and `Press Releases of US companies`. Visit [kay.ai](https://kay.ai) for the latest data drops. For any questions, join our [discord](https://discord.gg/hAnE4e5T6M) or [tweet at us](https://twitter.com/vishalrohra_)\n\n## Installation\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/elasticsearch_retriever.mdx",
    "filename": "elasticsearch_retriever.mdx",
    "size_bytes": 14887,
    "line_count": 383,
    "preview": "---\ntitle: Elasticsearch\n---\n\n>[Elasticsearch](https://www.elastic.co/elasticsearch/) is a distributed, RESTful search and analytics engine. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents. It supports keyword search, vector search, hybrid search and complex filtering.\n\nThe `ElasticsearchRetriever` is a generic wrapper to enable flexible access to all `Elasticsearch` features through the [Query DSL](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html).  For most use cases the other classes (`ElasticsearchStore`, `ElasticsearchEmbeddings`, etc.) should suffice, but if they don't you can use `ElasticsearchRetriever`.\n\nThis guide will help you get started with the Elasticsearch [retriever](/oss/langchain/retrieval). For detailed documentation of all `ElasticsearchRetriever` features and configurations head to the [API reference](https://python.langchain.com/api_reference/elasticsearch/retrievers/langchain_elasticsearch.retrievers.ElasticsearchRetriever.html).\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/sec_filings.mdx",
    "filename": "sec_filings.mdx",
    "size_bytes": 3598,
    "line_count": 90,
    "preview": "---\ntitle: SEC filing\n---\n\n>[SEC filing](https://www.sec.gov/edgar) is a financial statement or other formal document submitted to the U.S. Securities and Exchange Commission (SEC). Public companies, certain insiders, and broker-dealers are required to make regular `SEC filings`. Investors and financial professionals rely on these filings for information about companies they are evaluating for investment purposes.\n>\n>`SEC filings` data powered by [Kay.ai](https://kay.ai) and [Cybersyn](https://www.cybersyn.com/) via [Snowflake Marketplace](https://app.snowflake.com/marketplace/providers/GZTSZAS2KCS/Cybersyn%2C%20Inc).\n\n## Setup\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/kinetica.mdx",
    "filename": "kinetica.mdx",
    "size_bytes": 2480,
    "line_count": 94,
    "preview": "---\ntitle: Kinetica Vectorstore based Retriever\n---\n\n>[Kinetica](https://www.kinetica.com/) is a database with integrated support for vector similarity search\n\nIt supports:\n\n- exact and approximate nearest neighbor search\n- L2 distance, inner product, and cosine distance\n"
  }
,
  {
    "path": "python/integrations/retrievers/cohere.mdx",
    "filename": "cohere.mdx",
    "size_bytes": 11951,
    "line_count": 142,
    "preview": "---\ntitle: Cohere RAG\n---\n\n>[Cohere](https://cohere.ai/about) is a Canadian startup that provides natural language processing models that help companies improve human-machine interactions.\n\nThis notebook covers how to get started with the `Cohere RAG` retriever. This allows you to leverage the ability to search documents over various connectors or by supplying your own.\n\n```python\nimport getpass\n"
  }
,
  {
    "path": "python/integrations/retrievers/permit.mdx",
    "filename": "permit.mdx",
    "size_bytes": 5921,
    "line_count": 207,
    "preview": "---\ntitle: Permit\n---\n\nPermit is an access control platform that provides fine-grained, real-time permission management using various models such as RBAC, ABAC, and ReBAC. It enables organizations to enforce dynamic policies across their applications, ensuring that only authorized users can access specific resources.\n\n### Integration details\n\nThis notebook illustrates how to integrate [Permit.io](https://permit.io/) permissions into LangChain retrievers.\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/asknews.mdx",
    "filename": "asknews.mdx",
    "size_bytes": 12698,
    "line_count": 118,
    "preview": "---\ntitle: AskNews\n---\n\n> [AskNews](https://asknews.app) infuses any LLM with the latest global news (or historical news), using a single natural language query. Specifically, AskNews is enriching over 300k articles per day by translating, summarizing, extracting entities, and indexing them into hot and cold vector databases. AskNews puts these vector databases on a low-latency endpoint for you. When you query AskNews, you get back a prompt-optimized string that contains all the most pertinent enrichments (e.g. entities, classifications, translation, summarization). This means that you do not need to manage your own news RAG, and you do not need to worry about how to properly convey news information in a condensed way to your LLM.\n> AskNews is also committed to transparency, which is why our coverage is monitored and diversified across hundreds of countries, 13 languages, and 50 thousand sources. If you'd like to track our source coverage, you can visit our [transparency dashboard](https://asknews.app/en/transparency).\n\n## Setup\n\nThe integration lives in the `langchain-community` package. We also need to install the `asknews` package itself.\n"
  }
,
  {
    "path": "python/integrations/retrievers/bm25.mdx",
    "filename": "bm25.mdx",
    "size_bytes": 2144,
    "line_count": 92,
    "preview": "---\ntitle: BM25\n---\n\n>[BM25 (Wikipedia)](https://en.wikipedia.org/wiki/Okapi_BM25) also known as the `Okapi BM25`, is a ranking function used in information retrieval systems to estimate the relevance of documents to a given search query.\n>\n>`BM25Retriever` retriever uses the [`rank_bm25`](https://github.com/dorianbrown/rank_bm25) package.\n\n```python\npip install -qU  rank_bm25\n"
  }
,
  {
    "path": "python/integrations/retrievers/cohere-reranker.mdx",
    "filename": "cohere-reranker.mdx",
    "size_bytes": 14623,
    "line_count": 324,
    "preview": "---\ntitle: Cohere reranker\n---\n\n>[Cohere](https://cohere.ai/about) is a Canadian startup that provides natural language processing models that help companies improve human-machine interactions.\n\nThis notebook shows how to use [Cohere's rerank endpoint](https://docs.cohere.com/docs/reranking) in a retriever.\n\n```python\npip install -qU  cohere\n"
  }
,
  {
    "path": "python/integrations/retrievers/graph_rag.mdx",
    "filename": "graph_rag.mdx",
    "size_bytes": 11837,
    "line_count": 337,
    "preview": "---\ntitle: Graph RAG\n---\n\nThis guide provides an introduction to Graph RAG. For detailed documentation of all\nsupported features and configurations, refer to the\n[Graph RAG Project Page](https://datastax.github.io/graph-rag/).\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/embedchain.mdx",
    "filename": "embedchain.mdx",
    "size_bytes": 5977,
    "line_count": 112,
    "preview": "---\ntitle: Embedchain\n---\n\n>[Embedchain](https://github.com/embedchain/embedchain) is a RAG framework to create data pipelines. It loads, indexes, retrieves and syncs all the data.\n>\n>It is available as an [open source package](https://github.com/embedchain/embedchain) and as a [hosted platform solution](https://app.embedchain.ai/).\n\nThis notebook shows how to use a retriever that uses `Embedchain`.\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/greennode_reranker.mdx",
    "filename": "greennode_reranker.mdx",
    "size_bytes": 8149,
    "line_count": 207,
    "preview": "---\ntitle: GreenNode\n---\n\n>[GreenNode](https://greennode.ai/) is a global AI solutions provider and a **NVIDIA Preferred Partner**, delivering full-stack AI capabilities—from infrastructure to application—for enterprises across the US, MENA, and APAC regions. Operating on **world-class infrastructure** (LEED Gold, TIA‑942, Uptime Tier III), GreenNode empowers enterprises, startups, and researchers with a comprehensive suite of AI services\n\nThis guide provides a walkthrough on getting started with the `GreenNodeRerank` retriever. It enables you to perform document search using built-in connectors or by integrating your own data sources, leveraging GreenNode's reranking capabilities for improved relevance.\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/retrievers/nimble.mdx",
    "filename": "nimble.mdx",
    "size_bytes": 173296,
    "line_count": 131,
    "preview": "---\ntitle: Nimble\n---\n\n# NimbleSearchRetriever\n\n `NimbleSearchRetriever` enables developers to build RAG applications and AI Agents that can search, access, and retrieve online information from anywhere on the web.\n\n `NimbleSearchRetriever` harnesses Nimble's Data APIs to execute search queries and retrieve web data in an efficient, scalable, and effective fashion.\n It has two modes:\n"
  }
,
  {
    "path": "python/integrations/tools/steam.mdx",
    "filename": "steam.mdx",
    "size_bytes": 5126,
    "line_count": 101,
    "preview": "---\ntitle: Steam Toolkit\n---\n\n>[Steam (Wikipedia)](https://en.wikipedia.org/wiki/Steam_(service)) is a video game digital distribution service and storefront developed by `Valve Corporation`. It provides game updates automatically for Valve's games, and expanded to distributing third-party titles. `Steam` offers various features, like game server matchmaking with Valve Anti-Cheat measures, social networking, and game streaming services.\n\n>[Steam](https://store.steampowered.com/about/) is the ultimate destination for playing, discussing, and creating games.\n\nSteam toolkit has two tools:\n\n"
  }
,
  {
    "path": "python/integrations/tools/dappier.mdx",
    "filename": "dappier.mdx",
    "size_bytes": 14094,
    "line_count": 252,
    "preview": "---\ntitle: Dappier\n---\n\n[Dappier](https://dappier.com) connects any LLM or your Agentic AI to real-time, rights-cleared, proprietary data from trusted sources, making your AI an expert in anything. Our specialized models include Real-Time Web Search, News, Sports, Financial Stock Market Data, Crypto Data, and exclusive content from premium publishers. Explore a wide range of data models in our marketplace at [marketplace.dappier.com](https://marketplace.dappier.com).\n\n[Dappier](https://dappier.com) delivers enriched, prompt-ready, and contextually relevant data strings, optimized for seamless integration with LangChain. Whether you're building conversational AI, recommendation engines, or intelligent search, Dappier's LLM-agnostic RAG models ensure your AI has access to verified, up-to-date data—without the complexity of building and managing your own retrieval pipeline.\n\n# Dappier tool\n\n"
  }
,
  {
    "path": "python/integrations/tools/agentql.mdx",
    "filename": "agentql.mdx",
    "size_bytes": 29192,
    "line_count": 593,
    "preview": "---\ntitle: AgentQL\n---\n\n[AgentQL](https://www.agentql.com/) tools provides web interaction and structured data extraction from any web page using an [AgentQL query](https://docs.agentql.com/agentql-query) or a Natural Language prompt. AgentQL can be used across multiple languages and web pages without breaking over time and change.\n\n## Overview\n\nAgentQL provides the following three tools:\n\n"
  }
,
  {
    "path": "python/integrations/tools/edenai_tools.mdx",
    "filename": "edenai_tools.mdx",
    "size_bytes": 17951,
    "line_count": 317,
    "preview": "---\ntitle: Eden AI\n---\n\nThis Jupyter Notebook demonstrates how to use Eden AI tools with an Agent.\n\nEden AI is revolutionizing the AI landscape by uniting the best AI providers, empowering users to unlock limitless possibilities and tap into the true potential of artificial intelligence. With an all-in-one comprehensive and hassle-free platform, it allows users to deploy AI features to production lightning fast, enabling effortless access to the full breadth of AI capabilities via a single API. (website: [edenai.co/](https://edenai.co/) )\n\nBy including an Edenai tool in the list of tools provided to an Agent, you can grant your Agent the ability to do multiple tasks, such as:\n\n"
  }
,
  {
    "path": "python/integrations/tools/ddg.mdx",
    "filename": "ddg.mdx",
    "size_bytes": 10466,
    "line_count": 92,
    "preview": "---\ntitle: DuckDuckGo Search\n---\n\nThis guide shows over how to use the DuckDuckGo search component.\n\n## Usage\n\n```python\npip install -qU duckduckgo-search langchain-community\n"
  }
,
  {
    "path": "python/integrations/tools/amadeus.mdx",
    "filename": "amadeus.mdx",
    "size_bytes": 47391,
    "line_count": 340,
    "preview": "---\ntitle: Amadeus Toolkit\n---\n\nThis notebook walks you through connecting LangChain to the `Amadeus` travel APIs.\n\nThis `Amadeus` toolkit allows agents to make decision when it comes to travel, especially searching and booking trips with flights.\n\nTo use this toolkit, you will need to have your Amadeus API keys ready, explained in the [Get started Amadeus Self-Service APIs](https://developers.amadeus.com/get-started/get-started-with-self-service-apis-335). Once you've received a AMADEUS_CLIENT_ID and AMADEUS_CLIENT_SECRET, you can input them as environmental variables below.\n\n"
  }
,
  {
    "path": "python/integrations/tools/oracleai.mdx",
    "filename": "oracleai.mdx",
    "size_bytes": 7083,
    "line_count": 115,
    "preview": "---\ntitle: Oracle AI Vector Search Generate Summary\n---\n\nOracle AI Vector Search is designed for Artificial Intelligence (AI) workloads that allows you to query data based on semantics, rather than keywords.\nOne of the biggest benefits of Oracle AI Vector Search is that semantic search on unstructured data can be combined with relational search on business data in one single system.\nThis is not only powerful but also significantly more effective because you don't need to add a specialized vector database, eliminating the pain of data fragmentation between multiple systems.\n\nIn addition, your vectors can benefit from all of Oracle Database’s most powerful features, like the following:\n\n"
  }
,
  {
    "path": "python/integrations/tools/serpapi.mdx",
    "filename": "serpapi.mdx",
    "size_bytes": 1972,
    "line_count": 62,
    "preview": "---\ntitle: SerpApi\n---\n\nThis notebook goes over how to use the [SerpApi](https://serpapi.com/) component to search the web.\n\nSign up for a SerpApi account [here](https://serpapi.com/users/sign_up) to get 250 free searches per month. After signing up, you can find your API key on the [dashboard](https://serpapi.com/manage-api-key).\n\nThen set the environment variable `.env` file `SERPAPI_API_KEY` to your API key.\n\n"
  }
,
  {
    "path": "python/integrations/tools/azure_ai_services.mdx",
    "filename": "azure_ai_services.mdx",
    "size_bytes": 10167,
    "line_count": 186,
    "preview": "---\ntitle: Azure AI Services Toolkit\n---\n\nThis toolkit is used to interact with the `Azure AI Services API` to achieve some multimodal capabilities.\n\nCurrently There are five tools bundled in this toolkit:\n\n- **AzureAiServicesImageAnalysisTool**: used to extract caption, objects, tags, and text from images.\n- **AzureAiServicesDocumentIntelligenceTool**: used to extract text, tables, and key-value pairs from documents.\n"
  }
,
  {
    "path": "python/integrations/tools/human_tools.mdx",
    "filename": "human_tools.mdx",
    "size_bytes": 5070,
    "line_count": 155,
    "preview": "---\ntitle: Human as a tool\n---\n\nHuman are AGI so they can certainly be used as a tool to help out AI agent\nwhen it is confused.\n\n```python\npip install -qU  langchain-community\n```\n"
  }
,
  {
    "path": "python/integrations/tools/google_drive.mdx",
    "filename": "google_drive.mdx",
    "size_bytes": 4589,
    "line_count": 130,
    "preview": "---\ntitle: Google Drive\n---\n\nThis notebook walks through connecting a LangChain to the `Google Drive API`.\n\n## Prerequisites\n\n1. Create a Google Cloud project or use an existing project\n1. Enable the [Google Drive API](https://console.cloud.google.com/flows/enableapi?apiid=drive.googleapis.com)\n"
  }
,
  {
    "path": "python/integrations/tools/yahoo_finance_news.mdx",
    "filename": "yahoo_finance_news.mdx",
    "size_bytes": 6223,
    "line_count": 141,
    "preview": "---\ntitle: Yahoo Finance News\n---\n\nThis notebook goes over how to use the `yahoo_finance_news` tool with an agent.\n\n## Setting up\n\nFirst, you need to install `yfinance` python package.\n\n"
  }
,
  {
    "path": "python/integrations/tools/polygon.mdx",
    "filename": "polygon.mdx",
    "size_bytes": 232202,
    "line_count": 402,
    "preview": "---\ntitle: Polygon IO Toolkit and Tools\n---\n\nThis notebook shows how to use agents to interact with the [Polygon IO](https://polygon.io/) toolkit. The toolkit provides access to Polygon's Stock Market Data API.\n\n## Setup\n\n### Installation\n\n"
  }
,
  {
    "path": "python/integrations/tools/cogniswitch.mdx",
    "filename": "cogniswitch.mdx",
    "size_bytes": 4985,
    "line_count": 143,
    "preview": "---\ntitle: Cogniswitch Toolkit\n---\n\nCogniSwitch is used to build production ready applications that can consume, organize and retrieve knowledge flawlessly. Using the framework of your choice, in this case LangChain, CogniSwitch helps alleviate the stress of decision making when it comes to, choosing the right storage and retrieval formats. It also eradicates reliability issues and hallucinations when it comes to responses that are generated.\n\n## Setup\n\nVisit [this page](https://www.cogniswitch.ai/developer?utm_source=langchain&utm_medium=langchainbuild&utm_id=dev) to register a Cogniswitch account.\n\n"
  }
,
  {
    "path": "python/integrations/tools/parallel_extract.mdx",
    "filename": "parallel_extract.mdx",
    "size_bytes": 7771,
    "line_count": 247,
    "preview": "---\ntitle: Parallel Extract\n---\n\n>[Parallel](https://platform.parallel.ai/) is a real-time web search and content extraction platform designed specifically for LLMs and AI applications.\n\nThe `ParallelExtractTool` provides access to Parallel's Extract API, which extracts clean, structured content from web pages.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/tools/powerbi.mdx",
    "filename": "powerbi.mdx",
    "size_bytes": 3230,
    "line_count": 104,
    "preview": "---\ntitle: PowerBI Toolkit\n---\n\nThis notebook showcases an agent interacting with a `Power BI Dataset`. The agent is answering more general questions about a dataset, as well as recover from errors.\n\nNote that, as this agent is in active development, all answers might not be correct. It runs against the [executequery endpoint](https://learn.microsoft.com/en-us/rest/api/power-bi/datasets/execute-queries), which does not allow deletes.\n\n### Notes\n\n"
  }
,
  {
    "path": "python/integrations/tools/daytona_data_analysis.mdx",
    "filename": "daytona_data_analysis.mdx",
    "size_bytes": 6209,
    "line_count": 238,
    "preview": "---\ntitle: DaytonaDataAnalysisTool\n---\n\nThis guide provides a quick overview for getting started with the `DaytonaDataAnalysisTool`.\n\n<Tip>\n    **Detailed Usage Example**\n\n    For a detailed usage example of this tool, see the [Daytona documentation](https://www.daytona.io/docs/en/langchain-data-analysis).\n"
  }
,
  {
    "path": "python/integrations/tools/searchapi.mdx",
    "filename": "searchapi.mdx",
    "size_bytes": 36617,
    "line_count": 535,
    "preview": "---\ntitle: SearchApi\n---\n\nThis notebook shows examples of how to use SearchApi to search the web. Go to [https://www.searchapi.io/](https://www.searchapi.io/) to sign up for a free account and get API key.\n\n```python\nimport os\n\nos.environ[\"SEARCHAPI_API_KEY\"] = \"\"\n"
  }
,
  {
    "path": "python/integrations/tools/privy.mdx",
    "filename": "privy.mdx",
    "size_bytes": 4015,
    "line_count": 161,
    "preview": "---\ntitle: Privy\n---\n\n[Privy](https://privy.io) is powerful wallet infrastructure for AI agents, built for scale.\n\n## Overview\n\nCreate agents that can:\n\n"
  }
,
  {
    "path": "python/integrations/tools/gitlab.mdx",
    "filename": "gitlab.mdx",
    "size_bytes": 4826,
    "line_count": 153,
    "preview": "---\ntitle: GitLab Toolkit\n---\n\nThe `GitLab` toolkit contains tools that enable an LLM agent to interact with a gitlab repository.\nThe tool is a wrapper for the [python-gitlab](https://github.com/python-gitlab/python-gitlab) library.\n\n## Quickstart\n\n1. Install the python-gitlab library\n"
  }
,
  {
    "path": "python/integrations/tools/bodo.mdx",
    "filename": "bodo.mdx",
    "size_bytes": 8741,
    "line_count": 256,
    "preview": "---\ntitle: Bodo DataFrames\n---\n\nThis notebook gives an overview of how to create agents and perform question answering over large datasets\nwith the [langchain-bodo](https://pypi.org/project/langchain-bodo/) integration package, which uses [Bodo DataFrames](https://github.com/bodo-ai/Bodo) and the `Python` agent under the hood.\n\nBodo DataFrames is a high performance DataFrame library that can automatically accelerate and scale\nPandas code with a simple import change (see examples below). Because of it's strong Pandas compatibility, Bodo DataFrames\nenables LLMs, which are typically good at generating Pandas code, to answer questions about larger\n"
  }
,
  {
    "path": "python/integrations/tools/anchor_browser.mdx",
    "filename": "anchor_browser.mdx",
    "size_bytes": 6583,
    "line_count": 188,
    "preview": "---\ntitle: Anchor Browser\n---\n\nAnchor is a platform for AI Agentic browser automation, which solves the challenge of automating workflows for web applications that lack APIs or have limited API coverage. It simplifies the creation, deployment, and management of browser-based automations, transforming complex web interactions into simple API endpoints.\n\nThis guide provides a quick overview for getting started with Anchor Browser tools. For more information of Anchor Browser visit [Anchorbrowser.io](https://anchorbrowser.io?utm=langchain) or the [Anchor Browser Docs](https://docs.anchorbrowser.io?utm=langchain)\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/tools/bash.mdx",
    "filename": "bash.mdx",
    "size_bytes": 3450,
    "line_count": 112,
    "preview": "---\ntitle: Shell (bash)\n---\n\nGiving agents access to the shell is powerful (though risky outside a sandboxed environment).\n\nThe LLM can use it to execute any shell commands. A common use case for this is letting the LLM interact with your local file system.\n\n**Note:** Shell tool does not work with Windows OS.\n\n"
  }
,
  {
    "path": "python/integrations/tools/google_scholar.mdx",
    "filename": "google_scholar.mdx",
    "size_bytes": 3742,
    "line_count": 39,
    "preview": "---\ntitle: Google Scholar\n---\n\nThis notebook goes through how to use Google Scholar Tool\n\n```python\npip install -qU  google-search-results langchain-community\n```\n\n"
  }
,
  {
    "path": "python/integrations/tools/brightdata_serp.mdx",
    "filename": "brightdata_serp.mdx",
    "size_bytes": 6674,
    "line_count": 201,
    "preview": "---\ntitle: BrightDataSERP\ndescription: Query search engines with geo-targeting using Bright Data's SERP API\n---\n\n[Bright Data](https://brightdata.com/) provides a powerful SERP API that allows you to query search engines (Google, Bing, DuckDuckGo, Yandex) with geo-targeting and advanced customization options, particularly useful for AI agents requiring real-time web information.\n\n## Overview\n\n### Integration details\n"
  }
,
  {
    "path": "python/integrations/tools/github.mdx",
    "filename": "github.mdx",
    "size_bytes": 10603,
    "line_count": 218,
    "preview": "---\ntitle: GitHub Toolkit\n---\n\nThe `GitHub` toolkit contains tools that enable an LLM agent to interact with a github repository.\nThe tool is a wrapper for the [PyGitHub](https://github.com/PyGithub/PyGithub) library.\n\nFor detailed documentation of all GithubToolkit features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.github.toolkit.GitHubToolkit.html).\n\n## Setup\n"
  }
,
  {
    "path": "python/integrations/tools/composio.mdx",
    "filename": "composio.mdx",
    "size_bytes": 11074,
    "line_count": 397,
    "preview": "---\ntitle: Composio\ndescription: Access 500+ tools and integrations through Composio's unified API platform for AI agents, with OAuth handling, event-driven workflows, and multi-user support.\n---\n\n[Composio](https://composio.dev) is an integration platform that provides access to 500+ tools across popular applications like GitHub, Slack, Notion, and more. It enables AI agents to interact with external services through a unified API, handling authentication, permissions, and event-driven workflows.\n\n## Overview\n\n### Integration details\n"
  }
,
  {
    "path": "python/integrations/tools/filesystem.mdx",
    "filename": "filesystem.mdx",
    "size_bytes": 2323,
    "line_count": 86,
    "preview": "---\ntitle: File System\n---\n\nLangChain provides tools for interacting with a local file system out of the box. This notebook walks through some of them.\n\n**Note:** these tools are not recommended for use outside a sandboxed environment!\n\n```python\npip install -qU langchain-community\n"
  }
,
  {
    "path": "python/integrations/tools/google_places.mdx",
    "filename": "google_places.mdx",
    "size_bytes": 935,
    "line_count": 35,
    "preview": "---\ntitle: Google Places\n---\n\nThis notebook goes through how to use Google Places API\n\n```python\npip install -qU  googlemaps langchain-community\n```\n\n"
  }
,
  {
    "path": "python/integrations/tools/hyperbrowser_web_scraping_tools.mdx",
    "filename": "hyperbrowser_web_scraping_tools.mdx",
    "size_bytes": 14544,
    "line_count": 406,
    "preview": "---\ntitle: Hyperbrowser Web Scraping Tools\n---\n\n[Hyperbrowser](https://hyperbrowser.ai) is a platform for running and scaling headless browsers. It lets you launch and manage browser sessions at scale and provides easy to use solutions for any webscraping needs, such as scraping a single page or crawling an entire site.\n\nKey Features:\n\n- Instant Scalability - Spin up hundreds of browser sessions in seconds without infrastructure headaches\n- Simple Integration - Works seamlessly with popular tools like Puppeteer and Playwright\n"
  }
,
  {
    "path": "python/integrations/tools/semanticscholar.mdx",
    "filename": "semanticscholar.mdx",
    "size_bytes": 17375,
    "line_count": 152,
    "preview": "---\ntitle: Semantic Scholar API Tool\n---\n\nThis notebook demos how to use the semantic scholar tool with an agent.\n\n```python\n# start by installing semanticscholar api\npip install -qU  semanticscholar\n```\n"
  }
,
  {
    "path": "python/integrations/tools/nvidia_riva.mdx",
    "filename": "nvidia_riva.mdx",
    "size_bytes": 1060631,
    "line_count": 535,
    "preview": "---\ntitle: NVIDIA Riva ASR and TTS\n---\n\n## NVIDIA riva\n\n[NVIDIA Riva](https://www.nvidia.com/en-us/ai-data-science/products/riva/) is a GPU-accelerated multilingual speech and translation AI software development kit for building fully customizable, real-time conversational AI pipelines—including automatic speech recognition (ASR), text-to-speech (TTS), and neural machine translation (NMT) applications—that can be deployed in clouds, in data centers, at the edge, or on embedded devices.\n\nThe Riva Speech API server exposes a simple API for performing speech recognition, speech synthesis, and a variety of natural language processing inferences and is integrated into LangChain for ASR and TTS. See instructions on how to [setup a Riva Speech API](#3-setup) server below.\n\n"
  }
,
  {
    "path": "python/integrations/tools/bearly.mdx",
    "filename": "bearly.mdx",
    "size_bytes": 19719,
    "line_count": 219,
    "preview": "---\ntitle: Bearly Code Interpreter\n---\n\n> Bearly Code Interpreter allows for remote execution of code. This makes it perfect for a code sandbox for agents, to allow for safe implementation of things like Code Interpreter\n\nGet your api key here: [bearly.ai/dashboard/developers](https://bearly.ai/dashboard/developers)\n\n```python\npip install -qU langchain-community\n"
  }
,
  {
    "path": "python/integrations/tools/zapier.mdx",
    "filename": "zapier.mdx",
    "size_bytes": 11818,
    "line_count": 224,
    "preview": "---\ntitle: Zapier Natural Language Actions\n---\n\n**Deprecated** This API will be sunset on 2023-11-17: [nla.zapier.com/start/](https://nla.zapier.com/start/)\n\n>[Zapier Natural Language Actions](https://nla.zapier.com/start/) gives you access to the 5k+ apps, 20k+ actions on Zapier's platform through a natural language API interface.\n>\n>NLA supports apps like `Gmail`, `Salesforce`, `Trello`, `Slack`, `Asana`, `HubSpot`, `Google Sheets`, `Microsoft Teams`, and thousands more apps: [zapier.com/apps](https://zapier.com/apps)\n>`Zapier NLA` handles ALL the underlying API auth and translation from natural language --> underlying API call --> return simplified output for LLMs. The key idea is you, or your users, expose a set of actions via an oauth-like setup window, which you can then query and execute via a REST API.\n"
  }
,
  {
    "path": "python/integrations/tools/alpha_vantage.mdx",
    "filename": "alpha_vantage.mdx",
    "size_bytes": 3189,
    "line_count": 106,
    "preview": "---\ntitle: Alpha Vantage\n---\n\n>[Alpha Vantage](https://www.alphavantage.co) Alpha Vantage provides realtime and historical financial market data through a set of powerful and developer-friendly data APIs and spreadsheets.\n\nGenerate the `ALPHAVANTAGE_API_KEY` [at their website](https://www.alphavantage.co/support/#api-key)\n.\nUse the `AlphaVantageAPIWrapper` to get currency exchange rates.\n\n"
  }
,
  {
    "path": "python/integrations/tools/wikipedia.mdx",
    "filename": "wikipedia.mdx",
    "size_bytes": 4736,
    "line_count": 28,
    "preview": "---\ntitle: Wikipedia\n---\n\n>[Wikipedia](https://wikipedia.org/) is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. `Wikipedia` is the largest and most-read reference work in history.\n\nFirst, you need to install `wikipedia` python package.\n\n```python\npip install -qU  wikipedia\n"
  }
,
  {
    "path": "python/integrations/tools/ads4gpts.mdx",
    "filename": "ads4gpts.mdx",
    "size_bytes": 7643,
    "line_count": 197,
    "preview": "---\ntitle: ADS4GPTs\n---\n\nIntegrate AI native advertising into your Agentic application.\n\n## Overview\n\nThis notebook outlines how to use the ADS4GPTs Tools and Toolkit in LangChain directly. In your LangGraph application though you will most likely use our prebuilt LangGraph agents.\n\n"
  }
,
  {
    "path": "python/integrations/tools/pandas.mdx",
    "filename": "pandas.mdx",
    "size_bytes": 3826,
    "line_count": 153,
    "preview": "---\ntitle: Pandas Dataframe\n---\n\nThis notebook shows how to use agents to interact with a `Pandas DataFrame`. It is mostly optimized for question answering.\n\n**NOTE: this agent calls the `Python` agent under the hood, which executes LLM generated Python code - this can be bad if the LLM generated Python code is harmful. Use cautiously.**\n\n**NOTE: Since langchain migrated to v0.3 you should upgrade langchain_openai and langchain.   This would avoid import errors.**\n\n"
  }
,
  {
    "path": "python/integrations/tools/vectara.mdx",
    "filename": "vectara.mdx",
    "size_bytes": 36281,
    "line_count": 313,
    "preview": "---\ntitle: Vectara\n---\n\n[Vectara](https://vectara.com/) is the trusted AI Assistant and Agent platform which focuses on enterprise readiness for mission-critical applications. For more [details](../providers/vectara.ipynb).\n\n[Vectara](https://vectara.com/)  provides several tools that can be used with LangChain.\n\n- **VectaraSearch**: For semantic search over your corpus\n- **VectaraRAG**: For generating summaries using RAG\n"
  }
,
  {
    "path": "python/integrations/tools/zenguard.mdx",
    "filename": "zenguard.mdx",
    "size_bytes": 2953,
    "line_count": 87,
    "preview": "---\ntitle: ZenGuard AI\n---\n\n<a href=\"https://colab.research.google.com/github/langchain-ai/langchain/blob/v0.3/docs/docs/integrations/tools/zenguard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" /></a>\n\nThis tool lets you quickly set up [ZenGuard AI](https://www.zenguard.ai/) in your LangChain-powered application. The ZenGuard AI provides ultrafast guardrails to protect your GenAI application from:\n\n- Prompts Attacks\n- Veering of the pre-defined topics\n"
  }
,
  {
    "path": "python/integrations/tools/exa_search.mdx",
    "filename": "exa_search.mdx",
    "size_bytes": 6571,
    "line_count": 204,
    "preview": "---\ntitle: Exa Search\n---\n\nExa is a search engine fully designed for use by LLMs. Search for documents on the internet using **natural language queries**, then retrieve **cleaned HTML content** from desired documents.\n\nUnlike keyword-based search (Google), Exa's neural search capabilities allow it to semantically understand queries and return relevant documents. For example, we could search `\"fascinating article about cats\"` and compare the search results from [Google](https://www.google.com/search?q=fascinating+article+about+cats) and [Exa](https://search.exa.ai/search?q=fascinating%20article%20about%20cats&autopromptString=Here%20is%20a%20fascinating%20article%20about%20cats%3A). Google gives us SEO-optimized listicles based on the keyword \"fascinating\". Exa just works.\n\nThis notebook goes over how to use Exa Search with LangChain.\n\n"
  }
,
  {
    "path": "python/integrations/tools/azure_dynamic_sessions.mdx",
    "filename": "azure_dynamic_sessions.mdx",
    "size_bytes": 45444,
    "line_count": 212,
    "preview": "---\ntitle: Azure Container Apps dynamic sessions\n---\n\nAzure Container Apps dynamic sessions provides a secure and scalable way to run a Python code interpreter in Hyper-V isolated sandboxes. This allows your agents to run potentially untrusted code in a secure environment. The code interpreter environment includes many popular Python packages, such as NumPy, pandas, and scikit-learn. See the [Azure Container App docs](https://learn.microsoft.com/en-us/azure/container-apps/sessions-code-interpreter) for more info on how sessions work.\n\n## Setup\n\nBy default, the `SessionsPythonREPLTool` tool uses `DefaultAzureCredential` to authenticate with Azure. Locally, it'll use your credentials from the Azure CLI or VS Code. Install the Azure CLI and log in with `az login` to authenticate.\n\n"
  }
,
  {
    "path": "python/integrations/tools/awslambda.mdx",
    "filename": "awslambda.mdx",
    "size_bytes": 2002,
    "line_count": 48,
    "preview": "---\ntitle: AWS Lambda\n---\n\n>[`Amazon AWS Lambda`](https://aws.amazon.com/pm/lambda/) is a serverless computing service provided by `Amazon Web Services` (`AWS`). It helps developers to build and run applications and services without provisioning or managing servers. This serverless architecture enables you to focus on writing and deploying code, while AWS automatically takes care of scaling, patching, and managing the infrastructure required to run your applications.\n\nThis notebook goes over how to use the `AWS Lambda` Tool.\n\nBy including the `AWS Lambda` in the list of tools provided to an Agent, you can grant your Agent the ability to invoke code running in your AWS Cloud for whatever purposes you need.\n\n"
  }
,
  {
    "path": "python/integrations/tools/json.mdx",
    "filename": "json.mdx",
    "size_bytes": 5374,
    "line_count": 120,
    "preview": "---\ntitle: JSON Toolkit\n---\n\nThis notebook showcases an agent interacting with large `JSON/dict` objects.\nThis is useful when you want to answer questions about a JSON blob that's too large to fit in the context window of an LLM. The agent is able to iteratively explore the blob to find what it needs to answer the user's question.\n\nIn the below example, we are using the OpenAPI spec for the OpenAI API, which you can find [here](https://github.com/openai/openai-openapi/blob/master/openapi.yaml).\n\nWe will use the JSON agent to answer some questions about the API spec.\n"
  }
,
  {
    "path": "python/integrations/tools/searx_search.mdx",
    "filename": "searx_search.mdx",
    "size_bytes": 20228,
    "line_count": 399,
    "preview": "---\ntitle: SearxNG Search\n---\n\nThis notebook goes over how to use a self hosted `SearxNG` search API to search the web.\n\nYou can [check this link](https://docs.searxng.org/dev/search_api.html) for more informations about `Searx API` parameters.\n\n```python\nimport pprint\n"
  }
,
  {
    "path": "python/integrations/tools/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 24316,
    "line_count": 257,
    "preview": "---\ntitle: \"Tools and toolkits\"\n---\n\n[Tools](/oss/langchain/tools) are utilities designed to be called by a model: their inputs are designed to be generated by models, and their outputs are designed to be passed back to models.\n\nA toolkit is a collection of tools meant to be used together.\n\n## Search\n\n"
  }
,
  {
    "path": "python/integrations/tools/taiga.mdx",
    "filename": "taiga.mdx",
    "size_bytes": 6553,
    "line_count": 178,
    "preview": "---\ntitle: Taiga\n---\n\nThis guide provides a quick overview for getting started with Taiga tooling in [langchain_taiga](https://github.com/Shikenso-Analytics/langchain-taiga/blob/main/docs/tools.ipynb). For more details on each tool and configuration, see the docstrings in your repository or relevant doc pages.\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/tools/python.mdx",
    "filename": "python.mdx",
    "size_bytes": 1234,
    "line_count": 42,
    "preview": "---\ntitle: Python REPL\n---\n\nSometimes, for complex calculations, rather than have an LLM generate the answer directly, it can be better to have the LLM generate code to calculate the answer, and then run that code to get the answer. In order to easily do that, we provide a simple Python REPL to execute commands in.\n\nThis interface will only return things that are printed - therefore, if you want to use it to calculate an answer, make sure to have it print out the answer.\n\n<Warning>\n**Python REPL can execute arbitrary code on the host machine (e.g., delete files, make network requests). Use with caution.**\n"
  }
,
  {
    "path": "python/integrations/tools/mcp_toolbox.mdx",
    "filename": "mcp_toolbox.mdx",
    "size_bytes": 9775,
    "line_count": 259,
    "preview": "---\ntitle: MCP Toolbox for Databases\n---\n\nIntegrate your databases with LangChain agents using MCP Toolbox.\n\n## Overview\n\n[MCP Toolbox for Databases](https://github.com/googleapis/genai-toolbox) is an open source MCP server for databases. It was designed with enterprise-grade and production-quality in mind. It enables you to develop tools easier, faster, and more securely by handling the complexities such as connection pooling, authentication, and more.\n\n"
  }
,
  {
    "path": "python/integrations/tools/writer.mdx",
    "filename": "writer.mdx",
    "size_bytes": 12347,
    "line_count": 274,
    "preview": "---\ntitle: WRITER Tools\n---\n\nThis guide provides an overview for getting started with WRITER [tools](/oss/langchain/tools). For detailed documentation of all WRITER features and configurations, head to the [WRITER docs](https://dev.writer.com/home).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/tools/brightdata_unlocker.mdx",
    "filename": "brightdata_unlocker.mdx",
    "size_bytes": 5137,
    "line_count": 152,
    "preview": "---\ntitle: BrightDataUnlocker\n---\n\n[Bright Data](https://brightdata.com/) provides a powerful Web Unlocker API that allows you to access websites that might be protected by anti-bot measures, geo-restrictions, or other access limitations, making it particularly useful for AI agents requiring reliable web content extraction.\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/tools/tableau.mdx",
    "filename": "tableau.mdx",
    "size_bytes": 6772,
    "line_count": 151,
    "preview": "---\ntitle: Tableau\n---\n\nThis guide provides a quick overview for getting started with [Tableau](https://help.tableau.com/current/api/vizql-data-service/en-us/index.html).\n\n### Overview\n\nTableau's VizQL Data Service (aka VDS) provides developers with programmatic access to their Tableau Published Data Sources, allowing them to extend their business semantics for any custom workload or application, including AI Agents. The simple_datasource_qa tool adds VDS to the LangChain framework. This notebook shows you how you can use it to build agents that answer analytical questions grounded on your enterprise semantic models.\n\n"
  }
,
  {
    "path": "python/integrations/tools/databricks.mdx",
    "filename": "databricks.mdx",
    "size_bytes": 4722,
    "line_count": 130,
    "preview": "---\ntitle: Databricks Unity Catalog (UC)\n---\n\nThis notebook shows how to use UC functions as LangChain tools, with both LangChain and LangGraph agent APIs.\n\nSee Databricks documentation ([AWS](https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html)|[Azure](https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/sql-ref-syntax-ddl-create-sql-function)|[GCP](https://docs.gcp.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html)) to learn how to create SQL or Python functions in UC. Do not skip function and parameter comments, which are critical for LLMs to call functions properly.\n\nIn this example notebook, we create a simple Python function that executes arbitrary code and use it as a LangChain tool:\n\n"
  }
,
  {
    "path": "python/integrations/tools/e2b_data_analysis.mdx",
    "filename": "e2b_data_analysis.mdx",
    "size_bytes": 9530,
    "line_count": 214,
    "preview": "---\ntitle: E2B Data Analysis\n---\n\n[E2B's cloud environments](https://e2b.dev) are great runtime sandboxes for LLMs.\n\nE2B's Data Analysis sandbox allows for safe code execution in a sandboxed environment. This is ideal for building tools such as code interpreters, or Advanced Data Analysis like in ChatGPT.\n\nE2B Data Analysis sandbox allows you to:\n\n"
  }
,
  {
    "path": "python/integrations/tools/wolfram_alpha.mdx",
    "filename": "wolfram_alpha.mdx",
    "size_bytes": 803,
    "line_count": 45,
    "preview": "---\ntitle: Wolfram Alpha\n---\n\nThis notebook goes over how to use the wolfram alpha component.\n\nFirst, you need to set up your Wolfram Alpha developer account and get your APP ID:\n\n1. Go to wolfram alpha and sign up for a developer account [here](https://developer.wolframalpha.com/)\n2. Create an app and get your APP ID\n"
  }
,
  {
    "path": "python/integrations/tools/drasi.mdx",
    "filename": "drasi.mdx",
    "size_bytes": 8715,
    "line_count": 265,
    "preview": "---\ntitle: Drasi\ndescription: Connect agents to real-time data changes with Drasi's continuous query platform\n---\n\nThis guide provides a quick overview for getting started with the Drasi tool. For a detailed listing of all Drasi features, parameters, and configurations, head to the [Drasi documentation](https://drasi.io/), and the [langchain_drasi](https://github.com/drasi-project/langchain-drasi) repository.\n\n## Overview\n\nDrasi is a change detection platform that makes it easy and efficient to detect and react to changes in databases. The LangChain-Drasi integration creates reactive, change-driven AI agents by connecting external data changes with workflow execution. This allows agents to discover, subscribe to, and react to real-time query updates by bridging external data changes with agentic workflows. Drasi continuous queries stream real-time updates that trigger agent state transitions, modify memory, or dynamically control workflow execution—transforming static agents into ambient long-lived, responsive systems.\n"
  }
,
  {
    "path": "python/integrations/tools/apify_actors.mdx",
    "filename": "apify_actors.mdx",
    "size_bytes": 8968,
    "line_count": 210,
    "preview": "---\ntitle: Apify\n---\n\n>[Apify Actors](https://docs.apify.com/platform/actors) are cloud programs designed for a wide range of web scraping, crawling, and data extraction tasks. These actors facilitate automated data gathering from the web, enabling users to extract, process, and store information efficiently. Actors can be used to perform tasks like scraping e-commerce sites for product details, monitoring price changes, or gathering search engine results. They integrate seamlessly with [Apify Datasets](https://docs.apify.com/platform/storage/dataset), allowing the structured data collected by actors to be stored, managed, and exported in formats like JSON, CSV, or Excel for further analysis or use.\n\n## Overview\n\nThis notebook walks you through using [Apify Actors](https://docs.apify.com/platform/actors) with LangChain to automate web scraping and data extraction. The `langchain-apify` package integrates Apify's cloud-based tools with LangChain agents, enabling efficient data collection and processing for AI applications.\n\n"
  }
,
  {
    "path": "python/integrations/tools/financial_datasets.mdx",
    "filename": "financial_datasets.mdx",
    "size_bytes": 4281,
    "line_count": 141,
    "preview": "---\ntitle: FinancialDatasets Toolkit\n---\n\nThe [financial datasets](https://financialdatasets.ai/) stock market API provides REST endpoints that let you get financial data for 16,000+ tickers spanning 30+ years.\n\n## Setup\n\nTo use this toolkit, you need two API keys:\n\n"
  }
,
  {
    "path": "python/integrations/tools/office365.mdx",
    "filename": "office365.mdx",
    "size_bytes": 6251,
    "line_count": 113,
    "preview": "---\ntitle: Office365 Toolkit\n---\n\n>[Microsoft 365](https://www.office.com/) is a product family of productivity software, collaboration and cloud-based services owned by `Microsoft`.\n>\n>Note: `Office 365` was rebranded as `Microsoft 365`.\n\nThis notebook walks through connecting LangChain to `Office365` email and calendar.\n\n"
  }
,
  {
    "path": "python/integrations/tools/scrapeless_universal_scraping.mdx",
    "filename": "scrapeless_universal_scraping.mdx",
    "size_bytes": 9271,
    "line_count": 232,
    "preview": "---\ntitle: Scrapeless Universal Scraping\n---\n\n**Scrapeless** offers flexible and feature-rich data acquisition services with extensive parameter customization and multi-format export support. These capabilities empower LangChain to integrate and leverage external data more effectively. The core functional modules include:\n\n**DeepSerp**\n\n- **Google Search**: Enables comprehensive extraction of Google SERP data across all result types.\n  - Supports selection of localized Google domains (e.g., `google.com`, `google.ad`) to retrieve region-specific search results.\n"
  }
,
  {
    "path": "python/integrations/tools/google_lens.mdx",
    "filename": "google_lens.mdx",
    "size_bytes": 62281,
    "line_count": 546,
    "preview": "---\ntitle: Google Lens\n---\n\nThis notebook goes over how to use the Google Lens Tool to fetch information on an image.\n\nFirst, you need to sign up for an `SerpApi key` key at: [serpapi.com/users/sign_up](https://serpapi.com/users/sign_up).\n\nThen you must install `requests` with the command:\n\n"
  }
,
  {
    "path": "python/integrations/tools/bedrock_agentcore_browser.mdx",
    "filename": "bedrock_agentcore_browser.mdx",
    "size_bytes": 8242,
    "line_count": 277,
    "preview": "---\ntitle: Amazon Bedrock AgentCore Browser\n---\n\n[Amazon Bedrock AgentCore Browser](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/browser-tool.html) enables agents to interact with web pages through a managed Chrome browser. Agents can navigate websites, extract content, fill forms, click elements, and take screenshots in a secure, managed environment.\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/tools/ibm_watsonx.mdx",
    "filename": "ibm_watsonx.mdx",
    "size_bytes": 13450,
    "line_count": 307,
    "preview": "---\ntitle: IBM watsonx.ai\n---\n\n>`WatsonxToolkit` is a wrapper for IBM [watsonx.ai](https://www.ibm.com/products/watsonx-ai) Toolkit.\n\nThis example shows how to use `watsonx.ai` Toolkit using `LangChain`.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/tools/google_trends.mdx",
    "filename": "google_trends.mdx",
    "size_bytes": 2627,
    "line_count": 48,
    "preview": "---\ntitle: Google Trends\n---\n\nThis notebook goes over how to use the Google Trends Tool to fetch trends information.\n\nFirst, you need to sign up for an `SerpApi key` key at: [serpapi.com/users/sign_up](https://serpapi.com/users/sign_up).\n\nThen you must install `google-search-results` with the command:\n\n"
  }
,
  {
    "path": "python/integrations/tools/connery.mdx",
    "filename": "connery.mdx",
    "size_bytes": 8372,
    "line_count": 182,
    "preview": "---\ntitle: Connery Toolkit and Tools\n---\n\nUsing the Connery toolkit and tools, you can integrate Connery Actions into your LangChain agent.\n\n## What is connery?\n\nConnery is an open-source plugin infrastructure for AI.\n\n"
  }
,
  {
    "path": "python/integrations/tools/openapi_nla.mdx",
    "filename": "openapi_nla.mdx",
    "size_bytes": 10995,
    "line_count": 209,
    "preview": "---\ntitle: Natural Language API Toolkits\n---\n\n`Natural Language API` Toolkits (`NLAToolkits`) permit LangChain Agents to efficiently plan and combine calls across endpoints.\n\nThis notebook demonstrates a sample composition of the `Speak`, `Klarna`, and `Spoonacluar` APIs.\n\n### First, import dependencies and load the LLM\n\n"
  }
,
  {
    "path": "python/integrations/tools/you.mdx",
    "filename": "you.mdx",
    "size_bytes": 19478,
    "line_count": 134,
    "preview": "---\ntitle: You.com Search\n---\n\nThe [you.com API](https://api.you.com) is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.\n\n## Setup\n\nThe tool lives in the `langchain-community` package.\n\n"
  }
,
  {
    "path": "python/integrations/tools/requests.mdx",
    "filename": "requests.mdx",
    "size_bytes": 7958,
    "line_count": 212,
    "preview": "---\ntitle: Requests Toolkit\n---\n\nWe can use the Requests [toolkit](/oss/langchain/tools#toolkits) to construct agents that generate HTTP requests.\n\nFor detailed documentation of all API toolkit features and configurations head to the API reference for [RequestsToolkit](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.openapi.toolkit.RequestsToolkit.html).\n\n## ⚠️ Security note ⚠️\n\n"
  }
,
  {
    "path": "python/integrations/tools/salesforce.mdx",
    "filename": "salesforce.mdx",
    "size_bytes": 5545,
    "line_count": 212,
    "preview": "---\ntitle: Salesforce\n---\n\nA tool for interacting with Salesforce CRM using LangChain.\n\n## Overview\n\nThe `langchain-salesforce` package integrates LangChain with Salesforce CRM,\nallowing you to query data, manage records, and explore object schemas\n"
  }
,
  {
    "path": "python/integrations/tools/scraperapi.mdx",
    "filename": "scraperapi.mdx",
    "size_bytes": 5517,
    "line_count": 193,
    "preview": "---\ntitle: ScraperAPI\n---\n\nGive your AI agent the ability to browse websites, search Google and Amazon in just two lines of code.\n\nThe `langchain-scraperapi` package adds three ready-to-use LangChain tools backed by the [ScraperAPI](https://www.scraperapi.com/) service:\n\n| Tool class | Use it to |\n|------------|------------------|\n"
  }
,
  {
    "path": "python/integrations/tools/brave_search.mdx",
    "filename": "brave_search.mdx",
    "size_bytes": 2712,
    "line_count": 43,
    "preview": "---\ntitle: Brave Search\n---\n\nThis notebook goes over how to use the Brave Search tool.\nGo to the [Brave Website](https://brave.com/search/api/) to sign up for a free account and get an API key.\n\n```python\npip install -qU langchain-community\n```\n"
  }
,
  {
    "path": "python/integrations/tools/linkup_search.mdx",
    "filename": "linkup_search.mdx",
    "size_bytes": 17818,
    "line_count": 146,
    "preview": "---\ntitle: LinkupSearchTool\n---\n\n> [Linkup](https://www.linkup.so/) provides an API to connect LLMs to the web and the Linkup Premium Partner sources.\n\nThis guide provides a quick overview for getting started with LinkupSearchTool [tool](/oss/langchain/tools). For detailed documentation of all LinkupSearchTool features and configurations head to the [API reference](https://python.langchain.com/api_reference/linkup/tools/linkup_langchain.search_tool.LinkupSearchTool.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/tools/jira.mdx",
    "filename": "jira.mdx",
    "size_bytes": 6149,
    "line_count": 109,
    "preview": "---\ntitle: Jira Toolkit\n---\n\nThis notebook goes over how to use the `Jira` toolkit.\n\nThe `Jira` toolkit allows agents to interact with a given Jira instance, performing actions such as searching for issues and creating issues, the tool wraps the atlassian-python-api library, for more see: [atlassian-python-api.readthedocs.io/jira.html](https://atlassian-python-api.readthedocs.io/jira.html)\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/tools/scrapeless_scraping_api.mdx",
    "filename": "scrapeless_scraping_api.mdx",
    "size_bytes": 74109,
    "line_count": 321,
    "preview": "---\ntitle: Scrapeless Scraping API\n---\n\n**Scrapeless** offers flexible and feature-rich data acquisition services with extensive parameter customization and multi-format export support. These capabilities empower LangChain to integrate and leverage external data more effectively. The core functional modules include:\n\n**DeepSerp**\n\n- **Google Search**: Enables comprehensive extraction of Google SERP data across all result types.\n  - Supports selection of localized Google domains (e.g., `google.com`, `google.ad`) to retrieve region-specific search results.\n"
  }
,
  {
    "path": "python/integrations/tools/google_finance.mdx",
    "filename": "google_finance.mdx",
    "size_bytes": 2911,
    "line_count": 109,
    "preview": "---\ntitle: Google Finance\n---\n\nThis notebook goes over how to use the Google Finance Tool to get information from the Google Finance page.\n\nTo get an SerpApi key key, sign up at: [serpapi.com/users/sign_up](https://serpapi.com/users/sign_up).\n\nTo use the tool with LangChain install following packages\n\n"
  }
,
  {
    "path": "python/integrations/tools/golden_query.mdx",
    "filename": "golden_query.mdx",
    "size_bytes": 3033,
    "line_count": 83,
    "preview": "---\ntitle: Golden Query\n---\n\n>[Golden](https://golden.com) provides a set of natural language APIs for querying and enrichment using the Golden Knowledge Graph e.g. queries such as: `Products from OpenAI`, `Generative ai companies with series a funding`, and `rappers who invest` can be used to retrieve structured data about relevant entities.\n>\n>The `golden-query` langchain tool is a wrapper on top of the [Golden Query API](https://docs.golden.com/reference/query-api) which enables programmatic access to these results.\n>See the [Golden Query API docs](https://docs.golden.com/reference/query-api) for more information.\n\nThis notebook goes over how to use the `golden-query` tool.\n"
  }
,
  {
    "path": "python/integrations/tools/multion.mdx",
    "filename": "multion.mdx",
    "size_bytes": 4243,
    "line_count": 117,
    "preview": "---\ntitle: MultiOn Toolkit\n---\n\n[MultiON](https://www.multion.ai/blog/multion-building-a-brighter-future-for-humanity-with-ai-agents) has built an AI Agent that can interact with a broad array of web services and applications.\n\nThis notebook walks you through connecting LangChain to the `MultiOn` Client in your browser.\n\nThis enables custom agentic workflow that utilize the power of MultiON agents.\n\n"
  }
,
  {
    "path": "python/integrations/tools/pubmed.mdx",
    "filename": "pubmed.mdx",
    "size_bytes": 2596,
    "line_count": 31,
    "preview": "---\ntitle: PubMed\n---\n\n>[PubMed®](https://pubmed.ncbi.nlm.nih.gov/) comprises more than 35 million citations for biomedical literature from `MEDLINE`, life science journals, and online books. Citations may include links to full text content from PubMed Central and publisher web sites.\n\nThis notebook goes over how to use `PubMed` as a tool.\n\n```python\npip install xmltodict\n"
  }
,
  {
    "path": "python/integrations/tools/goat.mdx",
    "filename": "goat.mdx",
    "size_bytes": 4981,
    "line_count": 164,
    "preview": "---\ntitle: GOAT\n---\n\n[GOAT](https://github.com/goat-sdk/goat) is the finance toolkit for AI agents.\n\n## Overview\n\nCreate agents that can:\n\n"
  }
,
  {
    "path": "python/integrations/tools/dataforseo.mdx",
    "filename": "dataforseo.mdx",
    "size_bytes": 4085,
    "line_count": 129,
    "preview": "---\ntitle: DataForSEO\n---\n\n>[DataForSeo](https://dataforseo.com/) provides comprehensive SEO and digital marketing data solutions via API.\n>\n>The `DataForSeo API` retrieves `SERP` from the most popular search engines like `Google`, `Bing`, `Yahoo`. It also allows to >get SERPs from different search engine types like `Maps`, `News`, `Events`, etc.\n\nThis notebook demonstrates how to use the [DataForSeo API](https://dataforseo.com/apis) to obtain search engine results.\n\n"
  }
,
  {
    "path": "python/integrations/tools/google_cloud_texttospeech.mdx",
    "filename": "google_cloud_texttospeech.mdx",
    "size_bytes": 1275,
    "line_count": 40,
    "preview": "---\ntitle: Google Cloud Text-to-Speech\n---\n\n>[Google Cloud Text-to-Speech](https://cloud.google.com/text-to-speech) enables developers to synthesize natural-sounding speech with 100+ voices, available in multiple languages and variants. It applies DeepMind’s groundbreaking research in WaveNet and Google’s powerful neural networks to deliver the highest fidelity possible.\n>\n>It supports multiple languages, including English, German, Polish, Spanish, Italian, French, Portuguese, and Hindi.\n\nThis notebook shows how to interact with the `Google Cloud Text-to-Speech API` to achieve speech synthesis capabilities.\n\n"
  }
,
  {
    "path": "python/integrations/tools/robocorp.mdx",
    "filename": "robocorp.mdx",
    "size_bytes": 3634,
    "line_count": 119,
    "preview": "---\ntitle: Robocorp Toolkit\n---\n\nThis notebook covers how to get started with [Robocorp Action Server](https://github.com/robocorp/robocorp) action toolkit and LangChain.\n\nRobocorp is the easiest way to extend the capabilities of AI agents, assistants and copilots with custom actions.\n\n## Installation\n\n"
  }
,
  {
    "path": "python/integrations/tools/bing_search.mdx",
    "filename": "bing_search.mdx",
    "size_bytes": 12889,
    "line_count": 181,
    "preview": "---\ntitle: Bing Search\n---\n\n> [Bing Search](https://learn.microsoft.com/en-us/bing/search-apis/bing-web-search/) is an Azure service and enables safe, ad-free, location-aware search results, surfacing relevant information from billions of web documents. Help your users find what they're looking for from the world-wide-web by harnessing Bing's ability to comb billions of webpages, images, videos, and news with a single API call.\n\n## Setup\n\nFollowing the [instruction](https://learn.microsoft.com/en-us/bing/search-apis/bing-web-search/create-bing-search-service-resource) to create Azure Bing Search v7 service, and get the subscription key\n\n"
  }
,
  {
    "path": "python/integrations/tools/memgraph.mdx",
    "filename": "memgraph.mdx",
    "size_bytes": 2236,
    "line_count": 91,
    "preview": "---\ntitle: MemgraphToolkit\n---\n\nThis will help you get started with the Memgraph [toolkit](/oss/langchain/tools#toolkits).\n\nTools within `MemgraphToolkit` are designed for the interaction with the `Memgraph` database.\n\n## Setup\n\n"
  }
,
  {
    "path": "python/integrations/tools/playwright.mdx",
    "filename": "playwright.mdx",
    "size_bytes": 22262,
    "line_count": 218,
    "preview": "---\ntitle: PlayWright Browser Toolkit\n---\n\n>[Playwright](https://github.com/microsoft/playwright) is an open-source automation tool developed by `Microsoft` that allows you to programmatically control and automate web browsers. It is designed for end-to-end testing, scraping, and automating tasks across various web browsers such as `Chromium`, `Firefox`, and `WebKit`.\n\nThis toolkit is used to interact with the browser. While other tools (like the `Requests` tools) are fine for static sites, `PlayWright Browser` toolkits let your agent navigate the web and interact with dynamically rendered sites.\n\nSome tools bundled within the `PlayWright Browser` toolkit include:\n\n"
  }
,
  {
    "path": "python/integrations/tools/google_serper.mdx",
    "filename": "google_serper.mdx",
    "size_bytes": 36334,
    "line_count": 664,
    "preview": "---\ntitle: Google Serper\n---\n\nThis notebook goes over how to use the `Google Serper` component to search the web. First you need to sign up for a free account at [serper.dev](https://serper.dev) and get your api key.\n\n```python\npip install -qU  langchain-community langchain-openai\n```\n\n"
  }
,
  {
    "path": "python/integrations/tools/dalle_image_generator.mdx",
    "filename": "dalle_image_generator.mdx",
    "size_bytes": 2716,
    "line_count": 102,
    "preview": "---\ntitle: Dall-E Image Generator\n---\n\n>[OpenAI Dall-E](https://openai.com/dall-e-3) are text-to-image models developed by OpenAI using deep learning methodologies to generate digital images from natural language descriptions, called \"prompts\".\n\nThis notebook shows how you can generate images from a prompt synthesized using an OpenAI LLM. The images are generated using `Dall-E`, which uses the same OpenAI API key as the LLM.\n\n```python\npip install -qU  opencv-python scikit-image langchain-community\n"
  }
,
  {
    "path": "python/integrations/tools/clickup.mdx",
    "filename": "clickup.mdx",
    "size_bytes": 27021,
    "line_count": 577,
    "preview": "---\ntitle: ClickUp Toolkit\n---\n\n>[ClickUp](https://clickup.com/) is an all-in-one productivity platform that provides small and large teams across industries with flexible and customizable work management solutions, tools, and functions.\n\n>It is a cloud-based project management solution for businesses of all sizes featuring communication and collaboration tools to help achieve organizational goals.\n\n```python\npip install -qU langchain-community\n"
  }
,
  {
    "path": "python/integrations/tools/twilio.mdx",
    "filename": "twilio.mdx",
    "size_bytes": 1803,
    "line_count": 57,
    "preview": "---\ntitle: Twilio\n---\n\nThis notebook goes over how to use the [Twilio](https://www.twilio.com) API wrapper to send a message through SMS or [Twilio Messaging Channels](https://www.twilio.com/docs/messaging/channels).\n\nTwilio Messaging Channels facilitates integrations with 3rd party messaging apps and lets you send messages through WhatsApp Business Platform (GA), Facebook Messenger (Public Beta) and Google Business Messages (Private Beta).\n\n## Setup\n\n"
  }
,
  {
    "path": "python/integrations/tools/tilores.mdx",
    "filename": "tilores.mdx",
    "size_bytes": 7626,
    "line_count": 191,
    "preview": "---\ntitle: Tilores\n---\n\nThis notebook covers how to get started with the [Tilores](/oss/integrations/providers/tilores) tools.\nFor a more complex example you can checkout our [customer insights chatbot example](https://github.com/tilotech/identity-rag-customer-insights-chatbot).\n\n## Overview\n\n### Integration details\n"
  }
,
  {
    "path": "python/integrations/tools/valyu_search.mdx",
    "filename": "valyu_search.mdx",
    "size_bytes": 3837,
    "line_count": 116,
    "preview": "---\ntitle: ValyuContext\n---\n\n>[Valyu](https://www.valyu.network/) allows AI applications and agents to search the internet and proprietary data sources for relevant LLM ready information.\n\nThis notebook goes over how to use Valyu context tool in LangChain.\n\nFirst, get an Valyu API key and add it as an environment variable. Get $10 free credit  by [signing up here](https://platform.valyu.network/).\n\n"
  }
,
  {
    "path": "python/integrations/tools/arxiv.mdx",
    "filename": "arxiv.mdx",
    "size_bytes": 5505,
    "line_count": 111,
    "preview": "---\ntitle: ArXiv\n---\n\nThis notebook goes over how to use the `arxiv` tool with an agent.\n\nFirst, you need to install the `arxiv` python package.\n\n```python\npip install -qU  langchain-community arxiv\n"
  }
,
  {
    "path": "python/integrations/tools/ainetwork.mdx",
    "filename": "ainetwork.mdx",
    "size_bytes": 9386,
    "line_count": 235,
    "preview": "---\ntitle: AINetwork Toolkit\n---\n\n>[AI Network](https://www.ainetwork.ai/build-on-ain) is a layer 1 blockchain designed to accommodate large-scale AI models, utilizing a decentralized GPU network powered by the [$AIN token](https://www.ainetwork.ai/token), enriching AI-driven `NFTs` (`AINFTs`).\n>\n>The `AINetwork Toolkit` is a set of tools for interacting with the [AINetwork Blockchain](https://www.ainetwork.ai/public/whitepaper.pdf). These tools allow you to transfer `AIN`, read and write values, create apps, and set permissions for specific paths within the blockchain database.\n\n## Installing dependencies\n\n"
  }
,
  {
    "path": "python/integrations/tools/stripe.mdx",
    "filename": "stripe.mdx",
    "size_bytes": 2708,
    "line_count": 91,
    "preview": "---\ntitle: StripeAgentToolkit\n---\n\nThis guide provides a quick overview for getting started with Stripe's agent toolkit.\n\nYou can read more about `StripeAgentToolkit` in [Stripe's launch blog](https://stripe.dev/blog/adding-payments-to-your-agentic-workflows) or on the project's [PyPi page](https://pypi.org/project/stripe-agent-toolkit/).\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/tools/openweathermap.mdx",
    "filename": "openweathermap.mdx",
    "size_bytes": 2538,
    "line_count": 112,
    "preview": "---\ntitle: OpenWeatherMap\n---\n\nThis notebook goes over how to use the `OpenWeatherMap` component to fetch weather information.\n\nFirst, you need to sign up for an `OpenWeatherMap API` key:\n\n1. Go to OpenWeatherMap and sign up for an API key [here](https://openweathermap.org/api/)\n2. pip install pyowm\n"
  }
,
  {
    "path": "python/integrations/tools/scrapegraph.mdx",
    "filename": "scrapegraph.mdx",
    "size_bytes": 10315,
    "line_count": 258,
    "preview": "---\ntitle: ScrapeGraph\n---\n\nThis guide provides a quick overview for getting started with ScrapeGraph [tools](/oss/integrations/tools/). For detailed documentation of all ScrapeGraph features and configurations head to the [API reference](https://python.langchain.com/docs/integrations/tools/scrapegraph).\n\nFor more information about ScrapeGraph AI:\n\n- [ScrapeGraph AI Website](https://scrapegraphai.com)\n- [Open Source Project](https://github.com/ScrapeGraphAI/Scrapegraph-ai)\n"
  }
,
  {
    "path": "python/integrations/tools/reddit_search.mdx",
    "filename": "reddit_search.mdx",
    "size_bytes": 6356,
    "line_count": 158,
    "preview": "---\ntitle: Reddit Search\n---\n\nIn this notebook, we learn how the Reddit search tool works.\nFirst make sure that you have installed praw with the command below:\n\n```python\npip install -qU  praw\n```\n"
  }
,
  {
    "path": "python/integrations/tools/ibm_watsonx_sql.mdx",
    "filename": "ibm_watsonx_sql.mdx",
    "size_bytes": 10742,
    "line_count": 232,
    "preview": "---\ntitle: IBM watsonx.ai\n---\n\nThis example shows how to use `langchain-ibm` `watsonx.ai` SQL Database Toolkit that uses Flight service.\n\n<Warning>\nBuilding Q&A systems of SQL databases requires executing model-generated SQL queries, which carries inherent security risks. Make sure that your database connection permissions are always scoped as narrowly as possible for your agent's needs. This will mitigate, though not eliminate, the risks of building a model-driven system.\n</Warning>\n\n"
  }
,
  {
    "path": "python/integrations/tools/tavily_extract.mdx",
    "filename": "tavily_extract.mdx",
    "size_bytes": 252185,
    "line_count": 190,
    "preview": "---\ntitle: Tavily Extract\n---\n\n[Tavily](https://tavily.com) is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed. Tavily offers an [Extract](https://docs.tavily.com/api-reference/endpoint/extract) endpoint that can be used to extract content from a URLs.\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/tools/stackexchange.mdx",
    "filename": "stackexchange.mdx",
    "size_bytes": 943,
    "line_count": 23,
    "preview": "---\ntitle: StackExchange\n---\n\n>[Stack Exchange](https://stackexchange.com/) is a network of question-and-answer (Q&A) websites on topics in diverse fields, each site covering a specific topic, where questions, answers, and users are subject to a reputation award process. The reputation system allows the sites to be self-moderating.\n\nThe `StackExchange` component integrates the StackExchange API into LangChain allowing access to the [StackOverflow](https://stackoverflow.com/) site of the Stack Excchange network. Stack Overflow focuses on computer programming.\n\nThis notebook goes over how to use the `StackExchange` component.\n\n"
  }
,
  {
    "path": "python/integrations/tools/upstage_groundedness_check.mdx",
    "filename": "upstage_groundedness_check.mdx",
    "size_bytes": 1115,
    "line_count": 47,
    "preview": "---\ntitle: Upstage Groundedness Check\n---\n\nThis notebook covers how to get started with Upstage groundedness check models.\n\n## Installation\n\nInstall `langchain-upstage` package.\n\n"
  }
,
  {
    "path": "python/integrations/tools/jenkins.mdx",
    "filename": "jenkins.mdx",
    "size_bytes": 1731,
    "line_count": 93,
    "preview": "---\ntitle: Jenkins\n---\n\nTools for interacting with [Jenkins](https://www.jenkins.io/).\n\n## Overview\n\nThe `langchain-jenkins` package allows you to execute and control CI/CD pipelines with\nJenkins.\n"
  }
,
  {
    "path": "python/integrations/tools/nuclia.mdx",
    "filename": "nuclia.mdx",
    "size_bytes": 3678,
    "line_count": 88,
    "preview": "---\ntitle: Nuclia Understanding\n---\n\n>[Nuclia](https://nuclia.com) automatically indexes your unstructured data from any internal and external source, providing optimized search results and generative answers. It can handle video and audio transcription, image content extraction, and document parsing.\n\nThe `Nuclia Understanding API` supports the processing of unstructured data, including text, web pages, documents, and audio/video contents. It extracts all texts wherever it is (using speech-to-text or OCR when needed), it identifies entities, it also extracts metadata, embedded files (like images in a PDF), and web links. It also provides a summary of the content.\n\nTo use the `Nuclia Understanding API`, you need to have a `Nuclia` account. You can create one for free at [https://nuclia.cloud](https://nuclia.cloud), and then [create a NUA key](https://docs.nuclia.dev/docs/docs/using/understanding/intro).\n\n"
  }
,
  {
    "path": "python/integrations/tools/openapi.mdx",
    "filename": "openapi.mdx",
    "size_bytes": 28804,
    "line_count": 540,
    "preview": "---\ntitle: OpenAPI Toolkit\n---\n\nWe can construct agents to consume arbitrary APIs, here APIs conformant to the `OpenAPI`/`Swagger` specification.\n\n```python\n# NOTE: In this example. We must set `allow_dangerous_request=True` to enable the OpenAPI Agent to automatically use the Request Tool.\n# This can be dangerous for calling unwanted requests. Please make sure your custom OpenAPI spec (yaml) is safe.\nALLOW_DANGEROUS_REQUEST = True\n"
  }
,
  {
    "path": "python/integrations/tools/oxylabs.mdx",
    "filename": "oxylabs.mdx",
    "size_bytes": 4093,
    "line_count": 153,
    "preview": "---\ntitle: Oxylabs\n---\n\n>[Oxylabs](https://oxylabs.io/) is a market-leading web intelligence collection platform, driven by the highest business, ethics, and compliance standards, enabling companies worldwide to unlock data-driven insights.\n\n## Overview\n\nThis package contains the LangChain integration with Oxylabs, providing tools to scrape Google search results with Oxylabs Web Scraper API using LangChain's framework.\n\n"
  }
,
  {
    "path": "python/integrations/tools/valthera.mdx",
    "filename": "valthera.mdx",
    "size_bytes": 10952,
    "line_count": 353,
    "preview": "---\ntitle: Valthera\n---\n\nEnable AI agents to engage users when they're most likely to respond.\n\n## Overview\n\nValthera is an open-source framework that enables LLM Agents to engage users in a more meaningful way. It is built on BJ Fogg's Behavior Model (B=MAT) and leverages data from multiple sources (such as HubSpot, PostHog, and Snowflake) to assess a user's **motivation** and **ability** before triggering an action.\n\n"
  }
,
  {
    "path": "python/integrations/tools/youtube.mdx",
    "filename": "youtube.mdx",
    "size_bytes": 1080,
    "line_count": 41,
    "preview": "---\ntitle: YouTube\n---\n\n> [YouTube Search](https://github.com/joetats/youtube_search) package searches `YouTube` videos avoiding using their heavily rate-limited API.\n>\n> It uses the form on the `YouTube` homepage and scrapes the resulting page.\n\nThis notebook shows how to use a tool to search YouTube.\n\n"
  }
,
  {
    "path": "python/integrations/tools/bedrock_agentcore_code_interpreter.mdx",
    "filename": "bedrock_agentcore_code_interpreter.mdx",
    "size_bytes": 7235,
    "line_count": 248,
    "preview": "---\ntitle: Amazon Bedrock AgentCore Code Interpreter\n---\n\n[Amazon Bedrock AgentCore Code Interpreter](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/code-interpreter-tool.html) enables agents to execute code in secure, managed sandbox environments. Agents can run Python, JavaScript, and TypeScript code for calculations, data analysis, file manipulation, and visualizations.\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/tools/huggingface_tools.mdx",
    "filename": "huggingface_tools.mdx",
    "size_bytes": 963,
    "line_count": 39,
    "preview": "---\ntitle: HuggingFace Hub Tools\n---\n\n>[Huggingface Tools](https://huggingface.co/docs/transformers/v4.29.0/en/custom_tools) that supporting text I/O can be\nloaded directly using the `load_huggingface_tool` function.\n\n```python\n# Requires transformers>=4.29.0 and huggingface_hub>=0.14.1\npip install -qU  transformers huggingface_hub > /dev/null\n"
  }
,
  {
    "path": "python/integrations/tools/google_imagen.mdx",
    "filename": "google_imagen.mdx",
    "size_bytes": 6685780,
    "line_count": 197,
    "preview": "---\ntitle: Google Imagen\n---\n\n>[Imagen on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/image/overview) brings Google's state of the art image generative AI capabilities to application developers. With Imagen on Vertex AI, application developers can build next-generation AI products that transform their user's imagination into high quality visual assets using AI generation, in seconds.\n\nWith Imagen on LangChain , You can do the following tasks\n\n- [VertexAIImageGeneratorChat](#image-generation) : Generate novel images using only a text prompt (text-to-image AI generation).\n- [VertexAIImageEditorChat](#image-editing) : Edit an entire uploaded or generated image with a text prompt.\n"
  }
,
  {
    "path": "python/integrations/tools/brightdata-webscraperapi.mdx",
    "filename": "brightdata-webscraperapi.mdx",
    "size_bytes": 7971,
    "line_count": 242,
    "preview": "---\ntitle: BrightDataWebScraperAPI\ndescription: Extract structured data from 44 popular domains using Bright Data's Web Scraper API\n---\n\n[Bright Data](https://brightdata.com/) provides a powerful Web Scraper API that allows you to extract structured data from 44 popular domains, including e-commerce sites (Amazon, Walmart, eBay), social media (LinkedIn, Instagram, TikTok, Facebook), and more, making it particularly useful for AI agents requiring reliable structured web data feeds.\n\n## Overview\n\n### Integration details\n"
  }
,
  {
    "path": "python/integrations/tools/spark_sql.mdx",
    "filename": "spark_sql.mdx",
    "size_bytes": 10891,
    "line_count": 228,
    "preview": "---\ntitle: Spark SQL Toolkit\n---\n\nThis notebook shows how to use agents to interact with `Spark SQL`. Similar to [SQL Database Agent](/oss/integrations/tools/sql_database), it is designed to address general inquiries about `Spark SQL` and facilitate error recovery.\n\n**NOTE: Note that, as this agent is in active development, all answers might not be correct. Additionally, it is not guaranteed that the agent won't perform DML statements on your Spark cluster given certain questions. Be careful running it on sensitive data!**\n\n## Initialization\n\n"
  }
,
  {
    "path": "python/integrations/tools/compass.mdx",
    "filename": "compass.mdx",
    "size_bytes": 5318,
    "line_count": 176,
    "preview": "---\ntitle: Compass DeFi Toolkit\n---\n\nInteract with various DeFi protocols. Non-custodial.Tools return *unsigned transactions*. The toolkit is built on top of a Universal DeFi API ([Compass API](https://api.compasslabs.ai/)) allowing agents to perform financial operations like:\n\n- **Swapping tokens** on Uniswap and Aerodrome\n- **Lending** or **borrowing** assets using protocols on Aave\n- **Providing liquidity** on Aerodrome and Uniswap\n- **Transferring funds** between wallets.\n"
  }
,
  {
    "path": "python/integrations/tools/naver_search.mdx",
    "filename": "naver_search.mdx",
    "size_bytes": 4853,
    "line_count": 135,
    "preview": "---\ntitle: Naver Search\n---\n\nThe Naver Search Tool provides a simple interface to search Naver and get results.\n\n### Integration details\n\n| Class | Package | Serializable | JS support |  Version |\n| :--- | :--- | :---: | :---: | :---: |\n"
  }
,
  {
    "path": "python/integrations/tools/opengradient_toolkit.mdx",
    "filename": "opengradient_toolkit.mdx",
    "size_bytes": 6331,
    "line_count": 205,
    "preview": "---\ntitle: OpenGradientToolkit\n---\n\nThis notebook shows how to build tools using the OpenGradient toolkit. This toolkit gives users the ability to create custom tools based on models and workflows on the [OpenGradient network](https://www.opengradient.ai/).\n\n## Setup\n\nEnsure that you have an OpenGradient API key in order to access the OpenGradient network. If you already have an API key, simply set the environment variable:\n\n"
  }
,
  {
    "path": "python/integrations/tools/sql_database.mdx",
    "filename": "sql_database.mdx",
    "size_bytes": 16915,
    "line_count": 415,
    "preview": "---\ntitle: SQLDatabase Toolkit\n---\n\nThis will help you get started with the SQL Database [toolkit](/oss/langchain/tools#toolkits). For detailed documentation of all `SQLDatabaseToolkit` features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.sql.toolkit.SQLDatabaseToolkit.html).\n\nTools within the `SQLDatabaseToolkit` are designed to interact with a `SQL` database.\n\nA common application is to enable agents to answer questions using data in a relational database, potentially in an iterative fashion (e.g., recovering from errors).\n\n"
  }
,
  {
    "path": "python/integrations/tools/cassandra_database.mdx",
    "filename": "cassandra_database.mdx",
    "size_bytes": 13125,
    "line_count": 389,
    "preview": "---\ntitle: Cassandra Database Toolkit\n---\n\n>`Apache Cassandra®` is a widely used database for storing transactional application data. The introduction of functions and >tooling in Large Language Models has opened up some exciting use cases for existing data in Generative AI applications.\n\n>The `Cassandra Database` toolkit enables AI engineers to integrate agents with Cassandra data efficiently, offering\n>the following features:\n>\n> - Fast data access through optimized queries. Most queries should run in single-digit ms or less.\n"
  }
,
  {
    "path": "python/integrations/tools/prolog_tool.mdx",
    "filename": "prolog_tool.mdx",
    "size_bytes": 3584,
    "line_count": 161,
    "preview": "---\ntitle: Prolog\n---\n\nLangChain tools that use Prolog rules to generate answers.\n\n## Overview\n\nThe PrologTool class allows the generation of langchain tools that use Prolog rules to generate answers.\n\n"
  }
,
  {
    "path": "python/integrations/tools/ionic_shopping.mdx",
    "filename": "ionic_shopping.mdx",
    "size_bytes": 3233,
    "line_count": 95,
    "preview": "---\ntitle: Ionic Shopping Tool\n---\n\n[Ionic](https://www.ioniccommerce.com/) is a plug and play ecommerce marketplace for AI Assistants. By including the [Ionic Tool](https://github.com/ioniccommerce/ionic_langchain) in your agent, you are effortlessly providing your users with the ability to shop and transact directly within your agent, and you'll get a cut of the transaction.\n\nThis is a basic jupyter notebook demonstrating how to integrate the Ionic Tool into your agent. For more information on setting up your Agent with Ionic, see the Ionic [documentation](https://docs.ioniccommerce.com/introduction).\n\nThis Jupyter Notebook demonstrates how to use the Ionic tool with an Agent.\n\n"
  }
,
  {
    "path": "python/integrations/tools/infobip.mdx",
    "filename": "infobip.mdx",
    "size_bytes": 4026,
    "line_count": 109,
    "preview": "---\ntitle: Infobip\n---\n\nThis notebook that shows how to use [Infobip](https://www.infobip.com/) API wrapper to send SMS messages, emails.\n\nInfobip provides many services, but this notebook will focus on SMS and Email services. You can find more information about the API and other channels [here](https://www.infobip.com/docs/api).\n\n## Setup\n\n"
  }
,
  {
    "path": "python/integrations/tools/eleven_labs_tts.mdx",
    "filename": "eleven_labs_tts.mdx",
    "size_bytes": 1786,
    "line_count": 99,
    "preview": "---\ntitle: ElevenLabs Text2Speech\n---\n\nThis notebook shows how to interact with the `ElevenLabs API` to achieve text-to-speech capabilities.\n\nFirst, you need to set up an ElevenLabs account. You can follow the instructions [here](https://docs.elevenlabs.io/welcome/introduction).\n\n```python\npip install -qU  elevenlabs langchain-community\n"
  }
,
  {
    "path": "python/integrations/tools/scrapeless_crawl.mdx",
    "filename": "scrapeless_crawl.mdx",
    "size_bytes": 26445,
    "line_count": 308,
    "preview": "---\ntitle: Scrapeless Crawl\n---\n\n[**Scrapeless**](https://www.scrapeless.com/) offers flexible and feature-rich data acquisition services with extensive parameter customization and multi-format export support. These capabilities empower LangChain to integrate and leverage external data more effectively. The core functional modules include:\n\n**DeepSerp**\n\n- **Google Search**: Enables comprehensive extraction of Google SERP data across all result types.\n  - Supports selection of localized Google domains (e.g., `google.com`, `google.ad`) to retrieve region-specific search results.\n"
  }
,
  {
    "path": "python/integrations/tools/chatgpt_plugins.mdx",
    "filename": "chatgpt_plugins.mdx",
    "size_bytes": 5605,
    "line_count": 70,
    "preview": "---\ntitle: ChatGPT Plugins\n---\n\n<Warning>\n**Deprecated**\n\nOpenAI has [deprecated plugins](https://openai.com/index/chatgpt-plugins/).\n\n</Warning>\n"
  }
,
  {
    "path": "python/integrations/tools/wikidata.mdx",
    "filename": "wikidata.mdx",
    "size_bytes": 2231,
    "line_count": 56,
    "preview": "---\ntitle: Wikidata\n---\n\n>[Wikidata](https://wikidata.org/) is a free and open knowledge base that can be read and edited by both humans and machines. Wikidata is one of the world's largest open knowledge bases.\n\nFirst, you need to install `wikibase-rest-api-client` and `mediawikiapi` python packages.\n\n```python\npip install -qU wikibase-rest-api-client mediawikiapi\n"
  }
,
  {
    "path": "python/integrations/tools/memorize.mdx",
    "filename": "memorize.mdx",
    "size_bytes": 2843,
    "line_count": 96,
    "preview": "---\ntitle: Memorize\n---\n\nFine-tuning LLM itself to memorize information using unsupervised learning.\n\nThis tool requires LLMs that support fine-tuning. Currently, only `langchain.llms import GradientLLM` is supported.\n\n## Imports\n\n"
  }
,
  {
    "path": "python/integrations/tools/discord.mdx",
    "filename": "discord.mdx",
    "size_bytes": 5282,
    "line_count": 145,
    "preview": "---\ntitle: Discord\n---\n\nThis guide provides a quick overview for getting started with Discord tooling in [langchain_discord](/oss/integrations/tools/). For more details on each tool and configuration, see the docstrings in your repository or relevant doc pages.\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/tools/passio_nutrition_ai.mdx",
    "filename": "passio_nutrition_ai.mdx",
    "size_bytes": 4893,
    "line_count": 162,
    "preview": "---\ntitle: Passio NutritionAI\n---\n\nTo best understand how NutritionAI can give your agents super food-nutrition powers, let's build an agent that can find that information via Passio NutritionAI.\n\n## Define tools\n\nWe first need to create [the Passio NutritionAI tool](/oss/integrations/tools/passio_nutrition_ai).\n\n"
  }
,
  {
    "path": "python/integrations/tools/sceneXplain.mdx",
    "filename": "sceneXplain.mdx",
    "size_bytes": 3126,
    "line_count": 69,
    "preview": "---\ntitle: SceneXplain\n---\n\n[SceneXplain](https://scenex.jina.ai/) is an ImageCaptioning service accessible through the SceneXplain Tool.\n\nTo use this tool, you'll need to make an account and fetch your API Token [from the website](https://scenex.jina.ai/api). Then you can instantiate the tool.\n\n```python\nimport os\n"
  }
,
  {
    "path": "python/integrations/tools/lemonai.mdx",
    "filename": "lemonai.mdx",
    "size_bytes": 5629,
    "line_count": 101,
    "preview": "---\ntitle: Lemon Agent\n---\n\n>[Lemon Agent](https://github.com/felixbrock/lemon-agent) helps you build powerful AI assistants in minutes and automate workflows by allowing for accurate and reliable read and write operations in tools like `Airtable`, `Hubspot`, `Discord`, `Notion`, `Slack` and `GitHub`.\n\nSee [full docs here](https://github.com/felixbrock/lemonai-py-client).\n\nMost connectors available today are focused on read-only operations, limiting the potential of LLMs. Agents, on the other hand, have a tendency to hallucinate from time to time due to missing context or instructions.\n\n"
  }
,
  {
    "path": "python/integrations/tools/ifttt.mdx",
    "filename": "ifttt.mdx",
    "size_bytes": 2389,
    "line_count": 73,
    "preview": "---\ntitle: IFTTT WebHooks\n---\n\nThis notebook shows how to use IFTTT Webhooks.\n\nFrom [github.com/SidU/teams-langchain-js/wiki/Connecting-IFTTT-Services](https://github.com/SidU/teams-langchain-js/wiki/Connecting-IFTTT-Services).\n\n## Creating a webhook\n\n"
  }
,
  {
    "path": "python/integrations/tools/permit.mdx",
    "filename": "permit.mdx",
    "size_bytes": 4962,
    "line_count": 194,
    "preview": "---\ntitle: Permit\n---\n\nPermit is an access control platform that provides fine-grained, real-time permission management using various models such as RBAC, ABAC, and ReBAC. It enables organizations to enforce dynamic policies across their applications, ensuring that only authorized users can access specific resources.\n\n## Overview\n\nThis package provides two LangChain tools for JWT validation and permission checking using Permit:\n\n"
  }
,
  {
    "path": "python/integrations/tools/slack.mdx",
    "filename": "slack.mdx",
    "size_bytes": 4853,
    "line_count": 148,
    "preview": "---\ntitle: Slack Toolkit\n---\n\nThis will help you get started with the Slack [toolkit](/oss/langchain/tools#toolkits). For detailed documentation of all SlackToolkit features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.slack.toolkit.SlackToolkit.html).\n\n## Setup\n\nTo use this toolkit, you will need to get a token as explained in the [Slack API docs](https://api.slack.com/tutorials/tracks/getting-a-token). Once you've received a SLACK_USER_TOKEN, you can input it as an environment variable below.\n\n"
  }
,
  {
    "path": "python/integrations/tools/cdp_agentkit.mdx",
    "filename": "cdp_agentkit.mdx",
    "size_bytes": 4616,
    "line_count": 171,
    "preview": "---\ntitle: CDP Agentkit Toolkit\n---\n\nThe `CDP Agentkit` toolkit contains tools that enable an LLM agent to interact with the [Coinbase Developer Platform](https://docs.cdp.coinbase.com/). The toolkit provides a wrapper around the CDP SDK, allowing agents to perform onchain operations like transfers, trades, and smart contract interactions.\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/tools/tavily_search.mdx",
    "filename": "tavily_search.mdx",
    "size_bytes": 11619,
    "line_count": 191,
    "preview": "---\ntitle: Tavily Search\n---\n\n[Tavily's Search API](https://tavily.com) is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/tools/google_gmail.mdx",
    "filename": "google_gmail.mdx",
    "size_bytes": 5404,
    "line_count": 146,
    "preview": "---\ntitle: Gmail Toolkit\n---\n\nThis will help you get started with the Gmail [toolkit](/oss/langchain/tools#toolkits). This toolkit interacts with the Gmail API to read messages, draft and send messages, and more. For detailed documentation of all GmailToolkit features and configurations head to the [API reference](https://python.langchain.com/api_reference/google_community/gmail/langchain_google_community.gmail.toolkit.GmailToolkit.html).\n\n## Setup\n\nTo use this toolkit, you will need to set up your credentials explained in the [Gmail API docs](https://developers.google.com/gmail/api/quickstart/python#authorize_credentials_for_a_desktop_application). Once you've downloaded the `credentials.json` file, you can start using the Gmail API.\n\n"
  }
,
  {
    "path": "python/integrations/tools/asknews.mdx",
    "filename": "asknews.mdx",
    "size_bytes": 6863,
    "line_count": 78,
    "preview": "---\ntitle: AskNews\n---\n\n> [AskNews](https://asknews.app) infuses any LLM with the latest global news (or historical news), using a single natural language query. Specifically, AskNews is enriching over 300k articles per day by translating, summarizing, extracting entities, and indexing them into hot and cold vector databases. AskNews puts these vector databases on a low-latency endpoint for you. When you query AskNews, you get back a prompt-optimized string that contains all the most pertinent enrichments (e.g. entities, classifications, translation, summarization). This means that you do not need to manage your own news RAG, and you do not need to worry about how to properly convey news information in a condensed way to your LLM.\n> AskNews is also committed to transparency, which is why our coverage is monitored and diversified across hundreds of countries, 13 languages, and 50 thousand sources. If you'd like to track our source coverage, you can visit our [transparency dashboard](https://asknews.app/en/transparency).\n\n## Setup\n\nThe integration lives in the `langchain-community` package. We also need to install the `asknews` package itself.\n"
  }
,
  {
    "path": "python/integrations/tools/graphql.mdx",
    "filename": "graphql.mdx",
    "size_bytes": 3815,
    "line_count": 129,
    "preview": "---\ntitle: GraphQL\n---\n\n>[GraphQL](https://graphql.org/) is a query language for APIs and a runtime for executing those queries against your data. `GraphQL` provides a complete and understandable description of the data in your API, gives clients the power to ask for exactly what they need and nothing more, makes it easier to evolve APIs over time, and enables powerful developer tools.\n\nBy including a `BaseGraphQLTool` in the list of tools provided to an Agent, you can grant your Agent the ability to query data from GraphQL APIs for any purposes you need.\n\nThis Jupyter Notebook demonstrates how to use the `GraphQLAPIWrapper` component with an Agent.\n\n"
  }
,
  {
    "path": "python/integrations/tools/hyperbrowser_browser_agent_tools.mdx",
    "filename": "hyperbrowser_browser_agent_tools.mdx",
    "size_bytes": 10522,
    "line_count": 250,
    "preview": "---\ntitle: Hyperbrowser Browser Agent Tools\n---\n\n[Hyperbrowser](https://hyperbrowser.ai) is a platform for running, running browser agents, and scaling headless browsers. It lets you launch and manage browser sessions at scale and provides easy to use solutions for any webscraping needs, such as scraping a single page or crawling an entire site.\n\nKey Features:\n\n- Instant Scalability - Spin up hundreds of browser sessions in seconds without infrastructure headaches\n- Simple Integration - Works seamlessly with popular tools like Puppeteer and Playwright\n"
  }
,
  {
    "path": "python/integrations/tools/google_search.mdx",
    "filename": "google_search.mdx",
    "size_bytes": 3679,
    "line_count": 92,
    "preview": "---\ntitle: Google Search\n---\n\nThis notebook goes over how to use the google search component.\n\nFirst, you need to set up the proper API keys and environment variables. To set it up, create the GOOGLE_API_KEY in the Google Cloud credential console ([console.cloud.google.com/apis/credentials](https://console.cloud.google.com/apis/credentials)) and a GOOGLE_CSE_ID using the Programmable Search Engine ([programmablesearchengine.google.com/controlpanel/create](https://programmablesearchengine.google.com/controlpanel/create)). Next, it is good to follow the instructions found [here](https://stackoverflow.com/questions/37083058/programmatically-searching-google-in-python-using-custom-search).\n\nThen we will need to set some environment variables.\n\n"
  }
,
  {
    "path": "python/integrations/tools/nasa.mdx",
    "filename": "nasa.mdx",
    "size_bytes": 1428,
    "line_count": 48,
    "preview": "---\ntitle: NASA Toolkit\n---\n\nThis notebook shows how to use agents to interact with the NASA toolkit. The toolkit provides access to the NASA Image and Video Library API, with potential to expand and include other accessible NASA APIs in future iterations.\n\n**Note: NASA Image and Video Library search queries can result in large responses when the number of desired media results is not specified. Consider this prior to using the agent with LLM token credits.**\n\n## Example use\n\n"
  }
,
  {
    "path": "python/integrations/tools/fmp-data.mdx",
    "filename": "fmp-data.mdx",
    "size_bytes": 5904,
    "line_count": 258,
    "preview": "---\ntitle: FMP Data\n---\n\nAccess financial market data through natural language queries.\n\n## Overview\n\nThe FMP (Financial Modeling Prep) LangChain integration provides a seamless way to access financial market data through natural language queries. This integration offers two main components:\n\n"
  }
,
  {
    "path": "python/integrations/tools/mojeek_search.mdx",
    "filename": "mojeek_search.mdx",
    "size_bytes": 629,
    "line_count": 23,
    "preview": "---\ntitle: Mojeek Search\n---\n\nThe following notebook will explain how to get results using Mojeek Search. Please visit [Mojeek Website](https://www.mojeek.com/services/search/web-search-api/) to obtain an API key.\n\n```python\nfrom langchain_community.tools import MojeekSearch\n```\n\n"
  }
,
  {
    "path": "python/integrations/tools/gradio_tools.mdx",
    "filename": "gradio_tools.mdx",
    "size_bytes": 4898,
    "line_count": 138,
    "preview": "---\ntitle: Gradio\n---\n\nThere are many 1000s of `Gradio` apps on `Hugging Face Spaces`. This library puts them at the tips of your LLM's fingers 🦾\n\nSpecifically, `gradio-tools` is a Python library for converting `Gradio` apps into tools that can be leveraged by a large language model (LLM)-based agent to complete its task. For example, an LLM could use a `Gradio` tool to transcribe a voice recording it finds online and then summarize it for you. Or it could use a different `Gradio` tool to apply OCR to a document on your Google Drive and then answer questions about it.\n\nIt's very easy to create you own tool if you want to use a space that's not one of the pre-built tools. Please see this section of the gradio-tools documentation for information on how to do that. All contributions are welcome!\n\n"
  }
,
  {
    "path": "python/integrations/tools/google_books.mdx",
    "filename": "google_books.mdx",
    "size_bytes": 14396,
    "line_count": 158,
    "preview": "---\ntitle: Google Books\n---\n\nThe Google Books tool that supports the ReAct pattern and allows you to search the Google Books API. Google Books is the largest API in the world that keeps track of books in a curated manner. It has over 40 million entries, which can give users a significant amount of data.\n\n### Tool features\n\nCurrently the tool has the following capabilities:\n\n"
  }
,
  {
    "path": "python/integrations/tools/parallel_search.mdx",
    "filename": "parallel_search.mdx",
    "size_bytes": 11069,
    "line_count": 323,
    "preview": "---\ntitle: Parallel Search\n---\n\n>[Parallel](https://platform.parallel.ai/) is a real-time web search and content extraction platform designed specifically for LLMs and AI applications.\n\nThe `ParallelWebSearchTool` provides access to Parallel's Search API, which streamlines the traditional search → scrape → extract pipeline into a single API call, returning structured, LLM-optimized results.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/tools/jina_search.mdx",
    "filename": "jina_search.mdx",
    "size_bytes": 8117,
    "line_count": 142,
    "preview": "---\ntitle: Jina Search\n---\n\nThis guide provides a quick overview for getting started with Jina [tool](/oss/integrations/tools/). For detailed documentation of all Jina features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.jina_search.tool.JinaSearch.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/tools/google_jobs.mdx",
    "filename": "google_jobs.mdx",
    "size_bytes": 24409,
    "line_count": 221,
    "preview": "---\ntitle: Google Jobs\n---\n\nThis notebook goes over how to use the Google Jobs Tool to fetch current Job postings.\n\nFirst, you need to sign up for an `SerpApi key` key at: [serpapi.com/users/sign_up](https://serpapi.com/users/sign_up) and get your api key here: [serpapi.com/manage-api-key](https://serpapi.com/manage-api-key).\n\nThen you must install `google-search-results` with the command:\n    `pip install google-search-results`\n"
  }
,
  {
    "path": "python/integrations/tools/azure_cognitive_services.mdx",
    "filename": "azure_cognitive_services.mdx",
    "size_bytes": 8849,
    "line_count": 198,
    "preview": "---\ntitle: Azure Cognitive Services Toolkit\n---\n\nThis toolkit is used to interact with the `Azure Cognitive Services API` to achieve some multimodal capabilities.\n\nCurrently There are four tools bundled in this toolkit:\n\n- AzureCogsImageAnalysisTool: used to extract caption, objects, tags, and text from images. (Note: this tool is not available on Mac OS yet, due to the dependency on `azure-ai-vision` package, which is only supported on Windows and Linux currently.)\n- AzureCogsFormRecognizerTool: used to extract text, tables, and key-value pairs from documents.\n"
  }
,
  {
    "path": "python/integrations/tools/riza.mdx",
    "filename": "riza.mdx",
    "size_bytes": 2407,
    "line_count": 81,
    "preview": "---\ntitle: Riza Code Interpreter\n---\n\n> The Riza Code Interpreter is a WASM-based isolated environment for running Python or JavaScript generated by AI agents.\n\nIn this notebook we'll create an example of an agent that uses Python to solve a problem that an LLM can't solve on its own:\ncounting the number of 'r's in the word \"strawberry.\"\n\nBefore you get started grab an API key from the [Riza dashboard](https://dashboard.riza.io). For more guides and a full API reference\n"
  }
,
  {
    "path": "python/integrations/tools/google_calendar.mdx",
    "filename": "google_calendar.mdx",
    "size_bytes": 7976,
    "line_count": 201,
    "preview": "---\ntitle: Google Calendar Toolkit\n---\n\n> [Google Calendar](https://workspace.google.com/intl/en-419/products/calendar/) is a product of Google Workspace that allows users to organize their schedules and events. It is a cloud-based calendar that allows users to create, edit, and delete events. It also allows users to share their calendars with others.\n\n## Overview\n\nThis notebook will help you get started with the Google Calendar Toolkit. This toolkit interacts with the Google Calendar API to perform various operations on the calendar. It allows you to:\n\n"
  }
,
  {
    "path": "python/integrations/tools/dataherald.mdx",
    "filename": "dataherald.mdx",
    "size_bytes": 872,
    "line_count": 42,
    "preview": "---\ntitle: Dataherald\n---\n\nThis notebook goes over how to use the dataherald component.\n\nFirst, you need to set up your Dataherald account and get your API KEY:\n\n1. Go to dataherald and sign up [here](https://www.dataherald.com/)\n2. Once you are logged in your Admin Console, create an API KEY\n"
  }
,
  {
    "path": "python/integrations/chat/cloudflare_workersai.mdx",
    "filename": "cloudflare_workersai.mdx",
    "size_bytes": 6397,
    "line_count": 179,
    "preview": "---\ntitle: ChatCloudflareWorkersAI\n---\n\nThis will help you get started with CloudflareWorkersAI [chat models](/oss/langchain/models). For detailed documentation of all ChatCloudflareWorkersAI features and configurations head to the [API reference](https://python.langchain.com/docs/integrations/chat/cloudflare_workersai/).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/chat/dappier.mdx",
    "filename": "dappier.mdx",
    "size_bytes": 2671,
    "line_count": 64,
    "preview": "---\ntitle: Dappier AI\n---\n\n**Dappier: Powering AI with Dynamic, Real-Time Data Models**\n\nDappier offers a cutting-edge platform that grants developers immediate access to a wide array of real-time data models spanning news, entertainment, finance, market data, weather, and beyond. With our pre-trained data models, you can supercharge your AI applications, ensuring they deliver precise, up-to-date responses and minimize inaccuracies.\n\nDappier data models help you build next-gen LLM apps with trusted, up-to-date content from the world's leading brands. Unleash your creativity and enhance any GPT App or AI workflow with actionable, proprietary, data through a simple API. Augment your AI with proprietary data from trusted sources is the best way to ensure factual, up-to-date, responses with fewer hallucinations no matter the question.\n\n"
  }
,
  {
    "path": "python/integrations/chat/gpt_router.mdx",
    "filename": "gpt_router.mdx",
    "size_bytes": 1840,
    "line_count": 73,
    "preview": "---\ntitle: GPTRouter\n---\n\n[GPTRouter](https://github.com/Writesonic/GPTRouter) is an open source LLM API Gateway that offers a universal API for 30+ LLMs, vision, and image models, with smart fallbacks based on uptime and latency, automatic retries, and streaming.\n\nThis notebook covers how to get started with using LangChain + the GPTRouter I/O library.\n\n* Set `GPT_ROUTER_API_KEY` environment variable\n* or use the `gpt_router_api_key` keyword argument\n"
  }
,
  {
    "path": "python/integrations/chat/baseten.mdx",
    "filename": "baseten.mdx",
    "size_bytes": 5440,
    "line_count": 134,
    "preview": "---\ntitle: ChatBaseten\n---\n\nThis guide provides a quick overview for getting started with the Baseten [chat model](/oss/langchain/models). For a detailed listing of all ChatBaseten features, parameters, and configurations, head to the [ChatBaseten API reference](https://python.langchain.com/api_reference/baseten/chat_models/langchain_baseten.chat_models.ChatBaseten.html).\n\nBaseten provides inference designed for production applications. Built on the Baseten Inference Stack, these APIs deliver enterprise-grade performance and reliability for leading open-source or custom models: https://www.baseten.co/library/.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/chat/llama_api.mdx",
    "filename": "llama_api.mdx",
    "size_bytes": 1447,
    "line_count": 59,
    "preview": "---\ntitle: ChatLlamaAPI\n---\n\nThis notebook shows how to use LangChain with [LlamaAPI](https://llama-api.com/) - a hosted version of Llama2 that adds in support for function calling.\n\npip install -qU  llamaapi\n\n```python\nfrom llamaapi import LlamaAPI\n"
  }
,
  {
    "path": "python/integrations/chat/volcengine_maas.mdx",
    "filename": "volcengine_maas.mdx",
    "size_bytes": 1204,
    "line_count": 52,
    "preview": "---\ntitle: VolcEngineMaasChat\n---\n\nThis guide provides you with a guide on how to get started with volc engine maas chat models.\n\n```python\n# Install the package\npip install -qU  volcengine\n```\n"
  }
,
  {
    "path": "python/integrations/chat/tencent_hunyuan.mdx",
    "filename": "tencent_hunyuan.mdx",
    "size_bytes": 1622,
    "line_count": 68,
    "preview": "---\ntitle: Tencent Hunyuan\n---\n\n>[Tencent's hybrid model API](https://cloud.tencent.com/document/product/1729) (`Hunyuan API`)\n> implements dialogue communication, content generation,\n> analysis and understanding, and can be widely used in various scenarios such as intelligent\n> customer service, intelligent marketing, role playing, advertising copywriting, product description,\n> script creation, resume generation, article writing, code generation, data analysis, and content\n> analysis.\n"
  }
,
  {
    "path": "python/integrations/chat/litellm.mdx",
    "filename": "litellm.mdx",
    "size_bytes": 7296,
    "line_count": 156,
    "preview": "---\ntitle: ChatLiteLLM and ChatLiteLLMRouter\n---\n\n[LiteLLM](https://github.com/BerriAI/litellm) is a library that simplifies calling Anthropic, Azure, Huggingface, Replicate, etc.\n\nThis notebook covers how to get started with using LangChain + the LiteLLM I/O library.\n\nThis integration contains two main classes:\n\n"
  }
,
  {
    "path": "python/integrations/chat/google_generative_ai.mdx",
    "filename": "google_generative_ai.mdx",
    "size_bytes": 48823,
    "line_count": 1475,
    "preview": "---\ntitle: ChatGoogleGenerativeAI\ndescription: Get started using Gemini [chat models](/oss/langchain/models) in LangChain.\n---\n\nAccess Google's Generative AI models, including the Gemini family, via the **Gemini Developer API** or **Vertex AI**. The Gemini Developer API offers quick setup with API keys, ideal for individual developers. Vertex AI provides enterprise features and integrates with Google Cloud Platform.\n\nFor information on the latest models, model IDs, their features, context windows, etc. head to the [Google AI docs](https://ai.google.dev/gemini-api/docs).\n\n<Note>\n"
  }
,
  {
    "path": "python/integrations/chat/mistralai.mdx",
    "filename": "mistralai.mdx",
    "size_bytes": 4200,
    "line_count": 97,
    "preview": "---\ntitle: ChatMistralAI\n---\n\nThis will help you get started with Mistral [chat models](/oss/langchain/models). For detailed documentation of all `ChatMistralAI` features and configurations head to the [API reference](https://python.langchain.com/api_reference/mistralai/chat_models/langchain_mistralai.chat_models.ChatMistralAI.html). The `ChatMistralAI` class is built on top of the [Mistral API](https://docs.mistral.ai/api/). For a list of all the models supported by Mistral, check out [this page](https://docs.mistral.ai/getting-started/models/).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/chat/coze.mdx",
    "filename": "coze.mdx",
    "size_bytes": 1628,
    "line_count": 65,
    "preview": "---\ntitle: Chat with Coze Bot\n---\n\nChatCoze chat models API by coze.com. For more information, see [https://www.coze.com/open/docs/chat](https://www.coze.com/open/docs/chat)\n\n```python\nfrom langchain_community.chat_models import ChatCoze\nfrom langchain.messages import HumanMessage\n```\n"
  }
,
  {
    "path": "python/integrations/chat/naver.mdx",
    "filename": "naver.mdx",
    "size_bytes": 13675,
    "line_count": 331,
    "preview": "---\ntitle: ChatClovaX\n---\n\nThis guide provides a quick overview for getting started with Naver’s HyperCLOVA X [chat models](https://python.langchain.com/docs/concepts/chat_models) via CLOVA Studio. For detailed documentation of all ChatClovaX features and configurations head to the [API reference](https://guide.ncloud-docs.com/docs/clovastudio-dev-langchain).\n\n[CLOVA Studio](http://clovastudio.ncloud.com/) has several chat models. You can find information about the latest models, including their costs, context windows, and supported input types, in the CLOVA Studio Guide [documentation](https://guide.ncloud-docs.com/docs/clovastudio-model).\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/chat/oci_generative_ai.mdx",
    "filename": "oci_generative_ai.mdx",
    "size_bytes": 3953,
    "line_count": 75,
    "preview": "---\ntitle: ChatOCIGenAI\n---\n\nThis guide provides a quick overview for getting started with OCIGenAI [chat models](/oss/langchain/models). For detailed documentation of all ChatOCIGenAI features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/chat_models/langchain_community.chat_models.oci_generative_ai.ChatOCIGenAI.html).\n\nOracle Cloud Infrastructure (OCI) Generative AI is a fully managed service that provides a set of state-of-the-art, customizable Large Language Models (LLMs) that cover a wide range of use cases, and which is available through a single API.\nUsing the OCI Generative AI service you can access ready-to-use pretrained models, or create and host your own fine-tuned custom models based on your own data on dedicated AI clusters. Detailed documentation of the service and API is available __[here](https://docs.oracle.com/en-us/iaas/Content/generative-ai/home.htm)__ and __[here](https://docs.oracle.com/en-us/iaas/api/#/en/generative-ai/20231130/)__.\n\n## Overview\n"
  }
,
  {
    "path": "python/integrations/chat/nebius.mdx",
    "filename": "nebius.mdx",
    "size_bytes": 22807,
    "line_count": 407,
    "preview": "---\ntitle: Nebius Chat Models\n---\n\nThis page will help you get started with Nebius AI Studio [chat models](/oss/langchain/models). For detailed documentation of all ChatNebius features and configurations head to the [API reference](https://python.langchain.com/api_reference/nebius/chat_models/langchain_nebius.chat_models.ChatNebius.html).\n\n[Nebius AI Studio](https://studio.nebius.ai/) provides API access to a wide range of state-of-the-art Large Language Models and embedding models for various use cases.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/chat/llama_edge.mdx",
    "filename": "llama_edge.mdx",
    "size_bytes": 2466,
    "line_count": 73,
    "preview": "---\ntitle: LlamaEdge\n---\n\n[LlamaEdge](https://github.com/second-state/LlamaEdge) allows you to chat with LLMs of [GGUF](https://github.com/ggerganov/llama.cpp/blob/master/gguf-py/README.md) format both locally and via chat service.\n\n- `LlamaEdgeChatService` provides developers an OpenAI API compatible service to chat with LLMs via HTTP requests.\n\n- `LlamaEdgeChatLocal` enables developers to chat with LLMs locally (coming soon).\n\n"
  }
,
  {
    "path": "python/integrations/chat/huggingface.mdx",
    "filename": "huggingface.mdx",
    "size_bytes": 8763,
    "line_count": 248,
    "preview": "---\ntitle: ChatHuggingFace\n---\n\nThis will help you get started with `langchain_huggingface` [chat models](/oss/langchain/models). For detailed documentation of all `ChatHuggingFace` features and configurations head to the [API reference](https://python.langchain.com/api_reference/huggingface/chat_models/langchain_huggingface.chat_models.huggingface.ChatHuggingFace.html). For a list of models supported by Hugging Face check out [this page](https://huggingface.co/models).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/chat/featherless_ai.mdx",
    "filename": "featherless_ai.mdx",
    "size_bytes": 4448,
    "line_count": 107,
    "preview": "---\ntitle: ChatFeatherlessAi\n---\n\nThis will help you get started with FeatherlessAi [chat models](/oss/langchain/models). For detailed documentation of all ChatFeatherlessAi features and configurations head to the [API reference](https://python.langchain.com/api_reference/__package_name_short_snake__/chat_models/__module_name__.chat_models.ChatFeatherlessAi.html).\n\n- See [featherless.ai/](https://featherless.ai/) for an example.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/chat/azure_chat_openai.mdx",
    "filename": "azure_chat_openai.mdx",
    "size_bytes": 8856,
    "line_count": 193,
    "preview": "---\ntitle: AzureChatOpenAI\ndescription: Get started using OpenAI [chat models](/oss/langchain/models) via Azure in LangChain.\n---\n\nYou can find information about Azure OpenAI's latest models and their costs, context windows, and supported input types in the [Azure docs](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models).\n\n<Info>\n    **Azure OpenAI vs OpenAI**\n\n"
  }
,
  {
    "path": "python/integrations/chat/xai.mdx",
    "filename": "xai.mdx",
    "size_bytes": 6481,
    "line_count": 175,
    "preview": "---\ntitle: ChatXAI\ndescription: Get started using xAI [chat models](/oss/langchain/models) in LangChain.\n---\n\n<Warning>\n    This page makes reference to Grok models provided by [xAI](https://docs.x.ai/docs/overview) - not to be confused with [Groq](https://console.groq.com/docs/overview), a separate AI hardware and software company. See the [Groq provider page](/oss/integrations/providers/groq).\n</Warning>\n\n[xAI](https://console.x.ai/) offers an API to interact with Grok models.\n"
  }
,
  {
    "path": "python/integrations/chat/aimlapi.mdx",
    "filename": "aimlapi.mdx",
    "size_bytes": 3813,
    "line_count": 106,
    "preview": "---\ntitle: ChatAimlapi\n---\n\nThis guide helps you get started with AI/ML API [chat models](/oss/langchain/models). For detailed documentation of all `ChatAimlapi` features and configurations, head to the [API reference](https://python.langchain.com/api_reference/aimlapi/chat_models/langchain_aimlapi.chat_models.ChatAimlapi.html).\n\n[AI/ML API](https://aimlapi.com/app/?utm_source=langchain&utm_medium=github&utm_campaign=integration) provides unified access to hundreds of hosted foundation models with high availability and throughput.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/chat/seekrflow.mdx",
    "filename": "seekrflow.mdx",
    "size_bytes": 6697,
    "line_count": 217,
    "preview": "---\ntitle: ChatSeekrFlow\n---\n\n> [Seekr](https://www.seekr.com/) provides AI-powered solutions for structured, explainable, and transparent AI interactions.\n\nThis guide provides a quick overview for getting started with Seekr [chat models](/oss/langchain/models). For detailed documentation of all `ChatSeekrFlow` features and configurations, head to the [API reference](https://python.langchain.com/api_reference/community/chat_models/langchain_community.chat_models.seekrflow.ChatSeekrFlow.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/chat/premai.mdx",
    "filename": "premai.mdx",
    "size_bytes": 16153,
    "line_count": 378,
    "preview": "---\ntitle: ChatPremAI\n---\n\n[PremAI](https://premai.io/) is an all-in-one platform that simplifies the creation of robust, production-ready applications powered by Generative AI. By streamlining the development process, PremAI allows you to concentrate on enhancing user experience and driving overall growth for your application. You can quickly start using our platform [here](https://docs.premai.io/quick-start).\n\nThis example goes over how to use LangChain to interact with different chat models with `ChatPremAI`\n\n### Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/chat/sparkllm.mdx",
    "filename": "sparkllm.mdx",
    "size_bytes": 1652,
    "line_count": 60,
    "preview": "---\ntitle: SparkLLM Chat\n---\n\nSparkLLM chat models API by iFlyTek. For more information, see [iFlyTek Open Platform](https://www.xfyun.cn/).\n\n## Basic use\n\n```python\n\"\"\"For basic init and call\"\"\"\n"
  }
,
  {
    "path": "python/integrations/chat/baidu_qianfan_endpoint.mdx",
    "filename": "baidu_qianfan_endpoint.mdx",
    "size_bytes": 3329,
    "line_count": 133,
    "preview": "---\ntitle: QianfanChatEndpoint\n---\n\nBaidu AI Cloud Qianfan Platform is a one-stop large model development and service operation platform for enterprise developers. Qianfan not only provides including the model of Wenxin Yiyan (ERNIE-Bot) and the third-party open-source models, but also provides various AI development tools and the whole set of development environment, which facilitates customers to use and develop large model applications easily.\n\nBasically, those model are split into the following type:\n\n- Embedding\n- Chat\n"
  }
,
  {
    "path": "python/integrations/chat/groq.mdx",
    "filename": "groq.mdx",
    "size_bytes": 5625,
    "line_count": 119,
    "preview": "---\ntitle: ChatGroq\nsidebarTitle: Groq\ndescription: Get started using Groq [chat models](/oss/langchain/models) in LangChain.\n---\n\n<Warning>\n    This page makes reference to [Groq](https://console.groq.com/docs/overview), an AI hardware and software company. For information on how to use Grok models (provided by [xAI](https://docs.x.ai/docs/overview)), see the [xAI provider page](/oss/integrations/providers/xai).\n</Warning>\n\n"
  }
,
  {
    "path": "python/integrations/chat/ai21.mdx",
    "filename": "ai21.mdx",
    "size_bytes": 5214,
    "line_count": 147,
    "preview": "---\ntitle: ChatAI21\n---\n\nThis notebook covers how to get started with AI21 chat models.\nNote that different chat models support different parameters. See the [AI21 documentation](https://docs.ai21.com/reference) to learn more about the parameters in your chosen model.\n[See all AI21's LangChain components.](https://pypi.org/project/langchain-ai21/)\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/chat/anthropic_functions.mdx",
    "filename": "anthropic_functions.mdx",
    "size_bytes": 1963,
    "line_count": 55,
    "preview": "---\ntitle: (Deprecated) Experimental Anthropic Tools Wrapper\n---\n\n<Warning>\n**The Anthropic API officially supports tool-calling so this workaround is no longer needed. Please use [ChatAnthropic](/oss/integrations/chat/anthropic) with `langchain-anthropic>=0.1.15`.**\n\n</Warning>\n\nThis notebook shows how to use an experimental wrapper around Anthropic that gives it tool calling and structured output capabilities. It follows Anthropic's guide [here](https://platform.claude.com/docs/en/agents-and-tools/tool-use/overview)\n"
  }
,
  {
    "path": "python/integrations/chat/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 15830,
    "line_count": 770,
    "preview": "---\ntitle: \"Chat models\"\nmode: wide\n---\n\n[Chat models](/oss/langchain/models) are language models that use a sequence of [messages](/oss/langchain/messages) as inputs and return messages as outputs <Tooltip tip=\"Older models that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output. These models typically do not include the prefix 'Chat' in their name or include 'LLM' as a suffix.\">(as opposed to traditional, plaintext LLMs)</Tooltip>.\n\n## Featured models\n\n<Info>\n"
  }
,
  {
    "path": "python/integrations/chat/modelscope_chat_endpoint.mdx",
    "filename": "modelscope_chat_endpoint.mdx",
    "size_bytes": 3409,
    "line_count": 91,
    "preview": "---\ntitle: ModelScopeChatEndpoint\n---\n\n\nModelScope ([Home](https://www.modelscope.cn/) | [GitHub](https://github.com/modelscope/modelscope)) is built upon the notion of “Model-as-a-Service” (MaaS). It seeks to bring together most advanced machine learning models from the AI community, and streamlines the process of leveraging AI models in real-world applications. The core ModelScope library open-sourced in this repository provides the interfaces and implementations that allow developers to perform model inference, training and evaluation.\n\nThis will help you get started with ModelScope Chat Endpoint.\n\n## Overview\n"
  }
,
  {
    "path": "python/integrations/chat/openai.mdx",
    "filename": "openai.mdx",
    "size_bytes": 45651,
    "line_count": 1453,
    "preview": "---\ntitle: ChatOpenAI\ndescription: Get started using OpenAI [chat models](/oss/langchain/models) in LangChain.\n---\n\nYou can find information about OpenAI's latest models, their costs, context windows, and supported input types in the [OpenAI Platform](https://platform.openai.com) docs.\n\n<Tip>\n    **API Reference**\n\n"
  }
,
  {
    "path": "python/integrations/chat/octoai.mdx",
    "filename": "octoai.mdx",
    "size_bytes": 2470,
    "line_count": 48,
    "preview": "---\ntitle: ChatOctoAI\n---\n\n[OctoAI](https://docs.octoai.cloud/docs) offers easy access to efficient compute and enables users to integrate their choice of AI models into applications. The `OctoAI` compute service helps you run, tune, and scale AI applications easily.\n\nThis notebook demonstrates the use of `langchain.chat_models.ChatOctoAI` for [OctoAI endpoints](https://octoai.cloud/text).\n\n## Setup\n\n"
  }
,
  {
    "path": "python/integrations/chat/google_anthropic_vertex.mdx",
    "filename": "google_anthropic_vertex.mdx",
    "size_bytes": 4604,
    "line_count": 104,
    "preview": "---\ntitle: ChatAnthropicVertex\ndescription: Get started using Anthropic [chat models](/oss/langchain/models) via Vertex AI in LangChain.\n---\n\n> [Anthropic Claude 3](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude) models on Vertex AI offer fully managed and serverless models as APIs. To use a Claude model on Vertex AI, send a request directly to the Vertex AI API endpoint. Because Anthropic Claude 3 models use a managed API, there's no need to provision or manage infrastructure.\n\nNOTE : Anthropic Models on Vertex are implemented as Chat Model through class `ChatAnthropicVertex`\n\n```python\n"
  }
,
  {
    "path": "python/integrations/chat/symblai_nebula.mdx",
    "filename": "symblai_nebula.mdx",
    "size_bytes": 2504,
    "line_count": 109,
    "preview": "---\ntitle: Nebula (Symbl.ai)\n---\n\nThis notebook covers how to get started with [Nebula](https://docs.symbl.ai/docs/nebula-llm) - Symbl.ai's chat model.\n\n### Integration details\n\nHead to the [API reference](https://docs.symbl.ai/reference/nebula-chat) for detailed documentation.\n\n"
  }
,
  {
    "path": "python/integrations/chat/writer.mdx",
    "filename": "writer.mdx",
    "size_bytes": 7794,
    "line_count": 228,
    "preview": "---\ntitle: ChatWriter\n---\n\nThis guide provides a quick overview for getting started with WRITER [chat](/oss/langchain/models/).\n\nWRITER has several chat models. You can find information about their latest models and their costs, context windows, and supported input types in the [WRITER docs](https://dev.writer.com/home).\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/chat/baichuan.mdx",
    "filename": "baichuan.mdx",
    "size_bytes": 1620,
    "line_count": 47,
    "preview": "---\ntitle: Chat with Baichuan-192K\n---\n\nBaichuan chat models API by Baichuan Intelligent Technology. For more information, see [https://platform.baichuan-ai.com/docs/api](https://platform.baichuan-ai.com/docs/api)\n\n```python\nfrom langchain_community.chat_models import ChatBaichuan\nfrom langchain.messages import HumanMessage\n```\n"
  }
,
  {
    "path": "python/integrations/chat/databricks.mdx",
    "filename": "databricks.mdx",
    "size_bytes": 14179,
    "line_count": 316,
    "preview": "---\ntitle: ChatDatabricks\n---\n\n> [Databricks](https://www.databricks.com/) Lakehouse Platform unifies data, analytics, and AI on one platform.\n\nThis guide provides a quick overview for getting started with Databricks [chat models](/oss/langchain/models). For detailed documentation of all ChatDatabricks features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/chat_models/langchain_community.chat_models.databricks.ChatDatabricks.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/chat/azure_ai.mdx",
    "filename": "azure_ai.mdx",
    "size_bytes": 4892,
    "line_count": 107,
    "preview": "---\ntitle: AzureAIChatCompletionsModel\n---\n\nThis will help you get started with AzureAIChatCompletionsModel [chat models](/oss/langchain/models). For detailed documentation of all AzureAIChatCompletionsModel features and configurations, head to the [API reference](https://python.langchain.com/api_reference/azure_ai/chat_models/langchain_azure_ai.chat_models.AzureAIChatCompletionsModel.html)\n\nThe AzureAIChatCompletionsModel class uses the Azure AI Foundry SDK. AI Foundry has several chat models, including AzureOpenAI, Cohere, Llama, Phi-3/4, and DeepSeek-R1, among others. You can find information about their latest models and their costs, context windows, and supported input types in the [Azure docs](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview).\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/chat/zhipuai.mdx",
    "filename": "zhipuai.mdx",
    "size_bytes": 3378,
    "line_count": 131,
    "preview": "---\ntitle: ZHIPU AI\n---\n\nThis notebook shows how to use [ZHIPU AI API](https://open.bigmodel.cn/dev/api) in LangChain with the langchain.chat_models.ChatZhipuAI.\n\n>[*GLM-4*](https://open.bigmodel.cn/) is a multi-lingual large language model aligned with human intent, featuring capabilities in Q&A, multi-turn dialogue, and code generation. The overall performance of the new generation base model GLM-4 has been significantly improved compared to the previous generation, supporting longer contexts; Stronger multimodality; Support faster inference speed, more concurrency, greatly reducing inference costs; Meanwhile, GLM-4 enhances the capabilities of intelligent agents.\n\n## Getting started\n\n"
  }
,
  {
    "path": "python/integrations/chat/bedrock.mdx",
    "filename": "bedrock.mdx",
    "size_bytes": 16718,
    "line_count": 364,
    "preview": "---\ntitle: ChatBedrock\n---\n\nThis doc will help you get started with AWS Bedrock [chat models](/oss/langchain/models). Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI. Using Amazon Bedrock, you can easily experiment with and evaluate top FMs for your use case, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources. Since Amazon Bedrock is serverless, you don't have to manage any infrastructure, and you can securely integrate and deploy generative AI capabilities into your applications using the AWS services you are already familiar with.\n\nAWS Bedrock maintains a [Converse API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html) which provides a unified conversational interface for Bedrock models. This API does not yet support custom models. You can see a list of all [models that are supported here](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html).\n\n<Info>\n**We recommend the Converse API for users who do not need to use custom models. It can be accessed using [ChatBedrockConverse](https://python.langchain.com/api_reference/aws/chat_models/langchain_aws.chat_models.bedrock_converse.ChatBedrockConverse.html).**\n"
  }
,
  {
    "path": "python/integrations/chat/upstage.mdx",
    "filename": "upstage.mdx",
    "size_bytes": 1095,
    "line_count": 60,
    "preview": "---\ntitle: ChatUpstage\n---\n\nThis notebook covers how to get started with Upstage chat models.\n\n## Installation\n\nInstall `langchain-upstage` package.\n\n"
  }
,
  {
    "path": "python/integrations/chat/ibm_watsonx.mdx",
    "filename": "ibm_watsonx.mdx",
    "size_bytes": 14085,
    "line_count": 312,
    "preview": "---\ntitle: ChatWatsonx\n---\n\n>ChatWatsonx is a wrapper for IBM [watsonx.ai](https://www.ibm.com/products/watsonx-ai) foundation models.\n\nThe aim of these examples is to show how to communicate with `watsonx.ai` models using `LangChain` LLMs API.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/chat/sambanova.mdx",
    "filename": "sambanova.mdx",
    "size_bytes": 11391,
    "line_count": 311,
    "preview": "---\ntitle: ChatSambaNova\n---\n\nThis will help you get started with SambaNova [chat models](/oss/langchain/models/). For detailed documentation of all `ChatSambaNova` features and configurations head to the [API reference](https://docs.sambanova.ai/cloud/docs/get-started/overview).\n\n**[SambaNova](https://sambanova.ai/)'s** [SambaCloud](http://cloud.sambanova.ai?utm_source=langchain&utm_medium=external&utm_campaign=cloud_signup) is a cloud platform for performing inference with open-source models\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/chat/snowflake.mdx",
    "filename": "snowflake.mdx",
    "size_bytes": 3398,
    "line_count": 101,
    "preview": "---\ntitle: Snowflake Cortex\n---\n\n[Snowflake Cortex](https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions) gives you instant access to industry-leading Large Language Models (LLMs) trained by researchers at companies like Mistral, Reka, Meta, and Google, including [Snowflake Arctic](https://www.snowflake.com/en/data-cloud/arctic/), an open enterprise-grade model developed by Snowflake.\n\nThis example goes over how to use LangChain to interact with Snowflake Cortex.\n\n### Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/chat/gradientai.mdx",
    "filename": "gradientai.mdx",
    "size_bytes": 9568,
    "line_count": 147,
    "preview": "---\ntitle: ChatGradient\n---\n\nThis will help you getting started with DigitalOcean Gradient Chat Models.\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/chat/fireworks.mdx",
    "filename": "fireworks.mdx",
    "size_bytes": 6703,
    "line_count": 176,
    "preview": "---\ntitle: ChatFireworks\n---\n\nThis doc helps you get started with Fireworks AI [chat models](/oss/langchain/models). For detailed documentation of all ChatFireworks features and configurations head to the [API reference](https://python.langchain.com/api_reference/fireworks/chat_models/langchain_fireworks.chat_models.ChatFireworks.html).\n\nFireworks AI is an AI inference platform to run and customize models. For a list of all models served by Fireworks see the [Fireworks docs](https://fireworks.ai/models).\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/chat/azureml_chat_endpoint.mdx",
    "filename": "azureml_chat_endpoint.mdx",
    "size_bytes": 5751,
    "line_count": 97,
    "preview": "---\ntitle: AzureMLChatOnlineEndpoint\n---\n\n>[Azure Machine Learning](https://azure.microsoft.com/en-us/products/machine-learning/) is a platform used to build, train, and deploy machine learning models. Users can explore the types of models to deploy in the Model Catalog, which provides foundational and general purpose models from different providers.\n>\n>In general, you need to deploy models in order to consume its predictions (inference). In `Azure Machine Learning`, [Online Endpoints](https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints) are used to deploy these models with a real-time serving. They are based on the ideas of `Endpoints` and `Deployments` which allow you to decouple the interface of your production workload from the implementation that serves it.\n\nThis notebook goes over how to use a chat model hosted on an `Azure Machine Learning Endpoint`.\n\n"
  }
,
  {
    "path": "python/integrations/chat/maritalk.mdx",
    "filename": "maritalk.mdx",
    "size_bytes": 5355,
    "line_count": 155,
    "preview": "---\n\ntitle: Maritalk\n---\n\n<a href=\"https://colab.research.google.com/github/langchain-ai/langchain/blob/v0.3/docs/docs/integrations/chat/maritalk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\nMariTalk is an assistant developed by the Brazilian company [Maritaca AI](https://www.maritaca.ai).\nMariTalk is based on language models that have been specially trained to understand Portuguese well.\n\n"
  }
,
  {
    "path": "python/integrations/chat/predictionguard.mdx",
    "filename": "predictionguard.mdx",
    "size_bytes": 8932,
    "line_count": 262,
    "preview": "---\ntitle: ChatPredictionGuard\n---\n\n>[Prediction Guard](https://predictionguard.com) is a secure, scalable GenAI platform that safeguards sensitive data, prevents common AI malfunctions, and runs on affordable hardware.\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/chat/deepseek.mdx",
    "filename": "deepseek.mdx",
    "size_bytes": 3586,
    "line_count": 103,
    "preview": "---\ntitle: ChatDeepSeek\ndescription: Get started using DeepSeek [chat models](/oss/langchain/models) in LangChain.\n---\n\nThis will help you get started with DeepSeek's hosted [chat models](/oss/langchain/models).\n\n<Tip>\n    **API Reference**\n\n"
  }
,
  {
    "path": "python/integrations/chat/amazon_nova.mdx",
    "filename": "amazon_nova.mdx",
    "size_bytes": 10418,
    "line_count": 358,
    "preview": "---\ntitle: ChatAmazonNova\ndescription: Get started using Amazon Nova [chat models](/oss/langchain/models) in LangChain.\n---\n\nThis guide provides a quick overview for getting started with Amazon Nova chat models. Amazon Nova models are OpenAI-compatible and accessed via the OpenAI SDK pointed at Nova's endpoint, providing seamless integration with LangChain's standard interfaces.\n\nYou can find information about Amazon Nova's models, their features, and API details in the [Amazon Nova documentation](https://nova.amazon.com/dev/documentation).\n\n<Tip>\n"
  }
,
  {
    "path": "python/integrations/chat/edenai.mdx",
    "filename": "edenai.mdx",
    "size_bytes": 7760,
    "line_count": 242,
    "preview": "---\ntitle: Eden AI\n---\n\nEden AI is revolutionizing the AI landscape by uniting the best AI providers, empowering users to unlock limitless possibilities and tap into the true potential of artificial intelligence. With an all-in-one comprehensive and hassle-free platform, it allows users to deploy AI features to production lightning fast, enabling effortless access to the full breadth of AI capabilities via a single API. (website: [edenai.co/](https://edenai.co/))\n\nThis example goes over how to use LangChain to interact with Eden AI models\n\n-----------------------------------------------------------------------------------\n\n"
  }
,
  {
    "path": "python/integrations/chat/minimax.mdx",
    "filename": "minimax.mdx",
    "size_bytes": 668,
    "line_count": 33,
    "preview": "---\ntitle: MiniMaxChat\n---\n\n[Minimax](https://api.minimax.chat) is a Chinese startup that provides LLM service for companies and individuals.\n\nThis example goes over how to use LangChain to interact with MiniMax Inference for Chat.\n\n```python\nimport os\n"
  }
,
  {
    "path": "python/integrations/chat/tongyi.mdx",
    "filename": "tongyi.mdx",
    "size_bytes": 9410,
    "line_count": 194,
    "preview": "---\ntitle: ChatTongyi\n---\nTongyi Qwen is a large language model developed by Alibaba's Damo Academy. It is capable of understanding user intent through natural language understanding and semantic analysis, based on user input in natural language. It provides services and assistance to users in different domains and tasks. By providing clear and detailed instructions, you can obtain results that better align with your expectations.\nIn this notebook, we will introduce how to use langchain with [Tongyi](https://www.aliyun.com/product/dashscope) mainly in `Chat` corresponding\n to the package `langchain/chat_models` in langchain\n\n```python\n# Install the package\npip install -qU  dashscope\n"
  }
,
  {
    "path": "python/integrations/chat/google_vertex_ai.mdx",
    "filename": "google_vertex_ai.mdx",
    "size_bytes": 5714,
    "line_count": 153,
    "preview": "---\ntitle: ChatVertexAI\ndescription: Get started using [chat models](/oss/langchain/models) via Vertex AI in LangChain.\n---\n\n<Danger>\n    **Deprecated**\n\n    This integration is deprecated and will be removed in a future release. Please use [`ChatGoogleGenerativeAI`](/oss/integrations/chat/google_generative_ai) instead. See the full [release notes and migration guide](https://github.com/langchain-ai/langchain-google/discussions/1422).\n</Danger>\n"
  }
,
  {
    "path": "python/integrations/chat/parallel.mdx",
    "filename": "parallel.mdx",
    "size_bytes": 7586,
    "line_count": 271,
    "preview": "---\ntitle: ChatParallelWeb\ndescription: Get started using Parallel [chat models](/oss/langchain/models) in LangChain.\n---\n\nParallel provides real-time web research capabilities through an OpenAI-compatible chat interface, allowing your AI applications to access current information from the web.\n\n<Tip>\n    **API Reference**\n\n"
  }
,
  {
    "path": "python/integrations/chat/reka.mdx",
    "filename": "reka.mdx",
    "size_bytes": 21944,
    "line_count": 302,
    "preview": "---\ntitle: ChatReka\n---\n\nThis guide provides a quick overview for getting started with Reka [chat models](/oss/langchain/models).\n\nReka has several chat models. You can find information about their latest models and their costs, context windows, and supported input types in the [Reka docs](https://docs.reka.ai/available-models).\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/chat/qwen.mdx",
    "filename": "qwen.mdx",
    "size_bytes": 7816,
    "line_count": 165,
    "preview": "---\ntitle: ChatQwen\n---\n\nThis will help you get started with Qwen [chat models](/oss/langchain/models). For detailed documentation of all ChatQwen features and configurations head to the [API reference](https://pypi.org/project/langchain-qwq/).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/chat/xinference.mdx",
    "filename": "xinference.mdx",
    "size_bytes": 3487,
    "line_count": 101,
    "preview": "---\ntitle: ChatXinference\n---\n\n[Xinference](https://github.com/xorbitsai/inference) is a powerful and versatile library designed to serve LLMs,\nspeech recognition models, and multimodal models, even on your laptop. It supports a variety of models compatible with GGML, such as chatglm, baichuan, whisper, vicuna, orca, and many others.\n\n## Overview\n\n### Integration details\n"
  }
,
  {
    "path": "python/integrations/chat/jinachat.mdx",
    "filename": "jinachat.mdx",
    "size_bytes": 1992,
    "line_count": 69,
    "preview": "---\ntitle: JinaChat\n---\n\nThis notebook covers how to get started with JinaChat chat models.\n\n```python\nfrom langchain_community.chat_models import JinaChat\nfrom langchain.messages import HumanMessage, SystemMessage\nfrom langchain_core.prompts.chat import (\n"
  }
,
  {
    "path": "python/integrations/chat/llamacpp.mdx",
    "filename": "llamacpp.mdx",
    "size_bytes": 7616,
    "line_count": 246,
    "preview": "---\ntitle: Llama.cpp\n---\n\n>[llama.cpp python](https://github.com/abetlen/llama-cpp-python) library is a simple Python bindings for `@ggerganov`\n>[llama.cpp](https://github.com/ggerganov/llama.cpp).\n>\n>This package provides:\n>\n> - Low-level access to C API via ctypes interface.\n"
  }
,
  {
    "path": "python/integrations/chat/yandex.mdx",
    "filename": "yandex.mdx",
    "size_bytes": 1650,
    "line_count": 50,
    "preview": "---\ntitle: ChatYandexGPT\n---\n\nThis notebook goes over how to use LangChain with [YandexGPT](https://cloud.yandex.com/en/services/yandexgpt) chat model.\n\nTo use, you should have the `yandexcloud` python package installed.\n\n```python\npip install -qU  yandexcloud\n"
  }
,
  {
    "path": "python/integrations/chat/konko.mdx",
    "filename": "konko.mdx",
    "size_bytes": 2998,
    "line_count": 55,
    "preview": "---\ntitle: ChatKonko\n---\n\n>[Konko](https://www.konko.ai/) API is a fully managed Web API designed to help application developers:\n\n1. **Select** the right open source or proprietary LLMs for their application\n2. **Build** applications faster with integrations to leading application frameworks and fully managed APIs\n3. **Fine tune** smaller open-source LLMs to achieve industry-leading performance at a fraction of the cost\n4. **Deploy production-scale APIs** that meet security, privacy, throughput, and latency SLAs without infrastructure set-up or administration using Konko AI's SOC 2 compliant, multi-cloud infrastructure\n"
  }
,
  {
    "path": "python/integrations/chat/runpod.mdx",
    "filename": "runpod.mdx",
    "size_bytes": 8424,
    "line_count": 189,
    "preview": "---\ntitle: RunPod Chat Model\n---\n\nGet started with RunPod chat models.\n\n## Overview\n\nThis guide covers how to use the LangChain `ChatRunPod` class to interact with chat models hosted on [RunPod Serverless](https://www.runpod.io/serverless-gpu).\n\n"
  }
,
  {
    "path": "python/integrations/chat/everlyai.mdx",
    "filename": "everlyai.mdx",
    "size_bytes": 3794,
    "line_count": 115,
    "preview": "---\ntitle: ChatEverlyAI\n---\n\n>[EverlyAI](https://everlyai.xyz) allows you to run your ML models at scale in the cloud. It also provides API access to [several LLM models](https://everlyai.xyz).\n\nThis notebook demonstrates the use of `langchain.chat_models.ChatEverlyAI` for [EverlyAI Hosted Endpoints](https://everlyai.xyz/).\n\n* Set `EVERLYAI_API_KEY` environment variable\n* or use the `everlyai_api_key` keyword argument\n"
  }
,
  {
    "path": "python/integrations/chat/qwq.mdx",
    "filename": "qwq.mdx",
    "size_bytes": 15444,
    "line_count": 166,
    "preview": "---\ntitle: ChatQwQ\n---\n\nThis will help you get started with QwQ [chat models](/oss/langchain/models). For detailed documentation of all ChatQwQ features and configurations head to the [API reference](https://pypi.org/project/langchain-qwq/).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/chat/yi.mdx",
    "filename": "yi.mdx",
    "size_bytes": 7255,
    "line_count": 96,
    "preview": "---\ntitle: ChatYI\n---\n\nThis will help you get started with Yi [chat models](/oss/langchain/models). For detailed documentation of all ChatYi features and configurations head to the [API reference](https://python.langchain.com/api_reference/lanchain_community/chat_models/lanchain_community.chat_models.yi.ChatYi.html).\n\n[01.AI](https://www.lingyiwanwu.com/en), founded by Dr. Kai-Fu Lee, is a global company at the forefront of AI 2.0. They offer cutting-edge Large Language Models, including the Yi series, which range from 6B to hundreds of billions of parameters. 01.AI also provides multimodal models, an open API platform, and open-source options like Yi-34B/9B/6B and Yi-VL.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/chat/promptlayer_chatopenai.mdx",
    "filename": "promptlayer_chatopenai.mdx",
    "size_bytes": 2470,
    "line_count": 65,
    "preview": "---\ntitle: PromptLayerChatOpenAI\n---\n\nThis example showcases how to connect to [PromptLayer](https://www.promptlayer.com) to start recording your ChatOpenAI requests.\n\n## Install PromptLayer\n\nThe `promptlayer` package is required to use PromptLayer with OpenAI. Install `promptlayer` using pip.\n\n"
  }
,
  {
    "path": "python/integrations/chat/anthropic.mdx",
    "filename": "anthropic.mdx",
    "size_bytes": 104072,
    "line_count": 2945,
    "preview": "---\ntitle: ChatAnthropic\ndescription: Get started using Anthropic [chat models](/oss/langchain/models) in LangChain.\n---\n\nYou can find information about Anthropic's latest models, their costs, context windows, and supported input types in the [Claude](https://platform.claude.com/docs/en/about-claude/models/overview) docs.\n\n<Tip>\n    **API Reference**\n\n"
  }
,
  {
    "path": "python/integrations/chat/oci_data_science.mdx",
    "filename": "oci_data_science.mdx",
    "size_bytes": 10682,
    "line_count": 271,
    "preview": "---\ntitle: ChatOCIModelDeployment\n---\n\nThis will help you get started with OCIModelDeployment [chat models](/oss/langchain/models). For detailed documentation of all ChatOCIModelDeployment features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/chat_models/langchain_community.chat_models.oci_data_science.ChatOCIModelDeployment.html).\n\n[OCI Data Science](https://docs.oracle.com/en-us/iaas/data-science/using/home.htm) is a fully managed and serverless platform for data science teams to build, train, and manage machine learning models in the Oracle Cloud Infrastructure. You can use [AI Quick Actions](https://blogs.oracle.com/ai-and-datascience/post/ai-quick-actions-in-oci-data-science) to easily deploy LLMs on [OCI Data Science Model Deployment Service](https://docs.oracle.com/en-us/iaas/data-science/using/model-dep-about.htm). You may choose to deploy the model with popular inference frameworks such as vLLM or TGI. By default, the model deployment endpoint mimics the OpenAI API protocol.\n\n> For the latest updates, examples and experimental features, please see [ADS LangChain Integration](https://accelerated-data-science.readthedocs.io/en/latest/user_guide/large_language_model/langchain_models.html).\n\n"
  }
,
  {
    "path": "python/integrations/chat/yuan2.mdx",
    "filename": "yuan2.mdx",
    "size_bytes": 4896,
    "line_count": 181,
    "preview": "---\ntitle: Yuan2.0\n---\n\nThis notebook shows how to use [YUAN2 API](https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/docs/inference_server.md) in LangChain with the langchain.chat_models.ChatYuan2.\n\n[*Yuan2.0*](https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/README-EN.md) is a new generation Fundamental Large Language Model developed by IEIT System. We have published all three models, Yuan 2.0-102B, Yuan 2.0-51B, and Yuan 2.0-2B. And we provide relevant scripts for pretraining, fine-tuning, and inference services for other developers. Yuan2.0 is based on Yuan1.0, utilizing a wider range of high-quality pre training data and instruction fine-tuning datasets to enhance the model's understanding of semantics, mathematics, reasoning, code, knowledge, and other aspects.\n\n## Getting started\n\n"
  }
,
  {
    "path": "python/integrations/chat/deepinfra.mdx",
    "filename": "deepinfra.mdx",
    "size_bytes": 3120,
    "line_count": 115,
    "preview": "---\ntitle: DeepInfra\n---\n\n[DeepInfra](https://deepinfra.com/?utm_source=langchain) is a serverless inference as a service that provides access to a [variety of LLMs](https://deepinfra.com/models?utm_source=langchain) and [embeddings models](https://deepinfra.com/models?type=embeddings&utm_source=langchain). This notebook goes over how to use LangChain with DeepInfra for chat models.\n\n## Set the environment API Key\n\nMake sure to get your API key from DeepInfra. You have to [Login](https://deepinfra.com/login?from=%2Fdash) and get a new token.\n\n"
  }
,
  {
    "path": "python/integrations/chat/ernie.mdx",
    "filename": "ernie.mdx",
    "size_bytes": 1953,
    "line_count": 60,
    "preview": "---\ntitle: ErnieBotChat\n---\n\n[ERNIE-Bot](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/jlil56u11) is a large language model developed by Baidu, covering a huge amount of Chinese data.\nThis notebook covers how to get started with ErnieBot chat models.\n\n**Deprecated Warning**\n\nWe recommend users switch from `langchain_community.chat_models.ErnieBotChat` to `langchain_community.chat_models.QianfanChatEndpoint`.\n"
  }
,
  {
    "path": "python/integrations/chat/friendli.mdx",
    "filename": "friendli.mdx",
    "size_bytes": 5012,
    "line_count": 115,
    "preview": "---\ntitle: ChatFriendli\n---\n\n> [Friendli](https://friendli.ai/) enhances AI application performance and optimizes cost savings with scalable, efficient deployment options, tailored for high-demand AI workloads.\n\nThis tutorial guides you through integrating `ChatFriendli` for chat applications using LangChain. `ChatFriendli` offers a flexible approach to generating conversational AI responses, supporting both synchronous and asynchronous calls.\n\n## Setup\n\n"
  }
,
  {
    "path": "python/integrations/chat/together.mdx",
    "filename": "together.mdx",
    "size_bytes": 3965,
    "line_count": 102,
    "preview": "---\ntitle: ChatTogether\n---\n\n\nThis page will help you get started with Together AI [chat models](/oss/langchain/models). For detailed documentation of all ChatTogether features and configurations, head to the [API reference](https://python.langchain.com/api_reference/together/chat_models/langchain_together.chat_models.ChatTogether.html).\n\n[Together AI](https://www.together.ai/) offers an API to query [50+ leading open-source models](https://docs.together.ai/docs/chat-models)\n\n## Overview\n"
  }
,
  {
    "path": "python/integrations/chat/alibaba_cloud_pai_eas.mdx",
    "filename": "alibaba_cloud_pai_eas.mdx",
    "size_bytes": 2792,
    "line_count": 60,
    "preview": "---\ntitle: Alibaba Cloud PAI EAS\n---\n\n>[Alibaba Cloud PAI (Platform for AI)](https://www.alibabacloud.com/help/en/pai/?spm=a2c63.p38356.0.0.c26a426ckrxUwZ) is a lightweight and cost-efficient machine learning platform that uses cloud-native technologies. It provides you with an end-to-end modelling service. It accelerates model training based on tens of billions of features and hundreds of billions of samples in more than 100 scenarios.\n\n>[Machine Learning Platform for AI of Alibaba Cloud](https://www.alibabacloud.com/help/en/machine-learning-platform-for-ai/latest/what-is-machine-learning-pai) is a machine learning or deep learning engineering platform intended for enterprises and developers. It provides easy-to-use, cost-effective, high-performance, and easy-to-scale plug-ins that can be applied to various industry scenarios. With over 140 built-in optimization algorithms, `Machine Learning Platform for AI` provides whole-process AI engineering capabilities including data labelling (`PAI-iTAG`), model building (`PAI-Designer` and `PAI-DSW`), model training (`PAI-DLC`), compilation optimization, and inference deployment (`PAI-EAS`).\n>\n>`PAI-EAS` supports different types of hardware resources, including CPUs and GPUs, and features high throughput and low latency. It allows you to deploy large-scale complex models with a few clicks and perform elastic scale-ins and scale-outs in real-time. It also provides a comprehensive O&M and monitoring system.\n\n"
  }
,
  {
    "path": "python/integrations/chat/moonshot.mdx",
    "filename": "moonshot.mdx",
    "size_bytes": 970,
    "line_count": 39,
    "preview": "---\ntitle: MoonshotChat\n---\n\n[Moonshot](https://platform.moonshot.cn/) is a Chinese startup that provides LLM service for companies and individuals.\n\nThis example goes over how to use LangChain to interact with Moonshot Inference for Chat.\n\n```python\nimport os\n"
  }
,
  {
    "path": "python/integrations/chat/contextual.mdx",
    "filename": "contextual.mdx",
    "size_bytes": 5877,
    "line_count": 138,
    "preview": "---\ntitle: ChatContextual\n---\n\nThis will help you get started with Contextual AI's Grounded Language Model [chat models](/oss/langchain/models/).\n\nTo learn more about Contextual AI, please visit our [documentation](https://docs.contextual.ai/).\n\nThis integration requires the `contextual-client` Python SDK. Learn more about it [here](https://github.com/ContextualAI/contextual-client-python).\n\n"
  }
,
  {
    "path": "python/integrations/chat/greennode.mdx",
    "filename": "greennode.mdx",
    "size_bytes": 7486,
    "line_count": 194,
    "preview": "---\ntitle: ChatGreenNode\n---\n\n>[GreenNode](https://greennode.ai/) is a global AI solutions provider and a **NVIDIA Preferred Partner**, delivering full-stack AI capabilities—from infrastructure to application—for enterprises across the US, MENA, and APAC regions. Operating on **world-class infrastructure** (LEED Gold, TIA‑942, Uptime Tier III), GreenNode empowers enterprises, startups, and researchers with a comprehensive suite of AI services\n\nThis page will help you get started with GreenNode Serverless AI [chat models](/oss/langchain/models). For detailed documentation of all ChatGreenNode features and configurations head to the [API reference](https://python.langchain.com/api_reference/greennode/chat_models/langchain_greennode.chat_models.ChatGreenNode.html).\n\n[GreenNode AI](https://greennode.ai/) offers an API to query [20+ leading open-source models](https://aiplatform.console.greennode.ai/models)\n\n"
  }
,
  {
    "path": "python/integrations/chat/llama2_chat.mdx",
    "filename": "llama2_chat.mdx",
    "size_bytes": 8328,
    "line_count": 192,
    "preview": "---\ntitle: Llama2Chat\n---\n\nThis notebook shows how to augment Llama-2 `LLM`s with the `Llama2Chat` wrapper to support the [Llama-2 chat prompt format](https://huggingface.co/blog/llama2#how-to-prompt-llama-2). Several `LLM` implementations in LangChain can be used as interface to Llama-2 chat models. These include [ChatHuggingFace](/oss/integrations/chat/huggingface), [LlamaCpp](/oss/integrations/chat/llamacpp/), [GPT4All](/oss/integrations/llms/gpt4all), ..., to mention a few examples.\n\n`Llama2Chat` is a generic wrapper that implements `BaseChatModel` and can therefore be used in applications as [chat model](/oss/langchain/models). `Llama2Chat` converts a list of Messages into the [required chat prompt format](https://huggingface.co/blog/llama2#how-to-prompt-llama-2) and forwards the formatted prompt as `str` to the wrapped `LLM`.\n\n```python\nfrom langchain_classic.chains import LLMChain\n"
  }
,
  {
    "path": "python/integrations/chat/mlx.mdx",
    "filename": "mlx.mdx",
    "size_bytes": 4271,
    "line_count": 152,
    "preview": "---\ntitle: MLX\n---\n\nThis notebook shows how to get started using `MLX` LLM's as chat models.\n\nIn particular, we will:\n\n1. Utilize the [MLXPipeline](https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/llms/mlx_pipeline.py),\n2. Utilize the `ChatMLX` class to enable any of these LLMs to interface with LangChain's [Chat Messages](https://python.langchain.com/docs/modules/model_io/chat/#messages) abstraction.\n"
  }
,
  {
    "path": "python/integrations/chat/kinetica.mdx",
    "filename": "kinetica.mdx",
    "size_bytes": 8793,
    "line_count": 295,
    "preview": "---\ntitle: Kinetica Language To SQL Chat Model\n---\n\nThis notebook demonstrates how to use Kinetica to transform natural language into SQL\nand simplify the process of data retrieval. This demo is intended to show the mechanics\nof creating and using a chain as opposed to the capabilities of the LLM.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/chat/cohere.mdx",
    "filename": "cohere.mdx",
    "size_bytes": 5545,
    "line_count": 130,
    "preview": "---\ntitle: Cohere\n---\n\nThis notebook covers how to get started with [Cohere chat models](https://cohere.com/chat).\n\nHead to the [API reference](https://python.langchain.com/api_reference/community/chat_models/langchain_community.chat_models.cohere.ChatCohere.html) for detailed documentation of all attributes and methods.\n\n## Setup\n\n"
  }
,
  {
    "path": "python/integrations/chat/ollama.mdx",
    "filename": "ollama.mdx",
    "size_bytes": 62237,
    "line_count": 307,
    "preview": "---\ntitle: ChatOllama\ndescription: Get started using Ollama [chat models](/oss/langchain/models) in LangChain.\n---\n\n[Ollama](https://ollama.com/) allows you to run open-source Large Language Models (LLMs), such as `gpt-oss`, locally.\n\nOllama bundles model weights, configuration, and data into a single package, defined by a Modelfile. It optimizes setup and configuration details, including GPU usage.\n\nFor a complete list of supported models and model variants, see the [Ollama model library](https://ollama.com/search).\n"
  }
,
  {
    "path": "python/integrations/chat/abso.mdx",
    "filename": "abso.mdx",
    "size_bytes": 2538,
    "line_count": 75,
    "preview": "---\ntitle: ChatAbso\n---\n\nThis will help you get started with ChatAbso [chat models](https://python.langchain.com/docs/concepts/chat_models/). For detailed documentation of all ChatAbso features and configurations, head to the [API reference](https://python.langchain.com/api_reference/en/latest/chat_models/langchain_abso.chat_models.ChatAbso.html).\n\n- You can find the full documentation for the Abso router [here] ([abso.ai](https://abso.ai))\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/chat/cerebras.mdx",
    "filename": "cerebras.mdx",
    "size_bytes": 10031,
    "line_count": 210,
    "preview": "---\ntitle: ChatCerebras\n---\n\nThis guide provides a quick overview for getting started with Cerebras [chat models](/oss/langchain/models). For detailed documentation of all ChatCerebras features and configurations head to the [API reference](https://python.langchain.com/api_reference/cerebras/chat_models/langchain_cerebras.chat_models.ChatCerebras.html#).\n\nAt Cerebras, we've developed the world's largest and fastest AI processor, the Wafer-Scale Engine-3 (WSE-3). The Cerebras CS-3 system, powered by the WSE-3, represents a new class of AI supercomputer that sets the standard for generative AI training and inference with unparalleled performance and scalability.\n\nWith Cerebras as your inference provider, you can:\n\n"
  }
,
  {
    "path": "python/integrations/chat/outlines.mdx",
    "filename": "outlines.mdx",
    "size_bytes": 5753,
    "line_count": 190,
    "preview": "---\ntitle: ChatOutlines\n---\n\nThis will help you get started with Outlines [chat models](/oss/langchain/models/). For detailed documentation of all ChatOutlines features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/chat_models/langchain_community.chat_models.outlines.ChatOutlines.html).\n\n[Outlines](https://github.com/outlines-dev/outlines) is a library for constrained language generation. It allows you to use Large Language Models (LLMs) with various backends while applying constraints to the generated output.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/chat/netmind.mdx",
    "filename": "netmind.mdx",
    "size_bytes": 4790,
    "line_count": 104,
    "preview": "---\ntitle: ChatNetmind\n---\n\nThis will help you get started with Netmind [chat models](https://www.netmind.ai/). For detailed documentation of all ChatNetmind features and configurations head to the [API reference](https://github.com/protagolabs/langchain-netmind).\n\n- See [www.netmind.ai/](https://www.netmind.ai/) for an example.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/chat/pipeshift.mdx",
    "filename": "pipeshift.mdx",
    "size_bytes": 3758,
    "line_count": 105,
    "preview": "---\ntitle: ChatPipeshift\n---\n\nThis will help you get started with Pipeshift [chat models](/oss/langchain/models/). For detailed documentation of all ChatPipeshift features and configurations head to the [API reference](https://dashboard.pipeshift.com/docs).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/chat/vllm.mdx",
    "filename": "vllm.mdx",
    "size_bytes": 3397,
    "line_count": 96,
    "preview": "---\ntitle: vLLM Chat\n---\n\nvLLM can be deployed as a server that mimics the OpenAI API protocol. This allows vLLM to be used as a drop-in replacement for applications using OpenAI API. This server can be queried in the same format as OpenAI API.\n\n## Overview\n\nThis will help you get started with vLLM [chat models](/oss/langchain/models), which leverages the `langchain-openai` package. For detailed documentation of all @[`ChatOpenAI`] features and configurations head to the [API reference](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html).\n\n"
  }
,
  {
    "path": "python/integrations/chat/nvidia_ai_endpoints.mdx",
    "filename": "nvidia_ai_endpoints.mdx",
    "size_bytes": 14517,
    "line_count": 381,
    "preview": "---\ntitle: ChatNVIDIA\n---\n\nThis will help you get started with NVIDIA [chat models](/oss/langchain/models). For detailed documentation of all `ChatNVIDIA` features and configurations head to the [API reference](https://python.langchain.com/api_reference/nvidia_ai_endpoints/chat_models/langchain_nvidia_ai_endpoints.chat_models.ChatNVIDIA.html).\n\n## Overview\n\nThe `langchain-nvidia-ai-endpoints` package contains LangChain integrations for chat models and embeddings powered by [NVIDIA AI Foundation Models](https://www.nvidia.com/en-us/ai-data-science/foundation-models/), and hosted on the [NVIDIA API Catalog](https://build.nvidia.com/).\n\n"
  }
,
  {
    "path": "python/integrations/chat/perplexity.mdx",
    "filename": "perplexity.mdx",
    "size_bytes": 10610,
    "line_count": 194,
    "preview": "---\ntitle: ChatPerplexity\n---\n\n\nThis page will help you get started with Perplexity [chat models](/oss/langchain/models). For detailed documentation of all `ChatPerplexity` features and configurations head to the [API reference](https://python.langchain.com/api_reference/perplexity/chat_models/langchain_perplexity.chat_models.ChatPerplexity.html).\n\n## Overview\n\n### Integration details\n"
  }
,
  {
    "path": "python/integrations/callbacks/streamlit.mdx",
    "filename": "streamlit.mdx",
    "size_bytes": 3326,
    "line_count": 80,
    "preview": "---\ntitle: Streamlit\n---\n\n> **[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.**\n> Streamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front-end experience required.\n> See more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\n"
  }
,
  {
    "path": "python/integrations/callbacks/sagemaker_tracking.mdx",
    "filename": "sagemaker_tracking.mdx",
    "size_bytes": 6383,
    "line_count": 202,
    "preview": "---\ntitle: SageMaker Tracking\n---\n\n>[Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a fully managed service that is used to quickly and easily build, train and deploy machine learning (ML) models.\n\n>[Amazon SageMaker Experiments](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) is a capability of `Amazon SageMaker` that lets you organize, track, compare and evaluate ML experiments and model versions.\n\nThis notebook shows how LangChain Callback can be used to log and track prompts and other LLM hyperparameters into `SageMaker Experiments`. Here, we use different scenarios to showcase the capability:\n\n"
  }
,
  {
    "path": "python/integrations/callbacks/confident.mdx",
    "filename": "confident.mdx",
    "size_bytes": 7625,
    "line_count": 154,
    "preview": "---\ntitle: Confident\n---\n\n>[DeepEval](https://confident-ai.com) package for unit testing LLMs.\n> Using Confident, everyone can build robust language models through faster iterations\n> using both unit testing and integration testing. We provide support for each step in the iteration\n> from synthetic data creation to testing.\n\nIn this guide we will demonstrate how to test and measure LLMs in performance. We show how you can use our callback to measure performance and how you can define your own metric and log them into our dashboard.\n"
  }
,
  {
    "path": "python/integrations/callbacks/llmonitor.mdx",
    "filename": "llmonitor.mdx",
    "size_bytes": 3692,
    "line_count": 129,
    "preview": "---\ntitle: LLMonitor\n---\n\n>[LLMonitor](https://llmonitor.com?utm_source=langchain&utm_medium=py&utm_campaign=docs) is an open-source observability platform that provides cost and usage analytics, user tracking, tracing and evaluation tools.\n\n<video controls width='100%' >\n    <source src='https://llmonitor.com/videos/demo-annotated.mp4'/>\n</video>\n\n"
  }
,
  {
    "path": "python/integrations/callbacks/infino.mdx",
    "filename": "infino.mdx",
    "size_bytes": 13129,
    "line_count": 283,
    "preview": "---\ntitle: Infino\n---\n\n>[Infino](https://github.com/infinohq/infino) is a scalable telemetry store designed for logs, metrics, and traces. Infino can function as a standalone observability solution or as the storage layer in your observability stack.\n\nThis example shows how one can track the following while calling OpenAI and ChatOpenAI models via `LangChain` and [Infino](https://github.com/infinohq/infino):\n\n* prompt input\n* response from `ChatGPT` or any other `LangChain` model\n"
  }
,
  {
    "path": "python/integrations/callbacks/upstash_ratelimit.mdx",
    "filename": "upstash_ratelimit.mdx",
    "size_bytes": 6163,
    "line_count": 174,
    "preview": "---\ntitle: Upstash Ratelimit Callback\n---\n\nIn this guide, we will go over how to add rate limiting based on number of requests or the number of tokens using `UpstashRatelimitHandler`. This handler uses [ratelimit library of Upstash](https://github.com/upstash/ratelimit-py/), which utilizes [Upstash Redis](https://upstash.com/docs/redis/overall/getstarted).\n\nUpstash Ratelimit works by sending an HTTP request to Upstash Redis everytime the `limit` method is called. Remaining tokens/requests of the user are checked and updated. Based on the remaining tokens, we can stop the execution of costly operations like invoking an LLM or querying a vector store:\n\n```py\nresponse = ratelimit.limit()\n"
  }
,
  {
    "path": "python/integrations/callbacks/fiddler.mdx",
    "filename": "fiddler.mdx",
    "size_bytes": 3176,
    "line_count": 113,
    "preview": "---\ntitle: Fiddler\n---\n\n>[Fiddler](https://www.fiddler.ai/) is the pioneer in enterprise Generative and Predictive system ops, offering a unified platform that enables Data Science, MLOps, Risk, Compliance, Analytics, and other LOB teams to monitor, explain, analyze, and improve ML deployments at enterprise scale.\n\n## 1. Installation and setup\n\n```python\n#!pip install langchain langchain-community langchain-openai fiddler-client\n"
  }
,
  {
    "path": "python/integrations/callbacks/uptrain.mdx",
    "filename": "uptrain.mdx",
    "size_bytes": 15693,
    "line_count": 323,
    "preview": "---\ntitle: UpTrain\n---\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/langchain-ai/langchain/blob/v0.3/docs/docs/integrations/callbacks/uptrain.ipynb\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\n> UpTrain [[github](https://github.com/uptrain-ai/uptrain) || [website](https://uptrain.ai/) || [docs](https://docs.uptrain.ai/getting-started/introduction)] is an open-source platform to evaluate and improve LLM applications. It provides grades for 20+ preconfigured checks (covering language, code, embedding use cases), performs root cause analyses on instances of failure cases and provides guidance for resolving them.\n\n"
  }
,
  {
    "path": "python/integrations/callbacks/comet_tracing.mdx",
    "filename": "comet_tracing.mdx",
    "size_bytes": 1924,
    "line_count": 66,
    "preview": "---\ntitle: Comet Tracing\n---\n\nThere are two ways to trace your LangChains executions with Comet:\n\n1. Setting the `LANGCHAIN_COMET_TRACING` environment variable to \"true\". This is the recommended way.\n2. Import the `CometTracer` manually and pass it explicitely.\n\n```python\n"
  }
,
  {
    "path": "python/integrations/callbacks/promptlayer.mdx",
    "filename": "promptlayer.mdx",
    "size_bytes": 4129,
    "line_count": 111,
    "preview": "---\ntitle: PromptLayer\n---\n\n>[PromptLayer](https://docs.promptlayer.com/introduction) is a platform for prompt engineering. It also helps with the LLM observability to visualize requests, version prompts, and track usage.\n>\n>While `PromptLayer` does have LLMs that integrate directly with LangChain (e.g. [`PromptLayerOpenAI`](/oss/integrations/llms/promptlayer_openai)), using a callback is the recommended way to integrate `PromptLayer` with LangChain.\n\nIn this guide, we will go over how to setup the `PromptLayerCallbackHandler`.\n\n"
  }
,
  {
    "path": "python/integrations/callbacks/trubrics.mdx",
    "filename": "trubrics.mdx",
    "size_bytes": 4214,
    "line_count": 171,
    "preview": "---\ntitle: Trubrics\n---\n\n>[Trubrics](https://trubrics.com) is an LLM user analytics platform that lets you collect, analyse and manage user\nprompts & feedback on AI models.\n>\n>Check out [Trubrics repo](https://github.com/trubrics/trubrics-sdk) for more information on `Trubrics`.\n\nIn this guide, we will go over how to set up the `TrubricsCallbackHandler`.\n"
  }
,
  {
    "path": "python/integrations/callbacks/argilla.mdx",
    "filename": "argilla.mdx",
    "size_bytes": 11165,
    "line_count": 223,
    "preview": "---\ntitle: Argilla\n---\n\n>[Argilla](https://argilla.io/) is an open-source data curation platform for LLMs.\n> Using Argilla, everyone can build robust language models through faster data curation\n> using both human and machine feedback. We provide support for each step in the MLOps cycle,\n> from data labeling to model monitoring.\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/langchain-ai/langchain/blob/v0.3/docs/docs/integrations/callbacks/argilla.ipynb\">\n"
  }
,
  {
    "path": "python/integrations/callbacks/context.mdx",
    "filename": "context.mdx",
    "size_bytes": 3258,
    "line_count": 116,
    "preview": "---\ntitle: Context\n---\n\n>[Context](https://context.ai/) provides user analytics for LLM-powered products and features.\n\nWith `Context`, you can start understanding your users and improving their experiences in less than 30 minutes.\n\nIn this guide we will show you how to integrate with Context.\n\n"
  }
,
  {
    "path": "python/integrations/callbacks/labelstudio.mdx",
    "filename": "labelstudio.mdx",
    "size_bytes": 7041,
    "line_count": 176,
    "preview": "---\ntitle: Label Studio\n---\n\n>[Label Studio](https://labelstud.io/guide/get_started) is an open-source data labeling platform that provides LangChain with flexibility when it comes to labeling data for fine-tuning Large Language Models (LLMs). It also enables the preparation of custom training data and the collection and evaluation of responses through human feedback.\n\nIn this guide, you will learn how to connect a LangChain pipeline to `Label Studio` to:\n\n- Aggregate all input prompts, conversations, and responses in a single `Label Studio` project. This consolidates all the data in one place for easier labeling and analysis.\n- Refine prompts and responses to create a dataset for supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) scenarios. The labeled data can be used to further train the LLM to improve its performance.\n"
  }
,
  {
    "path": "python/integrations/callbacks/google_bigquery.mdx",
    "filename": "google_bigquery.mdx",
    "size_bytes": 24007,
    "line_count": 773,
    "preview": "---\ntitle: BigQuery Callback Handler\ndescription: Log events from LangChain to Google BigQuery for monitoring, auditing, and analyzing your LLM applications.\n---\n\n# BigQuery Callback Handler\n\n<div class=\"language-support-tag\">\n  <span class=\"lst-supported\">Community</span><span class=\"lst-python\">Python</span><span class=\"lst-preview\">Preview</span>\n</div>\n"
  }
,
  {
    "path": "python/integrations/providers/figma.mdx",
    "filename": "figma.mdx",
    "size_bytes": 703,
    "line_count": 23,
    "preview": "---\ntitle: Figma\n---\n\n>[Figma](https://www.figma.com/) is a collaborative web application for interface design.\n\n## Installation and setup\n\nThe Figma API requires an `access token`, `node_ids`, and a `file key`.\n\n"
  }
,
  {
    "path": "python/integrations/providers/myscale.mdx",
    "filename": "myscale.mdx",
    "size_bytes": 2719,
    "line_count": 68,
    "preview": "---\ntitle: MyScale\n---\n\nThis page covers how to use MyScale vector database within LangChain.\nIt is broken into two parts: installation and setup, and then references to specific MyScale wrappers.\n\nWith MyScale, you can manage both structured and unstructured (vectorized) data, and perform joint queries and analytics on both types of data using SQL. Plus, MyScale's cloud-native OLAP architecture, built on top of ClickHouse, enables lightning-fast data processing even on massive datasets.\n\n## Introduction\n"
  }
,
  {
    "path": "python/integrations/providers/dappier.mdx",
    "filename": "dappier.mdx",
    "size_bytes": 1914,
    "line_count": 68,
    "preview": "---\ntitle: Dappier\n---\n\n[Dappier](https://dappier.com) connects any LLM or your Agentic AI to\nreal-time, rights-cleared, proprietary data from trusted sources,\nmaking your AI an expert in anything. Our specialized models include\nReal-Time Web Search, News, Sports, Financial Stock Market Data,\nCrypto Data, and exclusive content from premium publishers. Explore a\nwide range of data models in our marketplace at\n"
  }
,
  {
    "path": "python/integrations/providers/agentql.mdx",
    "filename": "agentql.mdx",
    "size_bytes": 1394,
    "line_count": 43,
    "preview": "---\ntitle: AgentQL\n---\n\n[AgentQL](https://www.agentql.com/) provides web interaction and structured data extraction from any web page using an [AgentQL query](https://docs.agentql.com/agentql-query) or a Natural Language prompt. AgentQL can be used across multiple languages and web pages without breaking over time and change.\n\n## Installation and setup\n\nInstall the integration package:\n\n"
  }
,
  {
    "path": "python/integrations/providers/duckduckgo_search.mdx",
    "filename": "duckduckgo_search.mdx",
    "size_bytes": 712,
    "line_count": 33,
    "preview": "---\ntitle: DuckDuckGo Search\n---\n\n>[DuckDuckGo Search](https://github.com/deedy5/duckduckgo_search) is a package that\n> searches for words, documents, images, videos, news, maps and text\n> translation using the `DuckDuckGo.com` search engine. It is downloading files\n> and images to a local hard drive.\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/weather.mdx",
    "filename": "weather.mdx",
    "size_bytes": 451,
    "line_count": 29,
    "preview": "---\ntitle: Weather\n---\n\n>[OpenWeatherMap](https://openweathermap.org/) is an open-source weather service provider.\n\n\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/tair.mdx",
    "filename": "tair.mdx",
    "size_bytes": 766,
    "line_count": 31,
    "preview": "---\ntitle: Tair\n---\n\n>[Alibaba Cloud Tair](https://www.alibabacloud.com/help/en/tair/latest/what-is-tair) is a cloud native in-memory database service\n> developed by `Alibaba Cloud`. It provides rich data models and enterprise-grade capabilities to\n> support your real-time online scenarios while maintaining full compatibility with open-source `Redis`.\n> `Tair` also introduces persistent memory-optimized instances that are based on\n> new non-volatile memory (NVM) storage medium.\n\n"
  }
,
  {
    "path": "python/integrations/providers/college_confidential.mdx",
    "filename": "college_confidential.mdx",
    "size_bytes": 412,
    "line_count": 18,
    "preview": "---\ntitle: College Confidential\n---\n\n>[College Confidential](https://www.collegeconfidential.com/) gives information on 3,800+ colleges and universities.\n\n## Installation and setup\n\nThere isn't any special setup for it.\n\n"
  }
,
  {
    "path": "python/integrations/providers/rwkv.mdx",
    "filename": "rwkv.mdx",
    "size_bytes": 1960,
    "line_count": 67,
    "preview": "---\ntitle: RWKV-4\n---\n\nThis page covers how to use the `RWKV-4` wrapper within LangChain.\nIt is broken into two parts: installation and setup, and then usage with an example.\n\n## Installation and setup\n- Install the Python package with `pip install rwkv`\n- Install the tokenizer Python package with `pip install tokenizer`\n"
  }
,
  {
    "path": "python/integrations/providers/streamlit.mdx",
    "filename": "streamlit.mdx",
    "size_bytes": 655,
    "line_count": 30,
    "preview": "---\ntitle: Streamlit\n---\n\n>[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n>`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front-end experience required.\n>See more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/lakefs.mdx",
    "filename": "lakefs.mdx",
    "size_bytes": 532,
    "line_count": 20,
    "preview": "---\ntitle: lakeFS\n---\n\n>[lakeFS](https://docs.lakefs.io/) provides scalable version control over\n> the data lake, and uses Git-like semantics to create and access those versions.\n\n## Installation and setup\n\nGet the `ENDPOINT`, `LAKEFS_ACCESS_KEY`, and `LAKEFS_SECRET_KEY`.\n"
  }
,
  {
    "path": "python/integrations/providers/lindorm.mdx",
    "filename": "lindorm.mdx",
    "size_bytes": 1508,
    "line_count": 43,
    "preview": "---\ntitle: Lindorm\n---\n\nLindorm is a cloud-native multimodal database from Alibaba-Cloud, It supports unified access and integrated processing of various types of data, including wide tables, time-series, text, objects, streams, and spatial data. It is compatible with multiple standard interfaces such as SQL, HBase/Cassandra/S3, TSDB, HDFS, Solr, and Kafka, and seamlessly integrates with third-party ecosystem tools. This makes it suitable for scenarios such as logging, monitoring, billing, advertising, social networking, travel, and risk control. Lindorm is also one of the databases that support Alibaba's core businesses.\n\nTo use the AI and vector capabilities of Lindorm, you should [get the service](https://help.aliyun.com/document_detail/174640.html?spm=a2c4g.11186623.help-menu-172543.d_0_1_0.4c6367558DN8Uq) and install `langchain-lindorm-integration` package.\n\n```python\n!pip install -U langchain-lindorm-integration\n"
  }
,
  {
    "path": "python/integrations/providers/pymupdf4llm.mdx",
    "filename": "pymupdf4llm.mdx",
    "size_bytes": 479,
    "line_count": 15,
    "preview": "---\ntitle: PyMuPDF4LLM\n---\n\n[PyMuPDF4LLM](https://pymupdf.readthedocs.io/en/latest/pymupdf4llm) is aimed to make it easier to extract PDF content in Markdown format, needed for LLM & RAG applications.\n\n[langchain-pymupdf4llm](https://github.com/lakinduboteju/langchain-pymupdf4llm) integrates PyMuPDF4LLM to LangChain as a Document Loader.\n\n```python\npip install -qU langchain-pymupdf4llm\n"
  }
,
  {
    "path": "python/integrations/providers/fauna.mdx",
    "filename": "fauna.mdx",
    "size_bytes": 696,
    "line_count": 33,
    "preview": "---\ntitle: Fauna\n---\n\n>[Fauna](https://fauna.com/) is a distributed document-relational database\n> that combines the flexibility of documents with the power of a relational,\n> ACID compliant database that scales across regions, clouds or the globe.\n\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/oci.mdx",
    "filename": "oci.mdx",
    "size_bytes": 2242,
    "line_count": 62,
    "preview": "---\ntitle: Oracle Cloud Infrastructure (OCI)\n---\n\nThe `LangChain` integrations related to [Oracle Cloud Infrastructure](https://www.oracle.com/artificial-intelligence/).\n\n## OCI generative AI\n> Oracle Cloud Infrastructure (OCI) [Generative AI](https://docs.oracle.com/en-us/iaas/Content/generative-ai/home.htm) is a fully managed service that provides a set of state-of-the-art,\n> customizable large language models (LLMs) that cover a wide range of use cases, and which are available through a single API.\n> Using the OCI Generative AI service you can access ready-to-use pretrained models, or create and host your own fine-tuned\n"
  }
,
  {
    "path": "python/integrations/providers/oracleai.mdx",
    "filename": "oracleai.mdx",
    "size_bytes": 3055,
    "line_count": 67,
    "preview": "---\ntitle: Oracle AI Vector Search\n---\n\nOracle AI Vector Search is designed for Artificial Intelligence (AI) workloads that allows you to query data based on semantics, rather than keywords.\nOne of the biggest benefits of Oracle AI Vector Search is that semantic search on unstructured data can be combined with relational search on business data in one single system.\nThis is not only powerful but also significantly more effective because you don't need to add a specialized vector database, eliminating the pain of data fragmentation between multiple systems.\n\nIn addition, your vectors can benefit from all of Oracle Database’s most powerful features, like the following:\n\n"
  }
,
  {
    "path": "python/integrations/providers/teradata.mdx",
    "filename": "teradata.mdx",
    "size_bytes": 1407,
    "line_count": 29,
    "preview": "---\ntitle: Teradata\n---\n\nThis page covers how to use Teradata Vector Store within LangChain.\nIt is broken into two parts: installation and setup, and then references to specific Teradata wrappers.\n\n## Installation\n- Install the Python package with `pip install langchain-teradata`\n\n"
  }
,
  {
    "path": "python/integrations/providers/serpapi.mdx",
    "filename": "serpapi.mdx",
    "size_bytes": 1021,
    "line_count": 33,
    "preview": "---\ntitle: SerpApi\n---\n\nThis page covers how to use the [SerpApi web search APIs](https://serpapi.com/) within LangChain.\nIt is broken into two parts: installation and setup, and then references to the specific SerpAPI wrapper.\n\n## Installation and setup\n- Install requirements with `pip install google-search-results`\n- Get a SerpApi API key from [here](https://serpapi.com/manage-api-key) and set it as an environment variable (`SERPAPI_API_KEY`)\n"
  }
,
  {
    "path": "python/integrations/providers/lantern.mdx",
    "filename": "lantern.mdx",
    "size_bytes": 908,
    "line_count": 27,
    "preview": "---\ntitle: Lantern\n---\n\nThis page covers how to use the [Lantern](https://github.com/lanterndata/lantern) within LangChain\nIt is broken into two parts: setup, and then references to specific Lantern wrappers.\n\n## Setup\n1. The first step is to create a database with the `lantern` extension installed.\n\n"
  }
,
  {
    "path": "python/integrations/providers/ascend.mdx",
    "filename": "ascend.mdx",
    "size_bytes": 682,
    "line_count": 32,
    "preview": "---\ntitle: Ascend\n---\n\n>[Ascend](https://https://www.hiascend.com/) is Natural Process Unit provide by Huawei\n\nThis page covers how to use ascend NPU with LangChain.\n\n### Installation\n\n"
  }
,
  {
    "path": "python/integrations/providers/kuzu.mdx",
    "filename": "kuzu.mdx",
    "size_bytes": 1365,
    "line_count": 48,
    "preview": "---\ntitle: Kùzu\n---\n\n> [Kùzu](https://kuzudb.com/) is an embeddable, scalable, extremely fast graph database.\n> It is permissively licensed with an MIT license, and you can see its source code [here](https://github.com/kuzudb/kuzu).\n\n> Key characteristics of Kùzu:\n>- Performance and scalability: Implements modern, state-of-the-art join algorithms for graphs.\n>- Usability: Very easy to set up and get started with, as there are no servers (embedded architecture).\n"
  }
,
  {
    "path": "python/integrations/providers/undatasio.mdx",
    "filename": "undatasio.mdx",
    "size_bytes": 913,
    "line_count": 33,
    "preview": "---\ntitle: UnDatasIO\n---\n\n> The `undatasio` package from\n> [UnDatasIO](https://undatas.io) extracts clean text from raw source documents like\n> PDFs.\n> This page covers how to use the `undatasio`\n> ecosystem within LangChain.\n\n"
  }
,
  {
    "path": "python/integrations/providers/llamaedge.mdx",
    "filename": "llamaedge.mdx",
    "size_bytes": 702,
    "line_count": 26,
    "preview": "---\ntitle: LlamaEdge\n---\n\n>[LlamaEdge](https://llamaedge.com/docs/intro/) is the easiest & fastest way to run customized\n> and fine-tuned LLMs locally or on the edge.\n>\n>* Lightweight inference apps. `LlamaEdge` is in MBs instead of GBs\n>* Native and GPU accelerated performance\n>* Supports many GPU and hardware accelerators\n"
  }
,
  {
    "path": "python/integrations/providers/graphsignal.mdx",
    "filename": "graphsignal.mdx",
    "size_bytes": 1572,
    "line_count": 46,
    "preview": "---\ntitle: Graphsignal\n---\n\nThis page covers how to use [Graphsignal](https://app.graphsignal.com) to trace and monitor LangChain. Graphsignal enables full visibility into your application. It provides latency breakdowns by chains and tools, exceptions with full context, data monitoring, compute/GPU utilization, OpenAI cost analytics, and more.\n\n## Installation and setup\n\n- Install the Python library with `pip install graphsignal`\n- Create free Graphsignal account [here](https://graphsignal.com)\n"
  }
,
  {
    "path": "python/integrations/providers/plainid.mdx",
    "filename": "plainid.mdx",
    "size_bytes": 2750,
    "line_count": 56,
    "preview": "---\ntitle: PlainID\n---\n\n[PlainID](https://www.plainid.com/) provides policy-based authorization (PBAC) and centralized policy enforcement. This integration allows you to enforce fine-grained access control within your LangChain applications at different stages of the LLM chain.\n\nWith `langchain-plainid`, you can:\n- **Filter RAG data:** Dynamically filter documents retrieved from your vector store based on the user's permissions, ensuring they only see data they are authorized to access.\n- **Authorize prompts:** Control whether a user or tenant is allowed to *invoke* a chain or tool based on the *category* of their query.\n- **Anonymize data:** Detect and anonymize (mask or encrypt) PII or other sensitive entities in responses, based on policies defined in PlainID.\n"
  }
,
  {
    "path": "python/integrations/providers/toolbox.mdx",
    "filename": "toolbox.mdx",
    "size_bytes": 1135,
    "line_count": 31,
    "preview": "---\ntitle: MCP Toolbox\n---\n\nThe [MCP Toolbox](https://googleapis.github.io/genai-toolbox/getting-started/introduction/) in LangChain allows you to equip an agent with a set of tools. When the agent receives a query, it can intelligently select and use the most appropriate tool provided by MCP Toolbox to fulfill the request.\n\n## What is it?\n\nMCP Toolbox is essentially a container for your tools. Think of it as a multi-tool device for your agent; it can hold any tools you create. The agent then decides which specific tool to use based on the user's input.\n\n"
  }
,
  {
    "path": "python/integrations/providers/firecrawl.mdx",
    "filename": "firecrawl.mdx",
    "size_bytes": 584,
    "line_count": 30,
    "preview": "---\ntitle: FireCrawl\n---\n\n>[FireCrawl](https://firecrawl.dev/?ref=langchain) crawls and converts any website into LLM-ready data.\n> It crawls all accessible subpages and give you clean markdown\n> and metadata for each. No sitemap required.\n\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/scrapeless.mdx",
    "filename": "scrapeless.mdx",
    "size_bytes": 1330,
    "line_count": 34,
    "preview": "---\ntitle: Scrapeless\n---\n\n[Scrapeless](https://scrapeless.com) offers flexible and feature-rich data acquisition services with extensive parameter customization and multi-format export support.\n\n## Installation and setup\n\n<CodeGroup>\n```bash pip\n"
  }
,
  {
    "path": "python/integrations/providers/activeloop_deeplake.mdx",
    "filename": "activeloop_deeplake.mdx",
    "size_bytes": 1628,
    "line_count": 43,
    "preview": "---\ntitle: Activeloop Deep Lake\n---\n\n>[Activeloop Deep Lake](https://docs.activeloop.ai/) is a data lake for Deep Learning applications, allowing you to use it\n> as a vector store.\n\n## Why deep lake?\n\n- More than just a (multi-modal) vector store. You can later use the dataset to fine-tune your own LLM models.\n"
  }
,
  {
    "path": "python/integrations/providers/bookendai.mdx",
    "filename": "bookendai.mdx",
    "size_bytes": 413,
    "line_count": 20,
    "preview": "---\ntitle: bookend.ai\n---\n\nLangChain implements an integration with embeddings provided by [bookend.ai](https://bookend.ai/).\n\n\n## Installation and setup\n\n\n"
  }
,
  {
    "path": "python/integrations/providers/perigon.mdx",
    "filename": "perigon.mdx",
    "size_bytes": 2563,
    "line_count": 80,
    "preview": "---\ntitle: Perigon\n---\n\n>[Perigon](https://perigon.io/) is a comprehensive news API that provides access to real-time contextual information in news articles, stories, metadata and wikipedia pages from thousands of sources worldwide.\n>\n\n## Installation and setup\n\n`Perigon` integration exists in its own [partner package](https://pypi.org/project/langchain-perigon/). You can install it with:\n"
  }
,
  {
    "path": "python/integrations/providers/ydb.mdx",
    "filename": "ydb.mdx",
    "size_bytes": 681,
    "line_count": 31,
    "preview": "---\ntitle: YDB\n---\n\nAll functionality related to YDB.\n\n> [YDB](https://ydb.tech/) is a versatile open source Distributed SQL Database that combines\n> high availability and scalability with strong consistency and ACID transactions.\n> It accommodates transactional (OLTP), analytical (OLAP), and streaming workloads simultaneously.\n\n"
  }
,
  {
    "path": "python/integrations/providers/ray_serve.mdx",
    "filename": "ray_serve.mdx",
    "size_bytes": 3313,
    "line_count": 112,
    "preview": "---\ntitle: Ray Serve\n---\n\n[Ray Serve](https://docs.ray.io/en/latest/serve/index.html) is a scalable model serving library for building online inference APIs. Serve is particularly well suited for system composition, enabling you to build a complex inference service consisting of multiple chains and business logic all in Python code.\n\n## Goal of this notebook\n\nThis notebook shows a simple example of how to deploy an OpenAI chain into production. You can extend it to deploy your own self-hosted models where you can easily define amount of hardware resources (GPUs and CPUs) needed to run your model in production efficiently. Read more about available options including autoscaling in the Ray Serve [documentation](https://docs.ray.io/en/latest/serve/getting_started.html).\n\n"
  }
,
  {
    "path": "python/integrations/providers/baseten.mdx",
    "filename": "baseten.mdx",
    "size_bytes": 1342,
    "line_count": 41,
    "preview": "---\ntitle: Baseten\n---\n\n>[Baseten](https://baseten.co) is a provider of all the infrastructure you need to deploy and serve\n> ML models performantly, reliably, and scalably.\n\n>As a model inference platform, `Baseten` is a `Provider` in the LangChain ecosystem.\nThe `Baseten` integration currently implements `Chat Models` and `Embeddings` components.\n\n"
  }
,
  {
    "path": "python/integrations/providers/isaacus.mdx",
    "filename": "isaacus.mdx",
    "size_bytes": 3097,
    "line_count": 65,
    "preview": "---\ntitle: Isaacus\n---\n\n[Isaacus](https://isaacus.com/) is a foundational legal AI research company building AI models, apps, and tools for the legal tech ecosystem.\n\nIsaacus' offering includes [Kanon 2 Embedder](https://isaacus.com/blog/introducing-kanon-2-embedder), the world's best legal embedding model (as measured on the [Massive Legal Embedding Benchmark](https://isaacus.com/blog/introducing-mleb)), as well as [legal zero-shot classification](https://docs.isaacus.com/models/introduction#universal-classification) and [legal extractive question answering models](https://docs.isaacus.com/models/introduction#answer-extraction).\n\nIsaacus offers first-class support for LangChain's embedding interface, accessible via the [`langchain-isaacus`](https://pypi.org/project/langchain-isaacus/) integration package.\n\n"
  }
,
  {
    "path": "python/integrations/providers/dropbox.mdx",
    "filename": "dropbox.mdx",
    "size_bytes": 588,
    "line_count": 29,
    "preview": "---\ntitle: Dropbox\n---\n\n>[Dropbox](https://en.wikipedia.org/wiki/Dropbox) is a file hosting service that brings everything-traditional\n> files, cloud content, and web shortcuts together in one place.\n\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/apache.mdx",
    "filename": "apache.mdx",
    "size_bytes": 3038,
    "line_count": 63,
    "preview": "---\ntitle: Apache Software Foundation\n---\n\n>[The Apache Software Foundation (Wikipedia)](https://en.wikipedia.org/wiki/The_Apache_Software_Foundation)\n> is a decentralized open source community of developers. The software they\n> produce is distributed under the terms of the Apache License, a permissive\n> open-source license for free and open-source software (FOSS). The Apache projects\n> are characterized by a collaborative, consensus-based development process\n> and an open and pragmatic software license, which is to say that it\n"
  }
,
  {
    "path": "python/integrations/providers/cogniswitch.mdx",
    "filename": "cogniswitch.mdx",
    "size_bytes": 1613,
    "line_count": 53,
    "preview": "---\ntitle: CogniSwitch\n---\n\n>[CogniSwitch](https://www.cogniswitch.ai/aboutus) is an API based data platform that\n> enhances enterprise data by extracting entities, concepts and their relationships\n> thereby converting this data into a multidimensional format and storing it in\n> a database that can accommodate these enhancements. In our case the data is stored\n> in a knowledge graph. This enhanced data is now ready for consumption by LLMs and\n> other GenAI applications ensuring the data is consumable and context can be maintained.\n"
  }
,
  {
    "path": "python/integrations/providers/elevenlabs.mdx",
    "filename": "elevenlabs.mdx",
    "size_bytes": 831,
    "line_count": 35,
    "preview": "---\ntitle: ElevenLabs\n---\n\n>[ElevenLabs](https://elevenlabs.io/about) is a voice AI research & deployment company\n> with a mission to make content universally accessible in any language & voice.\n>\n>`ElevenLabs` creates the most realistic, versatile and contextually-aware\n> AI audio, providing the ability to generate speech in hundreds of\n> new and existing voices in 29 languages.\n"
  }
,
  {
    "path": "python/integrations/providers/microsoft.mdx",
    "filename": "microsoft.mdx",
    "size_bytes": 30471,
    "line_count": 843,
    "preview": "---\ntitle: Microsoft\n---\n\nThis page covers all LangChain integrations with [Microsoft Azure](https://portal.azure.com) and other [Microsoft](https://www.microsoft.com) products.\n\n## Chat models\n\nMicrosoft offers three main options for accessing chat models through Azure:\n\n"
  }
,
  {
    "path": "python/integrations/providers/portkey/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 7847,
    "line_count": 181,
    "preview": "---\ntitle: Portkey\n---\n\n[Portkey](https://portkey.ai) is the Control Panel for AI apps. With it's popular AI Gateway and Observability Suite, hundreds of teams ship **reliable**, **cost-efficient**, and **fast** apps.\n\n## LLMOps for LangChain\n\nPortkey brings production readiness to LangChain. With Portkey, you can\n\n"
  }
,
  {
    "path": "python/integrations/providers/portkey/logging_tracing_portkey.mdx",
    "filename": "logging_tracing_portkey.mdx",
    "size_bytes": 5210,
    "line_count": 150,
    "preview": "---\ntitle: Log, Trace, and Monitor\n---\n\nWhen building apps or agents using LangChain, you end up making multiple API calls to fulfill a single user request. However, these requests are not chained when you want to analyse them. With [**Portkey**](/oss/integrations/providers/portkey/), all the embeddings, completions, and other requests from a single user request will get logged and traced to a common ID, enabling you to gain full visibility of user interactions.\n\nThis notebook serves as a step-by-step guide on how to log, trace, and monitor LangChain LLM calls using `Portkey` in your LangChain app.\n\nFirst, let's import Portkey, OpenAI, and Agent tools\n\n"
  }
,
  {
    "path": "python/integrations/providers/forefrontai.mdx",
    "filename": "forefrontai.mdx",
    "size_bytes": 532,
    "line_count": 21,
    "preview": "---\ntitle: Forefront AI\n---\n\n> [Forefront AI](https://forefront.ai/) is a platform enabling you to\n> fine-tune and inference open-source text generation models\n\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/clova.mdx",
    "filename": "clova.mdx",
    "size_bytes": 496,
    "line_count": 16,
    "preview": "---\ntitle: Clova\n---\n\n>[CLOVA Studio](https://api.ncloud-docs.com/docs/ai-naver-clovastudio-summary) is a service\n> of [Naver Cloud Platform](https://www.ncloud.com/) that uses `HyperCLOVA` language models,\n> a hyperscale AI technology, to output phrases generated through AI technology based on user input.\n\n\n## Embedding models\n"
  }
,
  {
    "path": "python/integrations/providers/litellm.mdx",
    "filename": "litellm.mdx",
    "size_bytes": 700,
    "line_count": 31,
    "preview": "---\ntitle: LiteLLM\n---\n\n>[LiteLLM](https://github.com/BerriAI/litellm) is a library that simplifies calling Anthropic, Azure, Huggingface, Replicate, etc.\n\n## Installation and setup\n\n<CodeGroup>\n```bash pip\n"
  }
,
  {
    "path": "python/integrations/providers/daytona.mdx",
    "filename": "daytona.mdx",
    "size_bytes": 1211,
    "line_count": 26,
    "preview": "---\ntitle: Daytona\n---\n\n>[Daytona](https://www.daytona.io/) is dedicated to accelerating AI innovation by providing a secure, lightning-fast runtime that eliminates execution barriers and empowers both developers and AI systems to focus on what truly matters — transforming intelligent code into real-world solutions.\n>Daytona is open source. Check out the [GitHub repository](https://github.com/daytonaio/daytona) to learn more and contribute.\n\n## Installation and setup\n\nInstall the `langchain-daytona-data-analysis` package. For detailed installation instructions, see the [tool usage guide](/oss/integrations/tools/daytona_data_analysis).\n"
  }
,
  {
    "path": "python/integrations/providers/cloudflare.mdx",
    "filename": "cloudflare.mdx",
    "size_bytes": 1026,
    "line_count": 35,
    "preview": "---\ntitle: Cloudflare\n---\n\n>[Cloudflare, Inc. (Wikipedia)](https://en.wikipedia.org/wiki/Cloudflare) is an American company that provides\n> content delivery network services, cloud cybersecurity, DDoS mitigation, and ICANN-accredited\n> domain registration services.\n\n>[Cloudflare Workers AI](https://developers.cloudflare.com/workers-ai/) allows you to run machine\n> learning models, on the `Cloudflare` network, from your code via REST API.\n"
  }
,
  {
    "path": "python/integrations/providers/searchapi.mdx",
    "filename": "searchapi.mdx",
    "size_bytes": 2139,
    "line_count": 82,
    "preview": "---\ntitle: SearchApi\n---\n\nThis page covers how to use the [SearchApi](https://www.searchapi.io/) Google Search API within LangChain. SearchApi is a real-time SERP API for easy SERP scraping.\n\n## Setup\n\n- Go to [https://www.searchapi.io/](https://www.searchapi.io/) to sign up for a free account\n- Get the api key and set it as an environment variable (`SEARCHAPI_API_KEY`)\n"
  }
,
  {
    "path": "python/integrations/providers/yellowbrick.mdx",
    "filename": "yellowbrick.mdx",
    "size_bytes": 394,
    "line_count": 25,
    "preview": "---\ntitle: Yellowbrick\n---\n\n>[Yellowbrick](https://yellowbrick.com/) is a provider of\n> Enterprise Data Warehousing, Ad-hoc and Streaming Analytics,\n> BI and AI workloads.\n\n## Vector store\n\n"
  }
,
  {
    "path": "python/integrations/providers/mistralai.mdx",
    "filename": "mistralai.mdx",
    "size_bytes": 792,
    "line_count": 42,
    "preview": "---\ntitle: MistralAI\n---\n\n>[Mistral AI](https://docs.mistral.ai/api/) is a platform that offers hosting for their powerful open source models.\n\n\n## Installation and setup\n\nA valid [API key](https://console.mistral.ai/users/api-keys/) is needed to communicate with the API.\n"
  }
,
  {
    "path": "python/integrations/providers/ctransformers.mdx",
    "filename": "ctransformers.mdx",
    "size_bytes": 1717,
    "line_count": 59,
    "preview": "---\ntitle: C Transformers\n---\n\nThis page covers how to use the [C Transformers](https://github.com/marella/ctransformers) library within LangChain.\nIt is broken into two parts: installation and setup, and then references to specific C Transformers wrappers.\n\n## Installation and setup\n\n- Install the Python package with `pip install ctransformers`\n"
  }
,
  {
    "path": "python/integrations/providers/bilibili.mdx",
    "filename": "bilibili.mdx",
    "size_bytes": 434,
    "line_count": 25,
    "preview": "---\ntitle: BiliBili\n---\n\n>[Bilibili](https://www.bilibili.tv/) is one of the most beloved long-form video sites in China.\n\n## Installation and setup\n\n<CodeGroup>\n```bash pip\n"
  }
,
  {
    "path": "python/integrations/providers/coze.mdx",
    "filename": "coze.mdx",
    "size_bytes": 459,
    "line_count": 21,
    "preview": "---\ntitle: Coze\n---\n\n[Coze](https://www.coze.com/) is an AI chatbot development platform that enables\nthe creation and deployment of chatbots for handling diverse conversations across\nvarious applications.\n\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/mongodb_atlas.mdx",
    "filename": "mongodb_atlas.mdx",
    "size_bytes": 3052,
    "line_count": 109,
    "preview": "---\ntitle: MongoDB Atlas\n---\n\n>[MongoDB Atlas](https://www.mongodb.com/docs/atlas/) is a fully-managed cloud\n> database available in AWS, Azure, and GCP.  It now has support for native\n> Vector Search on the MongoDB document data.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/opendataloader_pdf.mdx",
    "filename": "opendataloader_pdf.mdx",
    "size_bytes": 2748,
    "line_count": 61,
    "preview": "---\ntitle: OpenDataLoader PDF\n---\n\n> **PDF Parsing for RAG** — Convert to Markdown & JSON, Fast, Local, No GPU\n\n> [OpenDataLoader PDF](https://github.com/opendataloader-project/opendataloader-pdf) converts PDFs into **LLM-ready Markdown and JSON** with accurate reading order, table extraction, and bounding boxes — all running locally on your machine.\n>\n> **Why developers choose OpenDataLoader:**\n> - **Deterministic** — Same input always produces same output (no LLM hallucinations)\n"
  }
,
  {
    "path": "python/integrations/providers/arize.mdx",
    "filename": "arize.mdx",
    "size_bytes": 630,
    "line_count": 32,
    "preview": "---\ntitle: Arize\n---\n\n[Arize](https://arize.com) is an AI observability and LLM evaluation platform that offers\nsupport for LangChain applications, providing detailed traces of input, embeddings, retrieval,\nfunctions, and output messages.\n\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/tencent.mdx",
    "filename": "tencent.mdx",
    "size_bytes": 3389,
    "line_count": 99,
    "preview": "---\ntitle: Tencent\n---\n\n>[Tencent Holdings Ltd. (Wikipedia)](https://en.wikipedia.org/wiki/Tencent) (Chinese: 腾讯; pinyin: Téngxùn)\n> is a Chinese multinational technology conglomerate and holding company headquartered\n> in Shenzhen. `Tencent` is one of the highest grossing multimedia companies in the\n> world based on revenue. It is also the world's largest company in the video game industry\n> based on its equity investments.\n\n"
  }
,
  {
    "path": "python/integrations/providers/privy.mdx",
    "filename": "privy.mdx",
    "size_bytes": 697,
    "line_count": 23,
    "preview": "---\ntitle: Privy\n---\n\n[Privy](https://privy.io) is powerful wallet infrastructure for AI agents, built for scale.\n\nGive agents a tool to:\n\n- Automatically create and manage wallets\n- Make payments in a variety of digital assets, including stablecoins\n"
  }
,
  {
    "path": "python/integrations/providers/supabase.mdx",
    "filename": "supabase.mdx",
    "size_bytes": 826,
    "line_count": 33,
    "preview": "---\ntitle: Supabase (Postgres)\n---\n\n>[Supabase](https://supabase.com/docs) is an open-source `Firebase` alternative.\n> `Supabase` is built on top of `PostgreSQL`, which offers strong `SQL`\n> querying capabilities and enables a simple interface with already-existing tools and frameworks.\n\n>[PostgreSQL](https://en.wikipedia.org/wiki/PostgreSQL) also known as `Postgres`,\n> is a free and open-source relational database management system (RDBMS)\n"
  }
,
  {
    "path": "python/integrations/providers/huawei.mdx",
    "filename": "huawei.mdx",
    "size_bytes": 929,
    "line_count": 45,
    "preview": "---\ntitle: Huawei\n---\n\n>[Huawei Technologies Co., Ltd.](https://www.huawei.com/) is a Chinese multinational\n> digital communications technology corporation.\n>\n>[Huawei Cloud](https://www.huaweicloud.com/intl/en-us/product/) provides a comprehensive suite of\n> global cloud computing services.\n\n"
  }
,
  {
    "path": "python/integrations/providers/moorcheh.mdx",
    "filename": "moorcheh.mdx",
    "size_bytes": 2378,
    "line_count": 71,
    "preview": "---\nkeywords: [moorcheh, vectorstore, semantic-search, embeddings]\n---\n\n# Moorcheh\n\n>[Moorcheh](https://www.moorcheh.ai/) is a lightning-fast semantic search engine and vector store. Instead of using simple distance metrics like L2 or Cosine, Moorcheh uses Maximally Informative Binarization (MIB) and Information-Theoretic Score (ITS) to retrieve accurate document chunks.\n\nThis page covers how to use Moorcheh within LangChain for vector storage, semantic search, and generative AI responses.\n\n"
  }
,
  {
    "path": "python/integrations/providers/naver.mdx",
    "filename": "naver.mdx",
    "size_bytes": 3843,
    "line_count": 95,
    "preview": "---\ntitle: NAVER\n---\n\nAll functionality related to `Naver`, including HyperCLOVA X models, particularly those accessible through `Naver Cloud` [CLOVA Studio](https://clovastudio.ncloud.com/).\n\n> [Naver](https://navercorp.com/) is a global technology company with cutting-edge technologies and a diverse business portfolio including search, commerce, fintech, content, cloud, and AI.\n\n> [Naver Cloud](https://www.navercloudcorp.com/lang/en/) is the cloud computing arm of Naver, a leading cloud service provider offering a comprehensive suite of cloud services to businesses through its [Naver Cloud Platform (NCP)](https://www.ncloud.com/).\n\n"
  }
,
  {
    "path": "python/integrations/providers/gitlab.mdx",
    "filename": "gitlab.mdx",
    "size_bytes": 894,
    "line_count": 33,
    "preview": "---\ntitle: GitLab\n---\n\n>[GitLab Inc.](https://about.gitlab.com/) is an open-core company\n> that operates `GitLab`, a DevOps software package that can develop,\n> secure, and operate software. `GitLab` includes a distributed version\n> control based on Git, including features such as access control, bug tracking,\n> software feature requests, task management, and wikis for every project,\n> as well as snippets.\n"
  }
,
  {
    "path": "python/integrations/providers/bodo.mdx",
    "filename": "bodo.mdx",
    "size_bytes": 2437,
    "line_count": 78,
    "preview": "---\ntitle: Bodo DataFrames\n---\n\n[Bodo DataFrames](https://github.com/bodo-ai/Bodo) is a high performance DataFrame library\nfor large scale Python data processing and drop-in replacement for Pandas; simply replace:\n\n```python\nimport pandas as pd\n```\n"
  }
,
  {
    "path": "python/integrations/providers/diffbot.mdx",
    "filename": "diffbot.mdx",
    "size_bytes": 1501,
    "line_count": 31,
    "preview": "---\ntitle: Diffbot\n---\n\n> [Diffbot](https://docs.diffbot.com/docs) is a suite of ML-based products that make it easy to structure and integrate web data.\n\n## Installation and setup\n\n[Get a free Diffbot API token](https://app.diffbot.com/get-started/) and [follow these instructions](https://docs.diffbot.com/reference/authentication) to authenticate your requests.\n\n"
  }
,
  {
    "path": "python/integrations/providers/aws.mdx",
    "filename": "aws.mdx",
    "size_bytes": 20095,
    "line_count": 619,
    "preview": "---\ntitle: AWS (Amazon)\n---\n\nThis page covers all LangChain integrations with the [Amazon Web Services (AWS)](https://aws.amazon.com/) platform.\n\n{/* ## Installation and setup\n\n<CodeGroup>\n    ```bash pip\n"
  }
,
  {
    "path": "python/integrations/providers/nomic.mdx",
    "filename": "nomic.mdx",
    "size_bytes": 1361,
    "line_count": 67,
    "preview": "---\ntitle: Nomic\n---\n\n>[Nomic](https://www.nomic.ai/) builds tools that enable everyone to interact with AI scale datasets and run AI models on consumer computers.\n>\n>`Nomic` currently offers two products:\n>\n>- `Atlas`: the Visual Data Engine\n>- `GPT4All`: the Open Source Edge Language Model Ecosystem\n"
  }
,
  {
    "path": "python/integrations/providers/anchor_browser.mdx",
    "filename": "anchor_browser.mdx",
    "size_bytes": 1581,
    "line_count": 46,
    "preview": "---\ntitle: Anchor Browser\n---\n\n[Anchor](https://anchorbrowser.io?utm=langchain) is the platform for AI Agentic browser automation, which solves the challenge of automating workflows for web applications that lack APIs or have limited API coverage. It simplifies the creation, deployment, and management of browser-based automations, transforming complex web interactions into simple API endpoints.\n\n`langchain-anchorbrowser` provides 3 main tools:\n- `AnchorContentTool` - For web content extractions in Markdown or HTML format.\n- `AnchorScreenshotTool` - For web page screenshots.\n- `AnchorWebTaskTools` - To perform web tasks.\n"
  }
,
  {
    "path": "python/integrations/providers/spark.mdx",
    "filename": "spark.mdx",
    "size_bytes": 1711,
    "line_count": 51,
    "preview": "---\ntitle: Spark\n---\n\n>[Apache Spark](https://spark.apache.org/) is a unified analytics engine for\n> large-scale data processing. It provides high-level APIs in Scala, Java,\n> Python, and R, and an optimized engine that supports general computation\n> graphs for data analysis. It also supports a rich set of higher-level\n> tools including `Spark SQL` for SQL and DataFrames, `pandas API on Spark`\n> for pandas workloads, `MLlib` for machine learning,\n"
  }
,
  {
    "path": "python/integrations/providers/shaleprotocol.mdx",
    "filename": "shaleprotocol.mdx",
    "size_bytes": 1582,
    "line_count": 47,
    "preview": "---\ntitle: Shale Protocol\n---\n\n[Shale Protocol](https://shaleprotocol.com) provides production-ready inference APIs for open LLMs. It's a Plug & Play API as it's hosted on a highly scalable GPU cloud infrastructure.\n\nOur free tier supports up to 1K daily requests per key as we want to eliminate the barrier for anyone to start building genAI apps with LLMs.\n\nWith Shale Protocol, developers/researchers can create apps and explore the capabilities of open LLMs at no cost.\n\n"
  }
,
  {
    "path": "python/integrations/providers/deepsparse.mdx",
    "filename": "deepsparse.mdx",
    "size_bytes": 1245,
    "line_count": 36,
    "preview": "---\ntitle: DeepSparse\n---\n\nThis page covers how to use the [DeepSparse](https://github.com/neuralmagic/deepsparse) inference runtime within LangChain.\nIt is broken into two parts: installation and setup, and then examples of DeepSparse usage.\n\n## Installation and setup\n\n- Install the Python package with `pip install deepsparse`\n"
  }
,
  {
    "path": "python/integrations/providers/ibm.mdx",
    "filename": "ibm.mdx",
    "size_bytes": 3184,
    "line_count": 72,
    "preview": "---\ntitle: IBM\n---\n\nLangChain integrations related to IBM technologies, including the\n[IBM watsonx.ai](https://www.ibm.com/products/watsonx-ai) platform and DB2 database.\n\n## Watsonx AI\n\nIBM® watsonx.ai™ AI studio is part of the IBM [watsonx](https://www.ibm.com/watsonx)™ AI and data platform, bringing together new generative\n"
  }
,
  {
    "path": "python/integrations/providers/mindsdb.mdx",
    "filename": "mindsdb.mdx",
    "size_bytes": 1137,
    "line_count": 16,
    "preview": "---\ntitle: MindsDB\n---\n\nMindsDB is the platform for customizing AI from enterprise data. With MindsDB and it's nearly 200 integrations to [data sources](https://docs.mindsdb.com/integrations/data-overview) and [AI/ML frameworks](https://docs.mindsdb.com/integrations/ai-overview), any developer can use their enterprise data to customize AI for their purpose, faster and more securely.\n\nWith MindsDB, you can connect any data source to any AI/ML model to implement and automate AI-powered applications. Deploy, serve, and fine-tune models in real-time, utilizing data from databases, vector stores, or applications. Do all that using universal tools developers already know.\n\nMindsDB integrates with LangChain, enabling users to:\n\n"
  }
,
  {
    "path": "python/integrations/providers/nebius.mdx",
    "filename": "nebius.mdx",
    "size_bytes": 3214,
    "line_count": 126,
    "preview": "---\ntitle: Nebius\n---\n\nAll functionality related to Nebius AI Studio\n\n>[Nebius AI Studio](https://studio.nebius.ai/) provides API access to a wide range of state-of-the-art large language models and embedding models for various use cases.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/overview.mdx",
    "filename": "overview.mdx",
    "size_bytes": 24002,
    "line_count": 70,
    "preview": "---\ntitle: LangChain integrations packages\nsidebarTitle: LangChain integrations\nmode: \"wide\"\n---\n{/* File generated automatically by pipeline/tools/partner_pkg_table.py */}\n{/* Do not manually edit */}\n\nLangChain offers an extensive ecosystem with 1000+ integrations across chat & embedding models, tools & toolkits, document loaders, vector stores, and more.\n\n"
  }
,
  {
    "path": "python/integrations/providers/modern_treasury.mdx",
    "filename": "modern_treasury.mdx",
    "size_bytes": 574,
    "line_count": 21,
    "preview": "---\ntitle: Modern Treasury\n---\n\n>[Modern Treasury](https://www.moderntreasury.com/) simplifies complex payment operations. It is a unified platform to power products and processes that move money.\n>- Connect to banks and payment systems\n>- Track transactions and balances in real-time\n>- Automate payment operations for scale\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/github.mdx",
    "filename": "github.mdx",
    "size_bytes": 1220,
    "line_count": 46,
    "preview": "---\ntitle: GitHub\n---\n\n>[GitHub](https://github.com/) is a developer platform that allows developers to create,\n> store, manage and share their code. It uses `Git` software, providing the\n> distributed version control of Git plus access control, bug tracking,\n> software feature requests, task management, continuous integration, and wikis for every project.\n\n\n"
  }
,
  {
    "path": "python/integrations/providers/golden.mdx",
    "filename": "golden.mdx",
    "size_bytes": 1503,
    "line_count": 36,
    "preview": "---\ntitle: Golden\n---\n\n>[Golden](https://golden.com) provides a set of natural language APIs for querying and enrichment using the Golden Knowledge Graph e.g. queries such as: `Products from OpenAI`, `Generative ai companies with series a funding`, and `rappers who invest` can be used to retrieve structured data about relevant entities.\n>\n>The `golden-query` langchain tool is a wrapper on top of the [Golden Query API](https://docs.golden.com/reference/query-api) which enables programmatic access to these results.\n>See the [Golden Query API docs](https://docs.golden.com/reference/query-api) for more information.\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/zotero.mdx",
    "filename": "zotero.mdx",
    "size_bytes": 637,
    "line_count": 19,
    "preview": "---\ntitle: Zotero\n---\n\n[Zotero](https://www.zotero.org/) is an open source reference management system intended for managing bibliographic data and related research materials. You can connect to your personal library, as well as shared group libraries, via the [API](https://www.zotero.org/support/dev/web_api/v3/start). This retriever implementation utilizes [PyZotero](https://github.com/urschrei/pyzotero) to access libraries.\n\n## Installation\n\n```bash\npip install pyzotero\n"
  }
,
  {
    "path": "python/integrations/providers/bananadev.mdx",
    "filename": "bananadev.mdx",
    "size_bytes": 2609,
    "line_count": 76,
    "preview": "---\ntitle: BananaDev\n---\n\n>[Banana](https://www.banana.dev/) provided serverless GPU inference for AI models,\n> a CI/CD build pipeline and a simple Python framework (`Potassium`) to server your models.\n\nThis page covers how to use the [Banana](https://www.banana.dev) ecosystem within LangChain.\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/infinispanvs.mdx",
    "filename": "infinispanvs.mdx",
    "size_bytes": 548,
    "line_count": 17,
    "preview": "---\ntitle: Infinispan VS\n---\n\n> [Infinispan](https://infinispan.org) Infinispan is an open-source in-memory data grid that provides\n> a key/value data store able to hold all types of data, from Java objects to plain text.\n> Since version 15 Infinispan supports vector search over caches.\n\n## Installation and setup\nSee [Get Started](https://infinispan.org/get-started/) to run an Infinispan server, you may want to disable authentication\n"
  }
,
  {
    "path": "python/integrations/providers/cerebriumai.mdx",
    "filename": "cerebriumai.mdx",
    "size_bytes": 750,
    "line_count": 34,
    "preview": "---\ntitle: CerebriumAI\n---\n\n>[Cerebrium](https://docs.cerebrium.ai/cerebrium/getting-started/introduction)  is a serverless GPU infrastructure provider.\n> It provides API access to several LLM models.\n\nSee the examples in the [CerebriumAI documentation](https://docs.cerebrium.ai/examples/langchain).\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/huggingface.mdx",
    "filename": "huggingface.mdx",
    "size_bytes": 5743,
    "line_count": 200,
    "preview": "---\ntitle: Hugging Face\n---\n\nThis page covers all LangChain integrations with [Hugging Face Hub](https://huggingface.co/) and libraries like [transformers](https://huggingface.co/docs/transformers/index), [sentence transformers](https://sbert.net/), and [datasets](https://huggingface.co/docs/datasets/index).\n\n{/* ## Installation and setup\n\n<CodeGroup>\n    ```bash pip\n"
  }
,
  {
    "path": "python/integrations/providers/mlflow_tracking.mdx",
    "filename": "mlflow_tracking.mdx",
    "size_bytes": 4773,
    "line_count": 132,
    "preview": "---\ntitle: MLflow\n---\n\n>[MLflow](https://mlflow.org/) is a versatile, open-source platform for managing workflows and artifacts across the machine learning and generative AI lifecycle. It has built-in integrations with many popular AI and ML libraries, but can be used with any library, algorithm, or deployment tool.\n\nMLflow's [LangChain integration](https://mlflow.org/docs/latest/llms/langchain/autologging.html) provides the following capabilities:\n\n- **[Tracing](https://mlflow.org/docs/latest/llms/langchain/autologging.html)**: Visualize data flows through your LangChain components with one line of code (`mlflow.langchain.autolog()`)\n- **[Experiment Tracking](https://mlflow.org/docs/latest/llms/langchain/index.html#experiment-tracking)**: Log artifacts, code, and metrics from your LangChain runs\n"
  }
,
  {
    "path": "python/integrations/providers/tensorlake.mdx",
    "filename": "tensorlake.mdx",
    "size_bytes": 3311,
    "line_count": 104,
    "preview": "---\ntitle: Tensorlake\n---\n\nTensorlake is the AI Data Cloud that reliably transforms data from unstructured sources into ingestion-ready formats for AI Applications.\n\nThe `langchain-tensorlake` package provides seamless integration between [Tensorlake](https://tensorlake.ai) and [LangChain](https://langchain.com),\nenabling you to build sophisticated document processing agents with enhanced parsing features, like signature detection.\n\n## Tensorlake feature overview\n"
  }
,
  {
    "path": "python/integrations/providers/docugami.mdx",
    "filename": "docugami.mdx",
    "size_bytes": 689,
    "line_count": 30,
    "preview": "---\ntitle: Docugami\n---\n\n>[Docugami](https://docugami.com) converts business documents into a Document XML Knowledge Graph, generating forests\n> of XML semantic trees representing entire documents. This is a rich representation that includes the semantic and\n> structural characteristics of various chunks in the document as an XML tree.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/gutenberg.mdx",
    "filename": "gutenberg.mdx",
    "size_bytes": 354,
    "line_count": 17,
    "preview": "---\ntitle: Gutenberg\n---\n\n>[Project Gutenberg](https://www.gutenberg.org/about/) is an online library of free eBooks.\n\n## Installation and setup\n\nThere isn't any special setup for it.\n\n"
  }
,
  {
    "path": "python/integrations/providers/airtable.mdx",
    "filename": "airtable.mdx",
    "size_bytes": 1189,
    "line_count": 29,
    "preview": "---\ntitle: Airtable\n---\n\n>[Airtable](https://en.wikipedia.org/wiki/Airtable) is a cloud collaboration service.\n`Airtable` is a spreadsheet-database hybrid, with the features of a database but applied to a spreadsheet.\n> The fields in an Airtable table are similar to cells in a spreadsheet, but have types such as 'checkbox',\n> 'phone number', and 'drop-down list', and can reference file attachments like images.\n\n>Users can create a database, set up column types, add records, link tables to one another, collaborate, sort records\n"
  }
,
  {
    "path": "python/integrations/providers/valyu.mdx",
    "filename": "valyu.mdx",
    "size_bytes": 1888,
    "line_count": 72,
    "preview": "---\ntitle: Valyu Deep Search\n---\n\n>[Valyu](https://www.valyu.network/) allows AI applications and agents to search the internet and proprietary data sources for relevant LLM ready information.\n\nThis notebook goes over how to use Valyu in LangChain.\n\nFirst, get an Valyu API key and add it as an environment variable. Get $10 free credit  by [signing up here](https://platform.valyu.network/).\n\n"
  }
,
  {
    "path": "python/integrations/providers/dell.mdx",
    "filename": "dell.mdx",
    "size_bytes": 795,
    "line_count": 30,
    "preview": "---\ntitle: Dell\n---\n\nDell is a global technology company that provides a range of hardware, software, and\nservices, including AI solutions. Their AI portfolio includes purpose-built\ninfrastructure for AI workloads, including Dell PowerScale storage systems optimized\nfor AI data management.\n\n## PowerScale\n"
  }
,
  {
    "path": "python/integrations/providers/xai.mdx",
    "filename": "xai.mdx",
    "size_bytes": 1229,
    "line_count": 41,
    "preview": "---\ntitle: xAI\n---\n\n<Warning>\n    This page makes reference to Grok models provided by [xAI](https://docs.x.ai/docs/overview) - not to be confused with [Groq](https://console.groq.com/docs/overview), a separate AI hardware and software company. See the [Groq provider page](/oss/integrations/providers/groq).\n</Warning>\n\n[xAI](https://console.x.ai) offers an API to interact with Grok models. This example goes over how to use LangChain to interact with xAI models.\n\n"
  }
,
  {
    "path": "python/integrations/providers/hyperbrowser.mdx",
    "filename": "hyperbrowser.mdx",
    "size_bytes": 5998,
    "line_count": 183,
    "preview": "---\ntitle: Hyperbrowser\n---\n\n> [Hyperbrowser](https://hyperbrowser.ai) is a platform for running and scaling headless browsers. It lets you launch and manage browser sessions at scale and provides easy to use solutions for any webscraping needs, such as scraping a single page or crawling an entire site.\n>\n> Key Features:\n>\n> - Instant Scalability - Spin up hundreds of browser sessions in seconds without infrastructure headaches\n> - Simple Integration - Works seamlessly with popular tools like Puppeteer and Playwright\n"
  }
,
  {
    "path": "python/integrations/providers/aimlapi.mdx",
    "filename": "aimlapi.mdx",
    "size_bytes": 1091,
    "line_count": 46,
    "preview": "---\ntitle: AI/ML API\n---\n\n>[AI/ML API](https://aimlapi.com/app/?utm_source=langchain&utm_medium=github&utm_campaign=integration) provides a single API for accessing 300+ hosted foundation models (DeepSeek, Gemini, GPT, and more) with enterprise-grade uptime and throughput.\n\n## Installation and setup\n\n- Install the AI/ML API integration package.\n\n"
  }
,
  {
    "path": "python/integrations/providers/wikipedia.mdx",
    "filename": "wikipedia.mdx",
    "size_bytes": 782,
    "line_count": 36,
    "preview": "---\ntitle: Wikipedia\n---\n\n>[Wikipedia](https://wikipedia.org/) is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. `Wikipedia` is the largest and most-read reference work in history.\n\n\n## Installation and setup\n\n<CodeGroup>\n"
  }
,
  {
    "path": "python/integrations/providers/confluence.mdx",
    "filename": "confluence.mdx",
    "size_bytes": 767,
    "line_count": 30,
    "preview": "---\ntitle: Confluence\n---\n\n>[Confluence](https://www.atlassian.com/software/confluence) is a wiki collaboration platform that saves and organizes all of the project-related material. `Confluence` is a knowledge base that primarily handles content management activities.\n\n\n## Installation and setup\n\n<CodeGroup>\n"
  }
,
  {
    "path": "python/integrations/providers/cognee.mdx",
    "filename": "cognee.mdx",
    "size_bytes": 1237,
    "line_count": 35,
    "preview": "---\ntitle: Cognee\n---\n\nCognee implements scalable, modular ECL (Extract, Cognify, Load) pipelines that allow\nyou to interconnect and retrieve past conversations, documents, and audio\ntranscriptions while reducing hallucinations, developer effort, and cost.\n\nCognee merges graph and vector databases to uncover hidden relationships and new\npatterns in your data. You can automatically model, load and retrieve entities and\n"
  }
,
  {
    "path": "python/integrations/providers/ads4gpts.mdx",
    "filename": "ads4gpts.mdx",
    "size_bytes": 2416,
    "line_count": 98,
    "preview": "---\ntitle: ADS4GPTs\n---\n\n> [ADS4GPTs](https://www.ads4gpts.com/) is building the open monetization backbone of the AI-Native internet. It helps AI applications monetize through advertising with a UX and Privacy first approach.\n\n## Installation and setup\n\n### Using pip\nYou can install the package directly from PyPI:\n"
  }
,
  {
    "path": "python/integrations/providers/pandas.mdx",
    "filename": "pandas.mdx",
    "size_bytes": 715,
    "line_count": 37,
    "preview": "---\ntitle: Pandas\n---\n\n>[pandas](https://pandas.pydata.org) is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\nbuilt on top of the `Python` programming language.\n\n## Installation and setup\n\nInstall the `pandas` package using `pip`:\n"
  }
,
  {
    "path": "python/integrations/providers/premai.mdx",
    "filename": "premai.mdx",
    "size_bytes": 15550,
    "line_count": 434,
    "preview": "---\ntitle: PremAI\n---\n\n[PremAI](https://premai.io/) is an all-in-one platform that simplifies the creation of robust, production-ready applications powered by Generative AI. By streamlining the development process, PremAI allows you to concentrate on enhancing user experience and driving overall growth for your application. You can quickly start using [our platform](https://docs.premai.io/quick-start).\n\n## ChatPremAI\n\nThis example goes over how to use LangChain to interact with different chat models with `ChatPremAI`\n\n"
  }
,
  {
    "path": "python/integrations/providers/zeusdb.mdx",
    "filename": "zeusdb.mdx",
    "size_bytes": 18206,
    "line_count": 612,
    "preview": "---\ntitle: ZeusDB\n---\n\n>[ZeusDB](https://www.zeusdb.com) is a high-performance vector database powered by Rust, offering advanced features like product quantization, persistent storage, and enterprise-grade logging.\n\nThis documentation shows how to use ZeusDB to bring enterprise-grade vector search capabilities to your LangChain applications.\n\n## Quick start\n\n"
  }
,
  {
    "path": "python/integrations/providers/beam.mdx",
    "filename": "beam.mdx",
    "size_bytes": 794,
    "line_count": 36,
    "preview": "---\ntitle: Beam\n---\n\n>[Beam](https://www.beam.cloud/) is a cloud computing platform that allows you to run your code\n> on remote servers with GPUs.\n\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/doctran.mdx",
    "filename": "doctran.mdx",
    "size_bytes": 1232,
    "line_count": 45,
    "preview": "---\ntitle: Doctran\n---\n\n>[Doctran](https://github.com/psychic-api/doctran) is a python package. It uses LLMs and open-source\n> NLP libraries to transform raw text into clean, structured, information-dense documents\n> that are optimized for vector space retrieval. You can think of `Doctran` as a black box where\n> messy strings go in and nice, clean, labelled strings come out.\n\n\n"
  }
,
  {
    "path": "python/integrations/providers/baai.mdx",
    "filename": "baai.mdx",
    "size_bytes": 1675,
    "line_count": 46,
    "preview": "---\ntitle: BAAI\n---\n\n>[Beijing Academy of Artificial Intelligence (BAAI) (Wikipedia)](https://en.wikipedia.org/wiki/Beijing_Academy_of_Artificial_Intelligence),\n> also known as `Zhiyuan Institute`, is a Chinese non-profit artificial\n> intelligence (AI) research laboratory. `BAAI` conducts AI research\n> and is dedicated to promoting collaboration among academia and industry,\n> as well as fostering top talent and a focus on long-term research on\n> the fundamentals of AI technology. As a collaborative hub, BAAI's founding\n"
  }
,
  {
    "path": "python/integrations/providers/grobid.mdx",
    "filename": "grobid.mdx",
    "size_bytes": 1732,
    "line_count": 48,
    "preview": "---\ntitle: Grobid\n---\n\nGROBID is a machine learning library for extracting, parsing, and re-structuring raw documents.\n\nIt is designed and expected to be used to parse academic papers, where it works particularly well.\n\n*Note*: if the articles supplied to Grobid are large documents (e.g. dissertations) exceeding a certain number\nof elements, they might not be processed.\n"
  }
,
  {
    "path": "python/integrations/providers/vectara.mdx",
    "filename": "vectara.mdx",
    "size_bytes": 9797,
    "line_count": 201,
    "preview": "---\ntitle: Vectara\n---\n\n[Vectara](https://vectara.com/) is the trusted AI Assistant and Agent platform which focuses on enterprise readiness for mission-critical applications.\nVectara serverless RAG-as-a-service provides all the components of RAG behind an easy-to-use API, including:\n\n1. A way to extract text from files (PDF, PPT, DOCX, etc)\n2. ML-based chunking that provides state of the art performance.\n3. The [Boomerang](https://vectara.com/how-boomerang-takes-retrieval-augmented-generation-to-the-next-level-via-grounded-generation/) embeddings model.\n"
  }
,
  {
    "path": "python/integrations/providers/sparkllm.mdx",
    "filename": "sparkllm.mdx",
    "size_bytes": 761,
    "line_count": 31,
    "preview": "---\ntitle: SparkLLM\n---\n\n>[SparkLLM](https://xinghuo.xfyun.cn/spark) is a large-scale cognitive model independently developed by iFLYTEK.\nIt has cross-domain knowledge and language understanding ability by learning a large amount of texts, codes and images.\nIt can understand and perform tasks based on natural dialogue.\n\n## Chat models\n\n"
  }
,
  {
    "path": "python/integrations/providers/typesense.mdx",
    "filename": "typesense.mdx",
    "size_bytes": 848,
    "line_count": 30,
    "preview": "---\ntitle: Typesense\n---\n\n> [Typesense](https://typesense.org) is an open-source, in-memory search engine, that you can either\n> [self-host](https://typesense.org/docs/guide/install-typesense.html#option-2-local-machine-self-hosting) or run\n> on [Typesense Cloud](https://cloud.typesense.org/).\n> `Typesense` focuses on performance by storing the entire index in RAM (with a backup on disk) and also\n> focuses on providing an out-of-the-box developer experience by simplifying available options and setting good defaults.\n\n"
  }
,
  {
    "path": "python/integrations/providers/exa_search.mdx",
    "filename": "exa_search.mdx",
    "size_bytes": 3846,
    "line_count": 145,
    "preview": "---\ntitle: Exa\n---\n\n>[Exa](https://exa.ai/) is a knowledge API for AI and developers.\n>\n\n## Installation and setup\n\n`Exa` integration exists in its own [partner package](https://pypi.org/project/langchain-exa/). You can install it with:\n"
  }
,
  {
    "path": "python/integrations/providers/groq.mdx",
    "filename": "groq.mdx",
    "size_bytes": 546,
    "line_count": 15,
    "preview": "---\ntitle: Groq\n---\n\n<Warning>\n    This page makes reference to [Groq](https://console.groq.com/docs/overview), an AI hardware and software company. For information on how to use Grok models (provided by [xAI](https://docs.x.ai/docs/overview)), see the [xAI provider page](/oss/integrations/providers/xai).\n</Warning>\n\n## Model interfaces\n\n"
  }
,
  {
    "path": "python/integrations/providers/hologres.mdx",
    "filename": "hologres.mdx",
    "size_bytes": 1556,
    "line_count": 31,
    "preview": "---\ntitle: Hologres\n---\n\n>[Hologres](https://www.alibabacloud.com/help/en/hologres/latest/introduction) is a unified real-time data warehousing service developed by Alibaba Cloud. You can use Hologres to write, update, process, and analyze large amounts of data in real time.\n>`Hologres` supports standard `SQL` syntax, is compatible with `PostgreSQL`, and supports most PostgreSQL functions. Hologres supports online analytical processing (OLAP) and ad hoc analysis for up to petabytes of data, and provides high-concurrency and low-latency online data services.\n\n>`Hologres` provides **vector database** functionality by adopting [Proxima](https://www.alibabacloud.com/help/en/hologres/latest/vector-processing).\n>`Proxima` is a high-performance software library developed by `Alibaba DAMO Academy`. It allows you to search for the nearest neighbors of vectors. Proxima provides higher stability and performance than similar open-source software such as Faiss. Proxima allows you to search for similar text or image embeddings with high throughput and low latency. Hologres is deeply integrated with Proxima to provide a high-performance vector search service.\n\n"
  }
,
  {
    "path": "python/integrations/providers/awadb.mdx",
    "filename": "awadb.mdx",
    "size_bytes": 523,
    "line_count": 27,
    "preview": "---\ntitle: AwaDB\n---\n\n>[AwaDB](https://github.com/awa-ai/awadb) is an AI Native database for the search and storage of embedding vectors used by LLM Applications.\n\n## Installation and setup\n\n```bash\npip install awadb\n"
  }
,
  {
    "path": "python/integrations/providers/ai21.mdx",
    "filename": "ai21.mdx",
    "size_bytes": 1013,
    "line_count": 59,
    "preview": "---\ntitle: AI21 Labs\n---\n\n>[AI21 Labs](https://www.ai21.com/about) is a company specializing in Natural\n> Language Processing (NLP), which develops AI systems\n> that can understand and generate natural language.\n\nThis page covers how to use the `AI21` ecosystem within `LangChain`.\n\n"
  }
,
  {
    "path": "python/integrations/providers/arangodb.mdx",
    "filename": "arangodb.mdx",
    "size_bytes": 859,
    "line_count": 33,
    "preview": "---\ntitle: ArangoDB\n---\n\n>[ArangoDB](https://github.com/arangodb/arangodb) is a scalable graph database system to\n> drive value from connected data, faster. Native graphs, an integrated search engine, and JSON support, via a single query language. ArangoDB runs on-prem, in the cloud – anywhere.\n\n## Installation and setup\n\nInstall the [ArangoDB Python Driver](https://github.com/ArangoDB-Community/python-arango) package with\n"
  }
,
  {
    "path": "python/integrations/providers/arcgis.mdx",
    "filename": "arcgis.mdx",
    "size_bytes": 894,
    "line_count": 33,
    "preview": "---\ntitle: ArcGIS\n---\n\n>[ArcGIS](https://www.esri.com/en-us/arcgis/about-arcgis/overview) is a family of client,\n> server and online geographic information system software developed and maintained by [Esri](https://www.esri.com/).\n>\n>`ArcGISLoader` uses the `arcgis` package.\n> `arcgis` is a Python library for the vector and raster analysis, geocoding, map making,\n> routing and directions. It administers, organizes and manages users,\n"
  }
,
  {
    "path": "python/integrations/providers/e2b.mdx",
    "filename": "e2b.mdx",
    "size_bytes": 497,
    "line_count": 28,
    "preview": "---\ntitle: E2B\n---\n\n>[E2B](https://e2b.dev/) provides open-source secure sandboxes\n> for AI-generated code execution. See more [here](https://github.com/e2b-dev).\n\n## Installation and setup\n\nYou have to install a python package:\n"
  }
,
  {
    "path": "python/integrations/providers/confident.mdx",
    "filename": "confident.mdx",
    "size_bytes": 850,
    "line_count": 34,
    "preview": "---\ntitle: Confident AI\n---\n\n>[Confident AI](https://confident-ai.com) is a creator of the `DeepEval`.\n>\n>[DeepEval](https://github.com/confident-ai/deepeval) is a package for unit testing LLMs.\n> Using `DeepEval`, everyone can build robust language models through faster iterations\n> using both unit testing and integration testing. `DeepEval provides support for each step in the iteration\n> from synthetic data creation to testing.\n"
  }
,
  {
    "path": "python/integrations/providers/obsidian.mdx",
    "filename": "obsidian.mdx",
    "size_bytes": 401,
    "line_count": 20,
    "preview": "---\ntitle: Obsidian\n---\n\n>[Obsidian](https://obsidian.md/) is a powerful and extensible knowledge base\nthat works on top of your local folder of plain text files.\n\n## Installation and setup\n\nAll instructions are in examples below.\n"
  }
,
  {
    "path": "python/integrations/providers/taiga.mdx",
    "filename": "taiga.mdx",
    "size_bytes": 1181,
    "line_count": 57,
    "preview": "---\ntitle: Taiga\n---\n\n> [Taiga](https://docs.taiga.io/) is an open-source project management platform designed for agile teams, offering features like Kanban, Scrum, and issue tracking.\n\n## Installation and setup\n\nInstall the `langchain-taiga` package:\n\n"
  }
,
  {
    "path": "python/integrations/providers/cnosdb.mdx",
    "filename": "cnosdb.mdx",
    "size_bytes": 3977,
    "line_count": 113,
    "preview": "---\ntitle: CnosDB\n---\n\n> [CnosDB](https://github.com/cnosdb/cnosdb) is an open-source distributed time series database with high performance, high compression rate and high ease of use.\n\n## Installation and setup\n\n```python\npip install cnos-connector\n"
  }
,
  {
    "path": "python/integrations/providers/browserbase.mdx",
    "filename": "browserbase.mdx",
    "size_bytes": 1374,
    "line_count": 36,
    "preview": "---\ntitle: Browserbase\n---\n\n[Browserbase](https://browserbase.com) is a developer platform to reliably run, manage, and monitor headless browsers.\n\nPower your AI data retrievals with:\n- [Serverless Infrastructure](https://docs.browserbase.com/under-the-hood) providing reliable browsers to extract data from complex UIs\n- [Stealth Mode](https://docs.browserbase.com/features/stealth-mode) with included fingerprinting tactics and automatic captcha solving\n- [Session Debugger](https://docs.browserbase.com/features/sessions) to inspect your Browser Session with networks timeline and logs\n"
  }
,
  {
    "path": "python/integrations/providers/openai.mdx",
    "filename": "openai.mdx",
    "size_bytes": 2820,
    "line_count": 76,
    "preview": "---\ntitle: OpenAI\n---\n\nThis page covers all LangChain integrations with [OpenAI](https://en.wikipedia.org/wiki/OpenAI)\n\n{/* ## Installation and setup\n\n<CodeGroup>\n    ```bash pip\n"
  }
,
  {
    "path": "python/integrations/providers/octoai.mdx",
    "filename": "octoai.mdx",
    "size_bytes": 883,
    "line_count": 37,
    "preview": "---\ntitle: OctoAI\n---\n\n>[OctoAI](https://docs.octoai.cloud/docs) offers easy access to efficient compute\n> and enables users to integrate their choice of AI models into applications.\n> The `OctoAI` compute service helps you run, tune, and scale AI applications easily.\n\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/symblai_nebula.mdx",
    "filename": "symblai_nebula.mdx",
    "size_bytes": 646,
    "line_count": 19,
    "preview": "---\ntitle: Nebula\n---\n\nThis page covers how to use [Nebula](https://symbl.ai/nebula), [Symbl.ai](https://symbl.ai/)'s LLM, ecosystem within LangChain.\nIt is broken into two parts: installation and setup, and then references to specific Nebula wrappers.\n\n## Installation and setup\n\n- Get an [Nebula API Key](https://info.symbl.ai/Nebula_Private_Beta.html) and set as environment variable `NEBULA_API_KEY`\n"
  }
,
  {
    "path": "python/integrations/providers/writer.mdx",
    "filename": "writer.mdx",
    "size_bytes": 1923,
    "line_count": 74,
    "preview": "---\ntitle: WRITER\n---\n\nAll functionality related to WRITER\n\n\n>This page covers how to use the [WRITER](https://writer.com/) ecosystem within LangChain. For further information see Writer [docs](https://dev.writer.com/home/introduction).\n>[Palmyra](https://writer.com/blog/palmyra/) is a Large Language Model (LLM) developed by `WRITER`.\n>\n"
  }
,
  {
    "path": "python/integrations/providers/baichuan.mdx",
    "filename": "baichuan.mdx",
    "size_bytes": 742,
    "line_count": 35,
    "preview": "---\ntitle: Baichuan\n---\n\n>[Baichuan Inc.](https://www.baichuan-ai.com/) is a Chinese startup in the era of AGI,\n> dedicated to addressing fundamental human needs: Efficiency, Health, and Happiness.\n\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/apache_doris.mdx",
    "filename": "apache_doris.mdx",
    "size_bytes": 747,
    "line_count": 30,
    "preview": "---\ntitle: Apache Doris\n---\n\n>[Apache Doris](https://doris.apache.org/) is a modern data warehouse for real-time analytics.\nIt delivers lightning-fast analytics on real-time data at scale.\n\n>Usually `Apache Doris` is categorized into OLAP, and it has showed excellent performance\n> in [ClickBench — a Benchmark For Analytical DBMS](https://benchmark.clickhouse.com/).\n> Since it has a super-fast vectorized execution engine, it could also be used as a fast vectordb.\n"
  }
,
  {
    "path": "python/integrations/providers/llmonitor.mdx",
    "filename": "llmonitor.mdx",
    "size_bytes": 681,
    "line_count": 24,
    "preview": "---\ntitle: LLMonitor\n---\n\n>[LLMonitor](https://llmonitor.com?utm_source=langchain&utm_medium=py&utm_campaign=docs) is an open-source observability platform that provides cost and usage analytics, user tracking, tracing and evaluation tools.\n\n## Installation and setup\n\nCreate an account on [llmonitor.com](https://llmonitor.com?utm_source=langchain&utm_medium=py&utm_campaign=docs), then copy your new app's `tracking id`.\n\n"
  }
,
  {
    "path": "python/integrations/providers/zep.mdx",
    "filename": "zep.mdx",
    "size_bytes": 4287,
    "line_count": 87,
    "preview": "---\ntitle: Zep\n---\n\n> Recall, understand, and extract data from chat histories. Power personalized AI experiences.\n\n>[Zep](https://www.getzep.com) is a long-term memory service for AI Assistant apps.\n> With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant,\n> while also reducing hallucinations, latency, and cost.\n\n"
  }
,
  {
    "path": "python/integrations/providers/browserless.mdx",
    "filename": "browserless.mdx",
    "size_bytes": 561,
    "line_count": 20,
    "preview": "---\ntitle: Browserless\n---\n\n>[Browserless](https://www.browserless.io/docs/start) is a service that allows you to\n> run headless Chrome instances in the cloud. It’s a great way to run browser-based\n> automation at scale without having to worry about managing your own infrastructure.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/azlyrics.mdx",
    "filename": "azlyrics.mdx",
    "size_bytes": 358,
    "line_count": 18,
    "preview": "---\ntitle: AZLyrics\n---\n\n>[AZLyrics](https://www.azlyrics.com/) is a large, legal, every day growing collection of lyrics.\n\n## Installation and setup\n\nThere isn't any special setup for it.\n\n"
  }
,
  {
    "path": "python/integrations/providers/johnsnowlabs.mdx",
    "filename": "johnsnowlabs.mdx",
    "size_bytes": 2739,
    "line_count": 120,
    "preview": "---\ntitle: Johnsnowlabs\n---\n\nGain access to the [johnsnowlabs](https://www.johnsnowlabs.com/) ecosystem of enterprise NLP libraries\nwith over 21.000 enterprise NLP models in over 200 languages with the open source `johnsnowlabs` library.\nFor all 24.000+ models, see the [John Snow Labs Model Models Hub](https://nlp.johnsnowlabs.com/models)\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/log10.mdx",
    "filename": "log10.mdx",
    "size_bytes": 3324,
    "line_count": 106,
    "preview": "---\ntitle: Log10\n---\n\nThis page covers how to use the [Log10](https://log10.io) within LangChain.\n\n## What is Log10?\n\nLog10 is an [open-source](https://github.com/log10-io/log10) proxiless LLM data management and application development platform that lets you log, debug and tag your LangChain calls.\n\n"
  }
,
  {
    "path": "python/integrations/providers/sap.mdx",
    "filename": "sap.mdx",
    "size_bytes": 891,
    "line_count": 33,
    "preview": "---\ntitle: SAP\n---\n\n>[SAP SE(Wikipedia)](https://www.sap.com/about/company.html) is a German multinational\n> software company. It develops enterprise software to manage business operation and\n> customer relations. The company is the world's leading\n> `enterprise resource planning (ERP)` software vendor.\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/tomarkdown.mdx",
    "filename": "tomarkdown.mdx",
    "size_bytes": 417,
    "line_count": 18,
    "preview": "---\ntitle: 2Markdown\n---\n\n>[2markdown](https://2markdown.com/) service transforms website content into structured markdown files.\n\n\n## Installation and setup\n\nWe need the `API key`. See [instructions how to get it](https://2markdown.com/login).\n"
  }
,
  {
    "path": "python/integrations/providers/dingo.mdx",
    "filename": "dingo.mdx",
    "size_bytes": 1075,
    "line_count": 39,
    "preview": "---\ntitle: DingoDB\n---\n\n>[DingoDB](https://github.com/dingodb) is a distributed multi-modal vector\n> database. It combines the features of a data lake and a vector database,\n> allowing for the storage of any type of data (key-value, PDF, audio,\n> video, etc.) regardless of its size. Utilizing DingoDB, you can construct\n> your own Vector Ocean (the next-generation data architecture following data\n> warehouse and data lake). This enables\n"
  }
,
  {
    "path": "python/integrations/providers/iflytek.mdx",
    "filename": "iflytek.mdx",
    "size_bytes": 1061,
    "line_count": 46,
    "preview": "---\ntitle: iFlytek\n---\n\n>[iFlytek](https://www.iflytek.com) is a Chinese information technology company\n> established in 1999. It creates voice recognition software and\n> voice-based internet/mobile products covering education, communication,\n> music, intelligent toys industries.\n\n\n"
  }
,
  {
    "path": "python/integrations/providers/mlflow.mdx",
    "filename": "mlflow.mdx",
    "size_bytes": 3055,
    "line_count": 127,
    "preview": "---\ntitle: MLflow AI Gateway for LLMs\n---\n\n>[The MLflow AI Gateway for LLMs](https://www.mlflow.org/docs/latest/llms/deployments/index.html) is a powerful tool designed to streamline the usage and management of various large\n> language model (LLM) providers, such as OpenAI and Anthropic, within an organization. It offers a high-level interface\n> that simplifies the interaction with these services by providing a unified endpoint to handle specific LLM related requests.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/tableau.mdx",
    "filename": "tableau.mdx",
    "size_bytes": 356,
    "line_count": 23,
    "preview": "---\ntitle: Tableau\n---\n\n[Tableau](https://www.tableau.com/) is an analytics platform that enables anyone to\nsee and understand data.\n\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/databricks.mdx",
    "filename": "databricks.mdx",
    "size_bytes": 6544,
    "line_count": 129,
    "preview": "---\ntitle: Databricks\n---\n\n> [Databricks](https://www.databricks.com/) Intelligence Platform is the world's first data intelligence platform powered by generative AI. Infuse AI into every facet of your business.\n\nDatabricks embraces the LangChain ecosystem in various ways:\n\n1. 🚀 **Model Serving** - Access state-of-the-art LLMs, such as DBRX, Llama3, Mixtral, or your fine-tuned models on [Databricks Model Serving](https://www.databricks.com/product/model-serving), via a highly available and low-latency inference endpoint. LangChain provides LLM (`Databricks`), Chat Model (`ChatDatabricks`), and Embeddings (`DatabricksEmbeddings`) implementations, streamlining the integration of your models hosted on Databricks Model Serving with your LangChain applications.\n2. 📃 **Vector Search** - [Databricks Vector Search](https://www.databricks.com/product/machine-learning/vector-search) is a serverless vector database seamlessly integrated within the Databricks Platform. Using `DatabricksVectorSearch`, you can incorporate the highly scalable and reliable similarity search engine into your LangChain applications.\n"
  }
,
  {
    "path": "python/integrations/providers/aim_tracking.mdx",
    "filename": "aim_tracking.mdx",
    "size_bytes": 5278,
    "line_count": 142,
    "preview": "---\ntitle: Aim\n---\n\nAim makes it super easy to visualize and debug LangChain executions. Aim tracks inputs and outputs of LLMs and tools, as well as actions of agents.\n\nWith Aim, you can easily debug and examine an individual execution:\n\n![227784778 06b806c7 74a1 4d15 ab85 9ece09b458aa](https://user-images.githubusercontent.com/13848158/227784778-06b806c7-74a1-4d15-ab85-9ece09b458aa.png)\n\n"
  }
,
  {
    "path": "python/integrations/providers/git.mdx",
    "filename": "git.mdx",
    "size_bytes": 611,
    "line_count": 27,
    "preview": "---\ntitle: Git\n---\n\n>[Git](https://en.wikipedia.org/wiki/Git) is a distributed version control system that tracks changes in any set of computer files, usually used for coordinating work among programmers collaboratively developing source code during software development.\n\n## Installation and setup\n\nFirst, you need to install `GitPython` python package.\n\n"
  }
,
  {
    "path": "python/integrations/providers/langchain_decorators.mdx",
    "filename": "langchain_decorators.mdx",
    "size_bytes": 12060,
    "line_count": 366,
    "preview": "---\ntitle: LangChain Decorators\n---\n\n~~~\nDisclaimer: `LangChain decorators` is not created by the LangChain team and is not supported by it.\n~~~\n\n>`LangChain decorators` is a layer on the top of LangChain that provides syntactic sugar 🍭 for writing custom langchain prompts and chains\n>\n"
  }
,
  {
    "path": "python/integrations/providers/metal.mdx",
    "filename": "metal.mdx",
    "size_bytes": 848,
    "line_count": 26,
    "preview": "---\ntitle: Metal\n---\n\nThis page covers how to use [Metal](https://getmetal.io) within LangChain.\n\n## What is metal?\n\nMetal is a  managed retrieval & memory platform built for production. Easily index your data into `Metal` and run semantic search and retrieval on it.\n\n"
  }
,
  {
    "path": "python/integrations/providers/azure_ai.mdx",
    "filename": "azure_ai.mdx",
    "size_bytes": 4388,
    "line_count": 154,
    "preview": "---\ntitle: Azure AI\n---\n\nThis page covers all LangChain integrations with [Microsoft Azure](https://azure.microsoft.com/) and its related projects.\n\nIntegration packages for Azure AI, Dynamic Sessions, SQL Server are maintained in\nthe [langchain-azure](https://github.com/langchain-ai/langchain-azure) repository.\n\n## Chat models\n"
  }
,
  {
    "path": "python/integrations/providers/infino.mdx",
    "filename": "infino.mdx",
    "size_bytes": 1274,
    "line_count": 43,
    "preview": "---\ntitle: Infino\n---\n\n>[Infino](https://github.com/infinohq/infino) is an open-source observability platform that stores both metrics and application logs together.\n\nKey features of `Infino` include:\n- **Metrics Tracking**: Capture time taken by LLM model to handle request, errors, number of tokens, and costing indication for the particular LLM.\n- **Data Tracking**: Log and store prompt, request, and response data for each LangChain interaction.\n- **Graph Visualization**: Generate basic graphs over time, depicting metrics such as request duration, error occurrences, token count, and cost.\n"
  }
,
  {
    "path": "python/integrations/providers/wolfram_alpha.mdx",
    "filename": "wolfram_alpha.mdx",
    "size_bytes": 1282,
    "line_count": 47,
    "preview": "---\ntitle: Wolfram Alpha\n---\n\n>[WolframAlpha](https://en.wikipedia.org/wiki/WolframAlpha) is an answer engine developed by `Wolfram Research`.\n> It answers factual queries by computing answers from externally sourced data.\n\nThis page covers how to use the `Wolfram Alpha API` within LangChain.\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/zhipuai.mdx",
    "filename": "zhipuai.mdx",
    "size_bytes": 738,
    "line_count": 20,
    "preview": "---\ntitle: Zhipu AI\n---\n\n>[Zhipu AI](https://www.zhipuai.cn/en/aboutus), originating from the technological\n> advancements of `Tsinghua University's Computer Science Department`,\n> is an artificial intelligence company with the mission of teaching machines\n> to think like humans. Its world-leading AI team has developed the cutting-edge\n> large language and multimodal models and built the high-precision billion-scale\n> knowledge graphs, the combination of which uniquely empowers us to create a powerful\n"
  }
,
  {
    "path": "python/integrations/providers/tigris.mdx",
    "filename": "tigris.mdx",
    "size_bytes": 687,
    "line_count": 27,
    "preview": "---\ntitle: Tigris\n---\n\n> [Tigris](https://tigrisdata.com) is an open-source Serverless NoSQL Database and Search Platform designed to simplify building high-performance vector search applications.\n> `Tigris` eliminates the infrastructure complexity of managing, operating, and synchronizing multiple tools, allowing you to focus on building great applications instead.\n\n## Installation and setup\n\n\n"
  }
,
  {
    "path": "python/integrations/providers/vectorize.mdx",
    "filename": "vectorize.mdx",
    "size_bytes": 1502,
    "line_count": 49,
    "preview": "---\ntitle: Vectorize\n---\n\n> [Vectorize](https://vectorize.io/) helps you build AI apps faster and with less hassle.\n> It automates data extraction, finds the best vectorization strategy using RAG evaluation,\n> and lets you quickly deploy real-time RAG pipelines for your unstructured data.\n> Your vector search indexes stay up-to-date, and it integrates with your existing vector database,\n> so you maintain full control of your data.\n> Vectorize handles the heavy lifting, freeing you to focus on building robust AI solutions without getting bogged down by data management.\n"
  }
,
  {
    "path": "python/integrations/providers/yeagerai.mdx",
    "filename": "yeagerai.mdx",
    "size_bytes": 2437,
    "line_count": 50,
    "preview": "---\ntitle: Yeager.ai\n---\n\nThis page covers how to use [Yeager.ai](https://yeager.ai) to generate LangChain tools and agents.\n\n## What is yeager.ai?\nYeager.ai is an ecosystem designed to simplify the process of creating AI agents and tools.\n\nIt features yAgents, a No-code LangChain Agent Builder, which enables users to build, test, and deploy AI solutions with ease. Leveraging the LangChain framework, yAgents allows seamless integration with various language models and resources, making it suitable for developers, researchers, and AI enthusiasts across diverse applications.\n"
  }
,
  {
    "path": "python/integrations/providers/meilisearch.mdx",
    "filename": "meilisearch.mdx",
    "size_bytes": 905,
    "line_count": 37,
    "preview": "---\ntitle: Meilisearch\n---\n\n> [Meilisearch](https://meilisearch.com) is an open-source, lightning-fast, and hyper\n> relevant search engine.\n> It comes with great defaults to help developers build snappy search experiences.\n>\n> You can [self-host Meilisearch](https://www.meilisearch.com/docs/learn/getting_started/installation#local-installation)\n> or run on [Meilisearch Cloud](https://www.meilisearch.com/pricing).\n"
  }
,
  {
    "path": "python/integrations/providers/upstage.mdx",
    "filename": "upstage.mdx",
    "size_bytes": 3495,
    "line_count": 105,
    "preview": "---\ntitle: Upstage\n---\n\n>[Upstage](https://upstage.ai) is a leading artificial intelligence (AI) company specializing in delivering above-human-grade performance LLM components.\n>\n>**Solar Pro** is an enterprise-grade LLM optimized for single-GPU deployment, excelling in instruction-following and processing structured formats like HTML and Markdown. It supports English, Korean, and Japanese with top multilingual performance and offers domain expertise in finance, healthcare, and legal.\n\n>Other than Solar, Upstage also offers features for real-world RAG (retrieval-augmented generation), such as **Document Parse** and **Groundedness Check**.\n\n"
  }
,
  {
    "path": "python/integrations/providers/localai.mdx",
    "filename": "localai.mdx",
    "size_bytes": 956,
    "line_count": 35,
    "preview": "---\ntitle: LocalAI\n---\n\n>[LocalAI](https://localai.io/) is the free, Open Source OpenAI alternative.\n> `LocalAI` act as a drop-in replacement REST API that’s compatible with OpenAI API\n> specifications for local inferencing. It allows you to run LLMs, generate images,\n> audio (and not only) locally or on-prem with consumer grade hardware,\n> supporting multiple model families and architectures.\n\n"
  }
,
  {
    "path": "python/integrations/providers/nvidia.mdx",
    "filename": "nvidia.mdx",
    "size_bytes": 4231,
    "line_count": 82,
    "preview": "---\ntitle: NVIDIA\n---\n\nThe `langchain-nvidia-ai-endpoints` package contains LangChain integrations for chat models and embeddings powered by [NVIDIA AI Foundation Models](https://www.nvidia.com/en-us/ai-data-science/foundation-models/), and hosted on the [NVIDIA API Catalog](https://build.nvidia.com/).\n\nNVIDIA AI Foundation models are community- and NVIDIA-built models that are optimized to deliver the best performance on NVIDIA-accelerated infrastructure. You can use the API to query live endpoints that are available on the NVIDIA API Catalog to get quick results from a DGX-hosted cloud compute environment, or you can download models from NVIDIA's API catalog with NVIDIA NIM, which is included with the NVIDIA AI Enterprise license. The ability to run models on-premises gives your enterprise ownership of your customizations and full control of your IP and AI application.\n\nNIM microservices are packaged as container images on a per model/model family basis and are distributed as NGC container images through the [NVIDIA NGC Catalog](https://catalog.ngc.nvidia.com/). At their core, NIM microservices are containers that provide interactive APIs for running inference on an AI Model.\n\n"
  }
,
  {
    "path": "python/integrations/providers/sambanova.mdx",
    "filename": "sambanova.mdx",
    "size_bytes": 2175,
    "line_count": 61,
    "preview": "---\ntitle: SambaNova\n---\n\nCustomers are turning to [SambaNova](https://sambanova.ai/) to quickly deploy state-of-the-art AI capabilities to gain competitive advantage. Our purpose-built enterprise-scale AI platform is the technology backbone for the next generation of AI computing. We power the foundation models that unlock the valuable business insights trapped in data.\n\nDesigned for AI, the SambaNova RDU was built with a revolutionary dataflow architecture. This design makes the RDU significantly more efficient for these workloads than GPUs as it eliminates redundant calls to memory, which are an inherent limitation of how GPUs function. This built-in efficiency is one of the features that makes the RDU capable of much higher performance than GPUs in a fraction of the footprint.\n\nOn top of our architecture We have developed some platforms that allow companies and developers to get full advantage of the RDU processors and open source models.\n\n"
  }
,
  {
    "path": "python/integrations/providers/snowflake.mdx",
    "filename": "snowflake.mdx",
    "size_bytes": 1229,
    "line_count": 40,
    "preview": "---\ntitle: Snowflake\n---\n\n> [Snowflake](https://www.snowflake.com/) is a cloud-based data-warehousing platform\n> that allows you to store and query large amounts of data.\n\nThis page covers how to use the `Snowflake` ecosystem within `LangChain`.\n\n## Embedding models\n"
  }
,
  {
    "path": "python/integrations/providers/cube.mdx",
    "filename": "cube.mdx",
    "size_bytes": 664,
    "line_count": 23,
    "preview": "---\ntitle: Cube\n---\n\n>[Cube](https://cube.dev/) is the Semantic Layer for building data apps. It helps\n> data engineers and application developers access data from modern data stores,\n> organize it into consistent definitions, and deliver it to every application.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/gradientai.mdx",
    "filename": "gradientai.mdx",
    "size_bytes": 3109,
    "line_count": 103,
    "preview": "---\ntitle: DigitalOcean Gradient AI Platform\n---\n\nThis will help you getting started with DigitalOcean Gradient [chat models](/oss/langchain/models).\n\n## Overview\n### Integration details\n\n| Class | Package | Downloads | Version |\n"
  }
,
  {
    "path": "python/integrations/providers/gpt4all.mdx",
    "filename": "gpt4all.mdx",
    "size_bytes": 1912,
    "line_count": 57,
    "preview": "---\ntitle: GPT4All\n---\n\nThis page covers how to use the `GPT4All` wrapper within LangChain. The tutorial is divided into two parts: installation and setup, followed by usage with an example.\n\n## Installation and setup\n\n- Install the Python package with `pip install gpt4all`\n- Download a [GPT4All model](https://gpt4all.io/index.html) and place it in your desired directory\n"
  }
,
  {
    "path": "python/integrations/providers/weaviate.mdx",
    "filename": "weaviate.mdx",
    "size_bytes": 2201,
    "line_count": 46,
    "preview": "---\ntitle: Weaviate\n---\n\n>[Weaviate](https://weaviate.io/) is an open-source vector database. It allows you to store data objects and vector embeddings from\n>your favorite ML models, and scale seamlessly into billions of data objects.\n\n\nWhat is `Weaviate`?\n- Weaviate is an open-source ​database of the type ​vector search engine.\n"
  }
,
  {
    "path": "python/integrations/providers/clickhouse.mdx",
    "filename": "clickhouse.mdx",
    "size_bytes": 974,
    "line_count": 32,
    "preview": "---\ntitle: ClickHouse\n---\n\n> [ClickHouse](https://clickhouse.com/) is the fast and resource efficient open-source database for real-time\n> apps and analytics with full SQL support and a wide range of functions to assist users in writing analytical queries.\n> It has data structures and distance search functions (like `L2Distance`) as well as\n> [approximate nearest neighbor search indexes](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/annindexes)\n> That enables ClickHouse to be used as a high performance and scalable vector database to store and search vectors with SQL.\n\n"
  }
,
  {
    "path": "python/integrations/providers/connery.mdx",
    "filename": "connery.mdx",
    "size_bytes": 910,
    "line_count": 30,
    "preview": "---\ntitle: Connery\n---\n\n>[Connery SDK](https://github.com/connery-io/connery-sdk) is an NPM package that\n> includes both an SDK and a CLI, designed for the development of plugins and actions.\n>\n>The CLI automates many things in the development process. The SDK\n> offers a JavaScript API for defining plugins and actions and packaging them\n> into a plugin server with a standardized REST API generated from the metadata.\n"
  }
,
  {
    "path": "python/integrations/providers/docarray.mdx",
    "filename": "docarray.mdx",
    "size_bytes": 1080,
    "line_count": 45,
    "preview": "---\ntitle: DocArray\n---\n\n> [DocArray](https://docarray.jina.ai/) is a library for nested, unstructured, multimodal data in transit,\n> including text, image, audio, video, 3D mesh, etc. It allows deep-learning engineers to efficiently process,\n> embed, search, recommend, store, and transfer multimodal data with a Pythonic API.\n\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/fireworks.mdx",
    "filename": "fireworks.mdx",
    "size_bytes": 1211,
    "line_count": 56,
    "preview": "---\ntitle: Fireworks AI\n---\n\n>[Fireworks AI](https://fireworks.ai) is a generative AI inference platform to run and\n> customize models with industry-leading speed and production-readiness.\n\n\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/airbyte.mdx",
    "filename": "airbyte.mdx",
    "size_bytes": 936,
    "line_count": 40,
    "preview": "---\ntitle: Airbyte\n---\n\n>[Airbyte](https://github.com/airbytehq/airbyte) is a data integration platform for ELT pipelines from APIs,\n> databases & files to warehouses & lakes. It has the largest catalog of ELT connectors to data warehouses and databases.\n\n## Installation and setup\n\n<CodeGroup>\n"
  }
,
  {
    "path": "python/integrations/providers/opengradient.mdx",
    "filename": "opengradient.mdx",
    "size_bytes": 2755,
    "line_count": 50,
    "preview": "---\ntitle: OpenGradient\n---\n\n[OpenGradient](https://www.opengradient.ai/) is a decentralized AI computing network enabling globally accessible, permissionless, and verifiable ML model inference.\n\nThe OpenGradient langchain package currently offers a toolkit that allows developers to build their own custom ML inference tools for models on the OpenGradient network. This was previously a challenge because of the context-window polluting nature of large model parameters -- imagine having to give your agent a 200x200 array of floating-point data!\n\nThe toolkit solves this problem by encapsulating all data processing logic within the tool definition itself. This approach keeps the agent's context window clean while giving developers complete flexibility to implement custom data processing and live-data retrieval for their ML models.\n\n"
  }
,
  {
    "path": "python/integrations/providers/maritalk.mdx",
    "filename": "maritalk.mdx",
    "size_bytes": 480,
    "line_count": 29,
    "preview": "---\ntitle: MariTalk\n---\n\n>[MariTalk](https://www.maritaca.ai/en) is an LLM-based chatbot trained to meet the needs of Brazil.\n\n## Installation and setup\n\nYou have to get the MariTalk API key.\n\n"
  }
,
  {
    "path": "python/integrations/providers/telegram.mdx",
    "filename": "telegram.mdx",
    "size_bytes": 672,
    "line_count": 19,
    "preview": "---\ntitle: Telegram\n---\n\n>[Telegram Messenger](https://web.telegram.org/a/) is a globally accessible freemium, cross-platform, encrypted, cloud-based and centralized instant messaging service. The application also provides optional end-to-end encrypted chats and video calling, VoIP, file sharing and several other features.\n\n\n## Installation and setup\n\nSee [setup instructions](/oss/integrations/document_loaders/telegram).\n"
  }
,
  {
    "path": "python/integrations/providers/docling.mdx",
    "filename": "docling.mdx",
    "size_bytes": 1484,
    "line_count": 50,
    "preview": "---\ntitle: Docling\n---\n\n> [Docling](https://github.com/DS4SD/docling) parses PDF, DOCX, PPTX, HTML, and other formats into a rich unified representation including document layout, tables etc., making them ready for generative AI workflows like RAG.\n>\n> This integration provides Docling's capabilities via the `DoclingLoader` document loader.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/koboldai.mdx",
    "filename": "koboldai.mdx",
    "size_bytes": 711,
    "line_count": 22,
    "preview": "---\ntitle: KoboldAI\n---\n\n>[KoboldAI](https://koboldai.com/) is a free, open-source project that allows users to run AI models locally\n> on their own computer.\n> It's a browser-based front-end that can be used for writing or role playing with an AI.\n>[KoboldAI](https://github.com/KoboldAI/KoboldAI-Client) is a \"a browser-based front-end for\n> AI-assisted writing with multiple local & remote AI models...\".\n> It has a public and local API that can be used in LangChain.\n"
  }
,
  {
    "path": "python/integrations/providers/you.mdx",
    "filename": "you.mdx",
    "size_bytes": 387,
    "line_count": 21,
    "preview": "---\ntitle: You\n---\n\n>[You](https://you.com/about) company provides an AI productivity platform.\n\n## Retriever\n\nSee a [usage example](/oss/integrations/retrievers/you-retriever).\n\n"
  }
,
  {
    "path": "python/integrations/providers/predictionguard.mdx",
    "filename": "predictionguard.mdx",
    "size_bytes": 3253,
    "line_count": 90,
    "preview": "---\ntitle: Prediction Guard\n---\n\nThis page covers how to use the Prediction Guard ecosystem within LangChain.\nIt is broken into two parts: installation and setup, and then references to specific Prediction Guard wrappers.\n\nThis integration is maintained in the [langchain-predictionguard](https://github.com/predictionguard/langchain-predictionguard)\npackage.\n\n"
  }
,
  {
    "path": "python/integrations/providers/salesforce.mdx",
    "filename": "salesforce.mdx",
    "size_bytes": 648,
    "line_count": 27,
    "preview": "---\ntitle: Salesforce\n---\n\n[Salesforce](https://www.salesforce.com/) is a cloud-based software company that\nprovides customer relationship management (CRM) solutions and a suite of enterprise\napplications focused on sales, customer service, marketing automation, and analytics.\n\n[langchain-salesforce](https://pypi.org/project/langchain-salesforce/) implements\ntools enabling LLMs to interact with Salesforce data.\n"
  }
,
  {
    "path": "python/integrations/providers/notion.mdx",
    "filename": "notion.mdx",
    "size_bytes": 632,
    "line_count": 22,
    "preview": "---\ntitle: Notion DB\n---\n\n>[Notion](https://www.notion.so/) is a collaboration platform with modified Markdown support that integrates kanban\n> boards, tasks, wikis and databases. It is an all-in-one workspace for notetaking, knowledge and data management,\n> and project and task management.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/llama_index.mdx",
    "filename": "llama_index.mdx",
    "size_bytes": 852,
    "line_count": 40,
    "preview": "---\ntitle: LlamaIndex\n---\n\n>[LlamaIndex](https://www.llamaindex.ai/) is the leading data framework for building LLM applications\n\n\n## Installation and setup\n\nYou need to install the `llama-index` python package.\n"
  }
,
  {
    "path": "python/integrations/providers/mediawikidump.mdx",
    "filename": "mediawikidump.mdx",
    "size_bytes": 1224,
    "line_count": 47,
    "preview": "---\ntitle: MediaWikiDump\n---\n\n>[MediaWiki XML Dumps](https://www.mediawiki.org/wiki/Manual:Importing_XML_dumps) contain the content of a wiki\n> (wiki pages with all their revisions), without the site-related data. A XML dump does not create a full backup\n> of the wiki database, the dump does not contain user accounts, images, edit logs, etc.\n\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/cratedb.mdx",
    "filename": "cratedb.mdx",
    "size_bytes": 6841,
    "line_count": 206,
    "preview": "---\ntitle: CrateDB\n---\n\n> [CrateDB] is a distributed and scalable SQL database for storing and\n> analyzing massive amounts of data in near real-time, even with complex\n> queries. It is PostgreSQL-compatible, based on Lucene, and inheriting\n> from Elasticsearch.\n\n\n"
  }
,
  {
    "path": "python/integrations/providers/singlestore.mdx",
    "filename": "singlestore.mdx",
    "size_bytes": 1175,
    "line_count": 23,
    "preview": "---\ntitle: SingleStore Integration\n---\n\n[SingleStore](https://singlestore.com/) is a high-performance, distributed SQL database designed to excel in both [cloud](https://www.singlestore.com/cloud/) and on-premises environments. It offers a versatile feature set, seamless deployment options, and exceptional performance.\n\nThis integration provides the following components to leverage SingleStore's capabilities:\n\n- **`SingleStoreLoader`**: Load documents directly from a SingleStore database table.\n- **`SingleStoreSemanticCache`**: Use SingleStore as a semantic cache for efficient storage and retrieval of embeddings.\n"
  }
,
  {
    "path": "python/integrations/providers/scraperapi.mdx",
    "filename": "scraperapi.mdx",
    "size_bytes": 1242,
    "line_count": 36,
    "preview": "---\ntitle: ScraperAPI\n---\n\n[ScraperAPI](https://www.scraperapi.com/) enables data collection from any public website with its web scraping API, without worrying about proxies, browsers, or CAPTCHA handling. [langchain-scraperapi](https://github.com/scraperapi/langchain-scraperapi) wraps this service, making it easy for AI agents to browse the web and scrape data from it.\n\n## Installation and setup\n\n<CodeGroup>\n```bash pip\n"
  }
,
  {
    "path": "python/integrations/providers/brave_search.mdx",
    "filename": "brave_search.mdx",
    "size_bytes": 1500,
    "line_count": 37,
    "preview": "---\ntitle: Brave Search\n---\n\n>[Brave Search](https://en.wikipedia.org/wiki/Brave_Search) is a search engine developed by Brave Software.\n> - `Brave Search` uses its own web index. As of May 2022, it covered over 10 billion pages and was used to serve 92%\n> of search results without relying on any third-parties, with the remainder being retrieved\n> server-side from the Bing API or (on an opt-in basis) client-side from Google. According\n> to Brave, the index was kept \"intentionally smaller than that of Google or Bing\" in order to\n> help avoid spam and other low-quality content, with the disadvantage that \"Brave Search is\n"
  }
,
  {
    "path": "python/integrations/providers/deepseek.mdx",
    "filename": "deepseek.mdx",
    "size_bytes": 187,
    "line_count": 9,
    "preview": "---\ntitle: DeepSeek\n---\n\n[DeepSeek](https://www.deepseek.com/) is a Chinese artificial intelligence company that develops LLMs.\n\n```python\nfrom langchain_deepseek import ChatDeepSeek\n```\n"
  }
,
  {
    "path": "python/integrations/providers/starrocks.mdx",
    "filename": "starrocks.mdx",
    "size_bytes": 813,
    "line_count": 29,
    "preview": "---\ntitle: StarRocks\n---\n\n>[StarRocks](https://www.starrocks.io/) is a High-Performance Analytical Database.\n`StarRocks` is a next-gen sub-second MPP database for full analytics scenarios, including multi-dimensional analytics, real-time analytics and ad-hoc query.\n\n>Usually `StarRocks` is categorized into OLAP, and it has showed excellent performance in [ClickBench — a Benchmark For Analytical DBMS](https://benchmark.clickhouse.com/). Since it has a super-fast vectorized execution engine, it could also be used as a fast vectordb.\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/rank_bm25.mdx",
    "filename": "rank_bm25.mdx",
    "size_bytes": 648,
    "line_count": 33,
    "preview": "---\ntitle: rank_bm25\n---\n\n[rank_bm25](https://github.com/dorianbrown/rank_bm25) is an open-source collection of algorithms\ndesigned to query documents and return the most relevant ones, commonly used for creating\nsearch engines.\n\nSee its [project page](https://github.com/dorianbrown/rank_bm25) for available algorithms.\n\n"
  }
,
  {
    "path": "python/integrations/providers/elasticsearch.mdx",
    "filename": "elasticsearch.mdx",
    "size_bytes": 3249,
    "line_count": 125,
    "preview": "---\ntitle: Elasticsearch\n---\n\n> [Elasticsearch](https://www.elastic.co/elasticsearch/) is a distributed, RESTful search and analytics engine.\n> It provides a distributed, multi-tenant-capable full-text search engine with an HTTP web interface and schema-free\n> JSON documents.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/gradient.mdx",
    "filename": "gradient.mdx",
    "size_bytes": 909,
    "line_count": 35,
    "preview": "---\ntitle: Gradient\n---\n\n>[Gradient](https://gradient.ai/) allows to fine tune and get completions on LLMs with a simple web API.\n\n## Installation and setup\n- Install the Python SDK :\n<CodeGroup>\n```bash pip\n"
  }
,
  {
    "path": "python/integrations/providers/gooseai.mdx",
    "filename": "gooseai.mdx",
    "size_bytes": 786,
    "line_count": 29,
    "preview": "---\ntitle: GooseAI\n---\n\n>[GooseAI](https://goose.ai) makes deploying NLP services easier and more accessible.\n> `GooseAI` is a fully managed inference service delivered via API.\n> With feature parity to other well known APIs, `GooseAI` delivers a plug-and-play solution\n> for serving open source language models at the industry's best economics by simply\n> changing 2 lines in your code.\n\n"
  }
,
  {
    "path": "python/integrations/providers/edenai.mdx",
    "filename": "edenai.mdx",
    "size_bytes": 1510,
    "line_count": 64,
    "preview": "---\ntitle: Eden AI\n---\n\n>[Eden AI](https://docs.edenai.co/docs/getting-started-with-eden-ai) user interface (UI)\n> is designed for handling the AI projects. With `Eden AI Portal`,\n> you can perform no-code AI using the best engines for the market.\n\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/datadog_logs.mdx",
    "filename": "datadog_logs.mdx",
    "size_bytes": 584,
    "line_count": 27,
    "preview": "---\ntitle: Datadog Logs\n---\n\n>[Datadog](https://www.datadoghq.com/) is a monitoring and analytics platform for cloud-scale applications.\n\n## Installation and setup\n\n<CodeGroup>\n```bash pip\n"
  }
,
  {
    "path": "python/integrations/providers/apify.mdx",
    "filename": "apify.mdx",
    "size_bytes": 3882,
    "line_count": 100,
    "preview": "---\ntitle: Apify\n---\n\n>[Apify](https://apify.com) is a cloud platform for web scraping and data extraction,\n>which provides an [ecosystem](https://apify.com/store) of more than a thousand\n>ready-made apps called *Actors* for various scraping, crawling, and extraction use cases.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/providers/nlpcloud.mdx",
    "filename": "nlpcloud.mdx",
    "size_bytes": 764,
    "line_count": 39,
    "preview": "---\ntitle: NLPCloud\n---\n\n>[NLP Cloud](https://docs.nlpcloud.com/#introduction) is an artificial intelligence platform that allows you to use the most advanced AI engines, and even train your own engines with your own data.\n\n\n## Installation and setup\n\n- Install the `nlpcloud` package.\n"
  }
,
  {
    "path": "python/integrations/providers/llamafile.mdx",
    "filename": "llamafile.mdx",
    "size_bytes": 938,
    "line_count": 32,
    "preview": "---\ntitle: Llamafile\n---\n\n>[llamafile](https://github.com/Mozilla-Ocho/llamafile) lets you distribute and run LLMs\n> with a single file.\n\n>`llamafile` makes open LLMs much more accessible to both developers and end users.\n> `llamafile` is doing that by combining [llama.cpp](https://github.com/ggerganov/llama.cpp) with\n> [Cosmopolitan Libc](https://github.com/jart/cosmopolitan) into one framework that collapses\n"
  }
,
  {
    "path": "python/integrations/providers/prolog.mdx",
    "filename": "prolog.mdx",
    "size_bytes": 698,
    "line_count": 32,
    "preview": "---\ntitle: SWI-Prolog\n---\n\nSWI-Prolog offers a comprehensive free Prolog environment.\n\n## Installation and setup\n\nOnce SWI-Prolog has been installed, install lanchain-prolog using pip:\n\n"
  }
,
  {
    "path": "python/integrations/providers/milvus.mdx",
    "filename": "milvus.mdx",
    "size_bytes": 542,
    "line_count": 30,
    "preview": "---\ntitle: Milvus\n---\n\n>[Milvus](https://milvus.io/docs/overview.md) is a database that stores, indexes, and manages\n> massive embedding vectors generated by deep neural networks and other machine learning (ML) models.\n\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/linkup.mdx",
    "filename": "linkup.mdx",
    "size_bytes": 1223,
    "line_count": 47,
    "preview": "---\ntitle: Linkup\n---\n\n> [Linkup](https://www.linkup.so/) provides an API to connect LLMs to the web and the Linkup Premium Partner sources.\n\n## Installation and setup\n\nTo use the Linkup provider, you first need a valid API key, which you can find by signing-up [here](https://app.linkup.so/sign-up).\nYou will also need the `langchain-linkup` package, which you can install using pip:\n"
  }
,
  {
    "path": "python/integrations/providers/qdrant.mdx",
    "filename": "qdrant.mdx",
    "size_bytes": 1038,
    "line_count": 48,
    "preview": "---\ntitle: Qdrant\n---\n\n>[Qdrant](https://qdrant.tech/documentation/) (read: quadrant) is a vector similarity search engine.\n> It provides a production-ready service with a convenient API to store, search, and manage\n> points - vectors with an additional payload. `Qdrant` is tailored to extended filtering support.\n\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/semadb.mdx",
    "filename": "semadb.mdx",
    "size_bytes": 1001,
    "line_count": 21,
    "preview": "---\ntitle: SemaDB\n---\n\n>[SemaDB](https://semafind.com/) is a no fuss vector similarity search engine. It provides a low-cost cloud hosted version to help you build AI applications with ease.\n\nWith SemaDB Cloud, our hosted version, no fuss means no pod size calculations, no schema definitions, no partition settings, no parameter tuning, no search algorithm tuning, no complex installation, no complex API. It is integrated with [RapidAPI](https://rapidapi.com/semafind-semadb/api/semadb) providing transparent billing, automatic sharding and an interactive API playground.\n\n## Installation\n\n"
  }
,
  {
    "path": "python/integrations/providers/gitbook.mdx",
    "filename": "gitbook.mdx",
    "size_bytes": 417,
    "line_count": 17,
    "preview": "---\ntitle: GitBook\n---\n\n>[GitBook](https://docs.gitbook.com/) is a modern documentation platform where teams can document everything from products to internal knowledge bases and APIs.\n\n## Installation and setup\n\nThere isn't any special setup for it.\n\n"
  }
,
  {
    "path": "python/integrations/providers/opensearch.mdx",
    "filename": "opensearch.mdx",
    "size_bytes": 839,
    "line_count": 23,
    "preview": "---\ntitle: OpenSearch\n---\n\nThis page covers how to use the OpenSearch ecosystem within LangChain.\nIt is broken into two parts: installation and setup, and then references to specific OpenSearch wrappers.\n\n## Installation and setup\n- Install the Python package with `pip install opensearch-py`\n## Wrappers\n"
  }
,
  {
    "path": "python/integrations/providers/fiddler.mdx",
    "filename": "fiddler.mdx",
    "size_bytes": 657,
    "line_count": 34,
    "preview": "---\ntitle: Fiddler\n---\n\n>[Fiddler](https://www.fiddler.ai/) provides a unified platform to monitor, explain, analyze,\n> and improve ML deployments at an enterprise scale.\n\n## Installation and setup\n\nSet up your model [with Fiddler](https://demo.fiddler.ai):\n"
  }
,
  {
    "path": "python/integrations/providers/voyageai.mdx",
    "filename": "voyageai.mdx",
    "size_bytes": 783,
    "line_count": 40,
    "preview": "---\ntitle: VoyageAI\n---\n\nAll functionality related to VoyageAI\n\n>[VoyageAI](https://www.voyageai.com/) Voyage AI builds embedding models, customized for your domain and company, for better retrieval quality.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/pinecone.mdx",
    "filename": "pinecone.mdx",
    "size_bytes": 2542,
    "line_count": 101,
    "preview": "---\ntitle: Pinecone\n---\n\n>[Pinecone](https://docs.pinecone.io/docs/overview) is a vector database with broad functionality.\n\n\n## Installation and setup\n\nInstall the Python SDK:\n"
  }
,
  {
    "path": "python/integrations/providers/rockset.mdx",
    "filename": "rockset.mdx",
    "size_bytes": 1008,
    "line_count": 34,
    "preview": "---\ntitle: Rockset\n---\n\n>[Rockset](https://rockset.com/product/) is a real-time analytics database service for serving low latency, high concurrency analytical queries at scale. It builds a Converged Index™ on structured and semi-structured data with an efficient store for vector embeddings. Its support for running SQL on schemaless data makes it a perfect choice for running vector search with metadata filters.\n\n## Installation and setup\n\nMake sure you have Rockset account and go to the web console to get the API key. Details can be found on [the website](https://rockset.com/docs/rest-api/).\n\n"
  }
,
  {
    "path": "python/integrations/providers/pubmed.mdx",
    "filename": "pubmed.mdx",
    "size_bytes": 836,
    "line_count": 36,
    "preview": "---\ntitle: PubMed\n---\n\n>[PubMed®](https://pubmed.ncbi.nlm.nih.gov/) by `The National Center for Biotechnology Information, National Library of Medicine`\n> comprises more than 35 million citations for biomedical literature from `MEDLINE`, life science journals, and online books.\n> Citations may include links to full text content from `PubMed Central` and publisher web sites.\n\n## Setup\nYou need to install a python package.\n"
  }
,
  {
    "path": "python/integrations/providers/minimax.mdx",
    "filename": "minimax.mdx",
    "size_bytes": 979,
    "line_count": 35,
    "preview": "---\ntitle: Minimax\n---\n\n>[Minimax](https://api.minimax.chat) is a Chinese startup that provides natural language processing models\n> for companies and individuals.\n\n## Installation and setup\nGet a [Minimax api key](https://api.minimax.chat/user-center/basic-information/interface-key) and set it as an environment variable (`MINIMAX_API_KEY`)\nGet a [Minimax group id](https://api.minimax.chat/user-center/basic-information) and set it as an environment variable (`MINIMAX_GROUP_ID`)\n"
  }
,
  {
    "path": "python/integrations/providers/goat.mdx",
    "filename": "goat.mdx",
    "size_bytes": 1548,
    "line_count": 41,
    "preview": "---\ntitle: GOAT\n---\n\n[GOAT](https://github.com/goat-sdk/goat) is the finance toolkit for AI agents.\n\nCreate agents that can:\n\n- Send and receive payments\n- Purchase physical and digital goods and services\n"
  }
,
  {
    "path": "python/integrations/providers/unstructured.mdx",
    "filename": "unstructured.mdx",
    "size_bytes": 8194,
    "line_count": 231,
    "preview": "---\ntitle: Unstructured\n---\n\n>The `unstructured` package from\n[Unstructured.IO](https://www.unstructured.io/) extracts clean text from raw source documents like\nPDFs and Word documents.\nThis page covers how to use the [`unstructured`](https://github.com/Unstructured-IO/unstructured)\necosystem within LangChain.\n\n"
  }
,
  {
    "path": "python/integrations/providers/remembrall.mdx",
    "filename": "remembrall.mdx",
    "size_bytes": 403,
    "line_count": 12,
    "preview": "---\ntitle: Remembrall\n---\n\n>[Remembrall](https://remembrall.dev/) is a platform that gives a language model\n> long-term memory, retrieval augmented generation, and complete observability.\n\n## Installation and setup\n\nTo get started, [sign in with GitHub on the Remembrall platform](https://remembrall.dev/login)\n"
  }
,
  {
    "path": "python/integrations/providers/runhouse.mdx",
    "filename": "runhouse.mdx",
    "size_bytes": 1216,
    "line_count": 31,
    "preview": "---\ntitle: Runhouse\n---\n\nThis page covers how to use the [Runhouse](https://github.com/run-house/runhouse) ecosystem within LangChain.\nIt is broken into three parts: installation and setup, LLMs, and Embeddings.\n\n## Installation and setup\n- Install the Python SDK with `pip install runhouse`\n- If you'd like to use on-demand cluster, check your cloud credentials with `sky check`\n"
  }
,
  {
    "path": "python/integrations/providers/arthur_tracking.mdx",
    "filename": "arthur_tracking.mdx",
    "size_bytes": 4976,
    "line_count": 100,
    "preview": "---\ntitle: Arthur\n---\n\n>[Arthur](https://arthur.ai) is a model monitoring and observability platform.\n\nThe following guide shows how to run a registered chat LLM with the Arthur callback handler to automatically log model inferences to Arthur.\n\nIf you do not have a model currently onboarded to Arthur, visit our [onboarding guide for generative text models](https://docs.arthur.ai/user-guide/walkthroughs/model-onboarding/generative_text_onboarding.html). For more information about how to use the `Arthur SDK`, visit our [docs](https://docs.arthur.ai/).\n\n"
  }
,
  {
    "path": "python/integrations/providers/astradb.mdx",
    "filename": "astradb.mdx",
    "size_bytes": 5140,
    "line_count": 183,
    "preview": "---\ntitle: Astra DB\n---\n\n> [DataStax Astra DB](https://docs.datastax.com/en/astra-db-serverless/index.html) is a serverless AI-ready database built on `Apache Cassandra®` and made conveniently available through an easy-to-use JSON API.\n\nSee a [tutorial provided by DataStax](https://docs.datastax.com/en/astra/astra-db-vector/tutorials/chatbot.html).\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/spreedly.mdx",
    "filename": "spreedly.mdx",
    "size_bytes": 800,
    "line_count": 17,
    "preview": "---\ntitle: Spreedly\n---\n\n>[Spreedly](https://docs.spreedly.com/) is a service that allows you to securely store credit cards and use them to transact against any number of payment gateways and third party APIs. It does this by simultaneously providing a card tokenization/vault service as well as a gateway and receiver integration service. Payment methods tokenized by Spreedly are stored at `Spreedly`, allowing you to independently store a card and then pass that card to different end points based on your business requirements.\n\n## Installation and setup\n\nSee [setup instructions](/oss/integrations/document_loaders/spreedly).\n\n"
  }
,
  {
    "path": "python/integrations/providers/openllm.mdx",
    "filename": "openllm.mdx",
    "size_bytes": 1592,
    "line_count": 65,
    "preview": "---\ntitle: OpenLLM\n---\n\nOpenLLM lets developers run any **open-source LLMs** as **OpenAI-compatible API** endpoints with **a single command**.\n\n- 🔬 Build for fast and production usages\n- 🚂 Support llama3, qwen2, gemma, etc, and many **quantized** versions [full list](https://github.com/bentoml/openllm-models)\n- ⛓️ OpenAI-compatible API\n- 💬 Built-in ChatGPT like UI\n"
  }
,
  {
    "path": "python/integrations/providers/motherduck.mdx",
    "filename": "motherduck.mdx",
    "size_bytes": 1427,
    "line_count": 58,
    "preview": "---\ntitle: MotherDuck\n---\n\n>[MotherDuck](https://motherduck.com/) is a cloud data warehouse powered by DuckDB.\n\n## Installation and setup\n\nFirst, you need to install `duckdb` python package.\n\n"
  }
,
  {
    "path": "python/integrations/providers/dataforseo.mdx",
    "filename": "dataforseo.mdx",
    "size_bytes": 1408,
    "line_count": 54,
    "preview": "---\ntitle: DataForSEO\n---\n\n>[DataForSeo](https://dataforseo.com/) provides comprehensive SEO and digital marketing data solutions via API.\n\nThis page provides instructions on how to use the DataForSEO search APIs within LangChain.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/featherless-ai.mdx",
    "filename": "featherless-ai.mdx",
    "size_bytes": 744,
    "line_count": 14,
    "preview": "---\ntitle: Featherless AI\n---\n\n[Featherless AI](https://featherless.ai/) is a serverless AI inference platform that offers access to over 4300+ open-source models. Our goal is to make all AI models available for serverless inference. We provide inference via API to a continually expanding library of open-weight models.\n\n# Installation and setup\n`pip install langchain-featherless-ai`\n1. Sign up for an account at [Featherless](https://featherless.ai/register)\n2. Subscribe to a plan and get your API key from [API Keys](https://featherless.ai/account/api-keys)\n"
  }
,
  {
    "path": "python/integrations/providers/searx.mdx",
    "filename": "searx.mdx",
    "size_bytes": 2985,
    "line_count": 92,
    "preview": "---\ntitle: SearxNG Search API\n---\n\nThis page covers how to use the SearxNG search API within LangChain.\nIt is broken into two parts: installation and setup, and then references to the specific SearxNG API wrapper.\n\n## Installation and setup\n\nWhile it is possible to utilize the wrapper in conjunction with  [public searx\n"
  }
,
  {
    "path": "python/integrations/providers/langfuse.mdx",
    "filename": "langfuse.mdx",
    "size_bytes": 7909,
    "line_count": 182,
    "preview": "---\ntitle: Langfuse\n---\n\n> **What is Langfuse?** [Langfuse](https://langfuse.com) is an open source LLM engineering platform that helps teams trace API calls, monitor performance, and debug issues in their AI applications.\n\n## Tracing LangChain\n\n[Langfuse Tracing](https://langfuse.com/docs/tracing) integrates with LangChain using LangChain Callbacks ([Python](https://python.langchain.com/docs/how_to/#callbacks), [JS](https://js.langchain.com/docs/how_to/#callbacks)). Thereby, the Langfuse SDK automatically creates a nested trace for every run of your LangChain applications. This allows you to log, analyze and debug your LangChain application.\n\n"
  }
,
  {
    "path": "python/integrations/providers/robocorp.mdx",
    "filename": "robocorp.mdx",
    "size_bytes": 956,
    "line_count": 45,
    "preview": "---\ntitle: Sema4 (fka Robocorp)\n---\n\n>[Robocorp](https://robocorp.com/) helps build and operate Python workers that run seamlessly anywhere at any scale\n\n\n## Installation and setup\n\nYou need to install `langchain-robocorp` python package:\n"
  }
,
  {
    "path": "python/integrations/providers/spacy.mdx",
    "filename": "spacy.mdx",
    "size_bytes": 580,
    "line_count": 34,
    "preview": "---\ntitle: spaCy\n---\n\n>[spaCy](https://spacy.io/) is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython.\n\n## Installation and setup\n\n\n<CodeGroup>\n"
  }
,
  {
    "path": "python/integrations/providers/jaguar.mdx",
    "filename": "jaguar.mdx",
    "size_bytes": 1852,
    "line_count": 64,
    "preview": "---\ntitle: Jaguar\n---\n\nThis page describes how to use Jaguar vector database within LangChain.\nIt contains three sections: introduction, installation and setup, and Jaguar API.\n\n\n## Introduction\n\n"
  }
,
  {
    "path": "python/integrations/providers/memgraph.mdx",
    "filename": "memgraph.mdx",
    "size_bytes": 1313,
    "line_count": 42,
    "preview": "---\ntitle: Memgraph\n---\n\n>Memgraph is a high-performance, in-memory graph database that is optimized for real-time queries and analytics.\n>Get started with Memgraph by visiting [their website](https://memgraph.com/).\n\n## Installation and setup\n\n- Install the Python SDK with `pip install langchain-memgraph`\n"
  }
,
  {
    "path": "python/integrations/providers/modal.mdx",
    "filename": "modal.mdx",
    "size_bytes": 2697,
    "line_count": 96,
    "preview": "---\ntitle: Modal\n---\n\nThis page covers how to use the Modal ecosystem to run LangChain custom LLMs.\nIt is broken into two parts:\n\n1. Modal installation and web endpoint deployment\n2. Using deployed web endpoint with `LLM` wrapper class.\n\n"
  }
,
  {
    "path": "python/integrations/providers/mongodb.mdx",
    "filename": "mongodb.mdx",
    "size_bytes": 446,
    "line_count": 23,
    "preview": "---\ntitle: MongoDB\n---\n\n>[MongoDB](https://www.mongodb.com/) is a NoSQL, document-oriented\n> database that supports JSON-like documents with a dynamic schema.\n\n**NOTE:**\n- See other `MongoDB` integrations on the [MongoDB Atlas page](/oss/integrations/providers/mongodb_atlas).\n\n"
  }
,
  {
    "path": "python/integrations/providers/clearml_tracking.mdx",
    "filename": "clearml_tracking.mdx",
    "size_bytes": 41869,
    "line_count": 481,
    "preview": "---\ntitle: ClearML\n---\n\n> [ClearML](https://github.com/allegroai/clearml) is a ML/DL development and production suite, it contains 5 main modules:\n>\n> - `Experiment Manager` - Automagical experiment tracking, environments and results\n> - `MLOps` - Orchestration, Automation & Pipelines solution for ML/DL jobs (K8s / Cloud / bare-metal)\n> - `Data-Management` - Fully differentiable data management & version control solution on top of object-storage (S3 / GS / Azure / NAS)\n> - `Model-Serving` - cloud-ready Scalable model serving solution!\n"
  }
,
  {
    "path": "python/integrations/providers/google_serper.mdx",
    "filename": "google_serper.mdx",
    "size_bytes": 2230,
    "line_count": 77,
    "preview": "---\ntitle: Serper\n---\n\nThis page covers how to use the [Serper](https://serper.dev) Google Search API within LangChain. Serper is a low-cost Google Search API that can be used to add answer box, knowledge graph, and organic results data from Google Search.\n\nIt is broken into two parts: setup, and then references to the specific Google Serper wrapper.\n\n## Setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/geopandas.mdx",
    "filename": "geopandas.mdx",
    "size_bytes": 672,
    "line_count": 31,
    "preview": "---\ntitle: Geopandas\n---\n\n>[GeoPandas](https://geopandas.org/) is an open source project to make working\n> with geospatial data in python easier. `GeoPandas` extends the datatypes used by\n> `pandas` to allow spatial operations on geometric types.\n> Geometric operations are performed by `shapely`.\n\n\n"
  }
,
  {
    "path": "python/integrations/providers/box.mdx",
    "filename": "box.mdx",
    "size_bytes": 5115,
    "line_count": 198,
    "preview": "---\ntitle: Box\n---\n\n[Box](https://box.com) is the Intelligent Content Cloud, a single platform that enables\norganizations to fuel collaboration, manage the entire content lifecycle, secure critical content,\nand transform business workflows with enterprise AI. Founded in 2005, Box simplifies work for\nleading global organizations, including AstraZeneca, JLL, Morgan Stanley, and Nationwide.\n\nIn this package, we make available a number of ways to include Box content in your AI workflows.\n"
  }
,
  {
    "path": "python/integrations/providers/pg_embedding.mdx",
    "filename": "pg_embedding.mdx",
    "size_bytes": 609,
    "line_count": 29,
    "preview": "---\ntitle: Postgres Embedding\n---\n\n> [pg_embedding](https://github.com/neondatabase/pg_embedding) is an open-source package for\n> vector similarity search using `Postgres` and the `Hierarchical Navigable Small Worlds`\n> algorithm for approximate nearest neighbor search.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/sqlite.mdx",
    "filename": "sqlite.mdx",
    "size_bytes": 850,
    "line_count": 32,
    "preview": "---\ntitle: SQLite\n---\n\n>[SQLite](https://en.wikipedia.org/wiki/SQLite) is a database engine written in the\n> C programming language. It is not a standalone app; rather, it is a library that\n> software developers embed in their apps. As such, it belongs to the family of\n> embedded databases. It is the most widely deployed database engine, as it is\n> used by several of the top web browsers, operating systems, mobile phones, and other embedded systems.\n\n"
  }
,
  {
    "path": "python/integrations/providers/parallel.mdx",
    "filename": "parallel.mdx",
    "size_bytes": 1297,
    "line_count": 39,
    "preview": "---\ntitle: Parallel\n---\n\nThis page covers all LangChain integrations with [Parallel](https://platform.parallel.ai/)\n\n## Installation and setup\n\nThe `Parallel` integration exists in its own [partner package](https://pypi.org/project/langchain-parallel/). You can install it with:\n\n"
  }
,
  {
    "path": "python/integrations/providers/mariadb.mdx",
    "filename": "mariadb.mdx",
    "size_bytes": 1024,
    "line_count": 43,
    "preview": "---\ntitle: MariaDB\n---\n\nThis page covers how to use the [MariaDB](https://github.com/mariadb/) ecosystem within LangChain.\nIt is broken into two parts: installation and setup, and then references to specific PGVector wrappers.\n\n## Installation\n- Install c/c connector\n\n"
  }
,
  {
    "path": "python/integrations/providers/clickup.mdx",
    "filename": "clickup.mdx",
    "size_bytes": 902,
    "line_count": 21,
    "preview": "---\ntitle: ClickUp\n---\n\n>[ClickUp](https://clickup.com/) is an all-in-one productivity platform that provides small and large teams across industries with flexible and customizable work management solutions, tools, and functions.\n>\n>It is a cloud-based project management solution for businesses of all sizes featuring communication and collaboration tools to help achieve organizational goals.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/wandb.mdx",
    "filename": "wandb.mdx",
    "size_bytes": 1214,
    "line_count": 42,
    "preview": "---\ntitle: Weights & Biases\n---\n\n>[Weights & Biases](https://wandb.ai/) is provider of the AI developer platform to train and\n> fine-tune AI models and develop AI applications.\n\n`Weights & Biase` products can be used to log metrics and artifacts during training,\nand to trace the execution of your code.\n\n"
  }
,
  {
    "path": "python/integrations/providers/xinference.mdx",
    "filename": "xinference.mdx",
    "size_bytes": 2985,
    "line_count": 137,
    "preview": "---\ntitle: Xorbits Inference (Xinference)\n---\n\nThis page demonstrates how to use [Xinference](https://github.com/xorbitsai/inference)\nwith LangChain.\n\n`Xinference` is a powerful and versatile library designed to serve LLMs,\nspeech recognition models, and multimodal models, even on your laptop.\nWith Xorbits Inference, you can effortlessly deploy and serve your or\n"
  }
,
  {
    "path": "python/integrations/providers/rebuff.mdx",
    "filename": "rebuff.mdx",
    "size_bytes": 3813,
    "line_count": 147,
    "preview": "---\ntitle: Rebuff\n---\n\n>[Rebuff](https://docs.rebuff.ai/) is a self-hardening prompt injection detector.\nIt is designed to protect AI applications from prompt injection (PI) attacks through a multi-stage defense.\n\n* [Homepage](https://rebuff.ai)\n* [Playground](https://playground.rebuff.ai)\n* [Docs](https://docs.rebuff.ai)\n"
  }
,
  {
    "path": "python/integrations/providers/ifixit.mdx",
    "filename": "ifixit.mdx",
    "size_bytes": 480,
    "line_count": 18,
    "preview": "---\ntitle: iFixit\n---\n\n>[iFixit](https://www.ifixit.com) is the largest, open repair community on the web. The site contains nearly 100k\n> repair manuals, 200k Questions & Answers on 42k devices, and all the data is licensed under `CC-BY-NC-SA 3.0`.\n\n## Installation and setup\n\nThere isn't any special setup for it.\n"
  }
,
  {
    "path": "python/integrations/providers/aleph_alpha.mdx",
    "filename": "aleph_alpha.mdx",
    "size_bytes": 1221,
    "line_count": 44,
    "preview": "---\ntitle: Aleph Alpha\n---\n\n>[Aleph Alpha](https://docs.aleph-alpha.com/) was founded in 2019 with the mission to research and build the foundational technology for an era of strong AI. The team of international scientists, engineers, and innovators researches, develops, and deploys transformative AI like large language and multimodal models and runs the fastest European commercial AI cluster.\n\n>[The Luminous series](https://docs.aleph-alpha.com/docs/introduction/luminous/) is a family of Large Language Models.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/pipelineai.mdx",
    "filename": "pipelineai.mdx",
    "size_bytes": 514,
    "line_count": 21,
    "preview": "---\ntitle: PipelineAI\n---\n\nThis page covers how to use the PipelineAI ecosystem within LangChain.\nIt is broken into two parts: installation and setup, and then references to specific PipelineAI wrappers.\n\n## Installation and setup\n\n- Install with `pip install pipeline-ai`\n"
  }
,
  {
    "path": "python/integrations/providers/facebook.mdx",
    "filename": "facebook.mdx",
    "size_bytes": 2989,
    "line_count": 111,
    "preview": "---\ntitle: Facebook - Meta\n---\n\n>[Meta Platforms, Inc.](https://www.facebook.com/), doing business as `Meta`, formerly\n> named `Facebook, Inc.`, and `TheFacebook, Inc.`, is an American multinational technology\n> conglomerate. The company owns and operates `Facebook`, `Instagram`, `Threads`,\n> and `WhatsApp`, among other products and services.\n\n## Embedding models\n"
  }
,
  {
    "path": "python/integrations/providers/dria.mdx",
    "filename": "dria.mdx",
    "size_bytes": 673,
    "line_count": 33,
    "preview": "---\ntitle: Dria\n---\n\n>[Dria](https://dria.co/) is a hub of public RAG models for developers to\n> both contribute and utilize a shared embedding lake.\n\nSee more details about the LangChain integration with Dria\nat [this page](https://dria.co/docs/integrations/langchain).\n\n"
  }
,
  {
    "path": "python/integrations/providers/epsilla.mdx",
    "filename": "epsilla.mdx",
    "size_bytes": 705,
    "line_count": 25,
    "preview": "---\ntitle: Epsilla\n---\n\nThis page covers how to use [Epsilla](https://github.com/epsilla-cloud/vectordb) within LangChain.\nIt is broken into two parts: installation and setup, and then references to specific Epsilla wrappers.\n\n## Installation and setup\n\n- Install the Python SDK with `pip/pip3 install pyepsilla`\n"
  }
,
  {
    "path": "python/integrations/providers/llamacpp.mdx",
    "filename": "llamacpp.mdx",
    "size_bytes": 1237,
    "line_count": 48,
    "preview": "---\ntitle: Llama.cpp\n---\n\n>[llama.cpp python](https://github.com/abetlen/llama-cpp-python) library is a simple Python bindings for `@ggerganov`\n>[llama.cpp](https://github.com/ggerganov/llama.cpp).\n>\n>This package provides:\n>\n> - Low-level access to C API via ctypes interface.\n"
  }
,
  {
    "path": "python/integrations/providers/tilores.mdx",
    "filename": "tilores.mdx",
    "size_bytes": 1342,
    "line_count": 36,
    "preview": "---\ntitle: Tilores\n---\n\n[Tilores](https://tilores.io) is a platform that provides advanced entity resolution solutions for data integration and management. Using cutting-edge algorithms, machine learning, and a user-friendly interfaces, Tilores helps organizations match, resolve, and consolidate data from disparate sources, ensuring high-quality, consistent information.\n\n## Installation and setup\n\n```python\npip install -U tilores-langchain\n"
  }
,
  {
    "path": "python/integrations/providers/arxiv.mdx",
    "filename": "arxiv.mdx",
    "size_bytes": 997,
    "line_count": 50,
    "preview": "---\ntitle: Arxiv\n---\n\n>[arXiv](https://arxiv.org/) is an open-access archive for 2 million scholarly articles in the fields of physics,\n> mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and\n> systems science, and economics.\n\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/alchemy.mdx",
    "filename": "alchemy.mdx",
    "size_bytes": 487,
    "line_count": 22,
    "preview": "---\ntitle: Alchemy\n---\n\n>[Alchemy](https://www.alchemy.com) is the platform to build blockchain applications.\n\n## Installation and setup\n\nCheck out the [installation guide](/oss/integrations/document_loaders/blockchain).\n\n"
  }
,
  {
    "path": "python/integrations/providers/html2text.mdx",
    "filename": "html2text.mdx",
    "size_bytes": 560,
    "line_count": 27,
    "preview": "---\ntitle: HTML to text\n---\n\n>[html2text](https://github.com/Alir3z4/html2text/) is a Python package that converts a page of `HTML` into clean, easy-to-read plain `ASCII text`.\n\nThe ASCII also happens to be a valid `Markdown` (a text-to-HTML format).\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/timbr.mdx",
    "filename": "timbr.mdx",
    "size_bytes": 1428,
    "line_count": 34,
    "preview": "---\ntitle: Timbr\n---\n\n>What is `Timbr`?\n\n>- Timbr is a `semantic SQL knowledge graph platform` that specializes in connecting data through ontology-driven semantic layers.\n>- Timbr allows you to represent and query data using business-friendly language, making it ideal for handling complex data relationships and business logic.\n>- Timbr provides `natural language to SQL` capabilities, making it easy to interact with your data using plain English queries.\n>- With Timbr, you can achieve high-performance `semantic data querying`, suitable for production-level analytics and business intelligence.\n"
  }
,
  {
    "path": "python/integrations/providers/wandb_tracking.mdx",
    "filename": "wandb_tracking.mdx",
    "size_bytes": 10635,
    "line_count": 288,
    "preview": "---\ntitle: Weights & Biases tracking\n---\n\nThis notebook goes over how to track your LangChain experiments into one centralized `Weights and Biases` dashboard.\n\nTo learn more about prompt engineering and the callback please refer to this notebook which explains both alongside the resultant dashboards you can expect to see:\n\n<a href=\"https://colab.research.google.com/drive/1DXH4beT4HFaRKy_Vm4PoxhXVDRf7Ym8L?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n"
  }
,
  {
    "path": "python/integrations/providers/anyscale.mdx",
    "filename": "anyscale.mdx",
    "size_bytes": 1157,
    "line_count": 42,
    "preview": "---\ntitle: Anyscale\n---\n\n>[Anyscale](https://www.anyscale.com) is a platform to run, fine tune and scale LLMs via production-ready APIs.\n> [Anyscale Endpoints](https://docs.anyscale.com/endpoints/overview) serve many open-source models in a cost-effective way.\n\n`Anyscale` also provides [an example](https://docs.anyscale.com/endpoints/model-serving/examples/langchain-integration)\nhow to setup `LangChain` with `Anyscale` for advanced chat agents.\n\n"
  }
,
  {
    "path": "python/integrations/providers/ainetwork.mdx",
    "filename": "ainetwork.mdx",
    "size_bytes": 748,
    "line_count": 30,
    "preview": "---\ntitle: AINetwork\n---\n\n>[AI Network](https://www.ainetwork.ai/build-on-ain) is a layer 1 blockchain designed to accommodate\n> large-scale AI models, utilizing a decentralized GPU network powered by the\n> [$AIN token](https://www.ainetwork.ai/token), enriching AI-driven `NFTs` (`AINFTs`).\n\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/stripe.mdx",
    "filename": "stripe.mdx",
    "size_bytes": 529,
    "line_count": 18,
    "preview": "---\ntitle: Stripe\n---\n\n>[Stripe](https://stripe.com/en-ca) is an Irish-American financial services and software as a service (SaaS) company. It offers payment-processing software and application programming interfaces for e-commerce websites and mobile applications.\n\n\n## Installation and setup\n\nSee [setup instructions](/oss/integrations/document_loaders/stripe).\n"
  }
,
  {
    "path": "python/integrations/providers/infinity.mdx",
    "filename": "infinity.mdx",
    "size_bytes": 392,
    "line_count": 13,
    "preview": "---\ntitle: Infinity\n---\n\n>[Infinity](https://github.com/michaelfeil/infinity) allows the creation of text embeddings.\n\n## Text embedding model\n\nThere exists an infinity Embedding model, which you can access with\n```python\n"
  }
,
  {
    "path": "python/integrations/providers/uptrain.mdx",
    "filename": "uptrain.mdx",
    "size_bytes": 623,
    "line_count": 28,
    "preview": "---\ntitle: UpTrain\n---\n\n>[UpTrain](https://uptrain.ai/) is an open-source unified platform to evaluate and\n>improve Generative AI applications. It provides grades for 20+ preconfigured evaluations\n>(covering language, code, embedding use cases), performs root cause analysis on failure\n>cases and gives insights on how to resolve them.\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/openweathermap.mdx",
    "filename": "openweathermap.mdx",
    "size_bytes": 1425,
    "line_count": 58,
    "preview": "---\ntitle: OpenWeatherMap\n---\n\n>[OpenWeatherMap](https://openweathermap.org/api/) provides all essential weather data for a specific location:\n>- Current weather\n>- Minute forecast for 1 hour\n>- Hourly forecast for 48 hours\n>- Daily forecast for 8 days\n>- National weather alerts\n"
  }
,
  {
    "path": "python/integrations/providers/helicone.mdx",
    "filename": "helicone.mdx",
    "size_bytes": 1481,
    "line_count": 53,
    "preview": "---\ntitle: Helicone\n---\n\nThis page covers how to use the [Helicone](https://helicone.ai) ecosystem within LangChain.\n\n## What is helicone?\n\nHelicone is an [open-source](https://github.com/Helicone/helicone) observability platform that proxies your OpenAI traffic and provides you key insights into your spend, latency and usage.\n\n"
  }
,
  {
    "path": "python/integrations/providers/stochasticai.mdx",
    "filename": "stochasticai.mdx",
    "size_bytes": 526,
    "line_count": 19,
    "preview": "---\ntitle: StochasticAI\n---\n\nThis page covers how to use the StochasticAI ecosystem within LangChain.\nIt is broken into two parts: installation and setup, and then references to specific StochasticAI wrappers.\n\n## Installation and setup\n- Install with `pip install stochasticx`\n- Get an StochasticAI api key and set it as an environment variable (`STOCHASTICAI_API_KEY`)\n"
  }
,
  {
    "path": "python/integrations/providers/scrapegraph.mdx",
    "filename": "scrapegraph.mdx",
    "size_bytes": 1485,
    "line_count": 51,
    "preview": "---\ntitle: ScrapeGraph AI\n---\n\n>[ScrapeGraph AI](https://scrapegraphai.com) is a service that provides AI-powered web scraping capabilities.\n>It offers tools for extracting structured data, converting webpages to markdown, and processing local HTML content\n>using natural language prompts.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/bageldb.mdx",
    "filename": "bageldb.mdx",
    "size_bytes": 628,
    "line_count": 29,
    "preview": "---\ntitle: BagelDB\n---\n\n> [BagelDB](https://www.bageldb.ai/) (`Open Vector Database for AI`), is like GitHub for AI data.\nIt is a collaborative platform where users can create,\nshare, and manage vector datasets. It can support private projects for independent developers,\ninternal collaborations for enterprises, and public contributions for data DAOs.\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/tigergraph.mdx",
    "filename": "tigergraph.mdx",
    "size_bytes": 732,
    "line_count": 33,
    "preview": "---\ntitle: TigerGraph\n---\n\n>[TigerGraph](https://www.tigergraph.com/tigergraph-db/) is a natively distributed and high-performance graph database.\n> The storage of data in a graph format of vertices and edges leads to rich relationships,\n> ideal for grouding LLM responses.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/stackexchange.mdx",
    "filename": "stackexchange.mdx",
    "size_bytes": 1123,
    "line_count": 44,
    "preview": "---\ntitle: Stack Exchange\n---\n\n>[Stack Exchange](https://en.wikipedia.org/wiki/Stack_Exchange) is a network of\nquestion-and-answer (Q&A) websites on topics in diverse fields, each site covering\na specific topic, where questions, answers, and users are subject to a reputation award process.\n\nThis page covers how to use the `Stack Exchange API` within LangChain.\n\n"
  }
,
  {
    "path": "python/integrations/providers/blackboard.mdx",
    "filename": "blackboard.mdx",
    "size_bytes": 1034,
    "line_count": 24,
    "preview": "---\ntitle: Blackboard\n---\n\n>[Blackboard Learn](https://en.wikipedia.org/wiki/Blackboard_Learn) (previously the `Blackboard Learning Management System`)\n> is a web-based virtual learning environment and learning management system developed by Blackboard Inc.\n> The software features course management, customizable open architecture, and scalable design that allows\n> integration with student information systems and authentication protocols. It may be installed on local servers,\n> hosted by `Blackboard ASP Solutions`, or provided as Software as a Service hosted on Amazon Web Services.\n> Its main purposes are stated to include the addition of online elements to courses traditionally delivered\n"
  }
,
  {
    "path": "python/integrations/providers/google.mdx",
    "filename": "google.mdx",
    "size_bytes": 22022,
    "line_count": 519,
    "preview": "---\ntitle: Google\n---\n\nThis page covers all LangChain integrations with [Google Gemini](https://ai.google.dev/gemini-api/docs), [Google Cloud](https://cloud.google.com/), and other Google products (such as Google Maps, YouTube, and [more](#other-google-products)).\n\n<Note>\n    **Unified SDK & Package Consolidation**\n\n    As of `langchain-google-genai` 4.0.0, this package uses the consolidated [`google-genai`](https://googleapis.github.io/python-genai/) SDK and now supports **both the Gemini Developer API and Vertex AI** backends.\n"
  }
,
  {
    "path": "python/integrations/providers/whatsapp.mdx",
    "filename": "whatsapp.mdx",
    "size_bytes": 586,
    "line_count": 20,
    "preview": "---\ntitle: WhatsApp\n---\n\n>[WhatsApp](https://www.whatsapp.com/) (also called `WhatsApp Messenger`) is a freeware, cross-platform, centralized instant messaging (IM) and voice-over-IP (VoIP) service. It allows users to send text and voice messages, make voice and video calls, and share images, documents, user locations, and other content.\n\n\n## Installation and setup\n\nThere isn't any special setup for it.\n"
  }
,
  {
    "path": "python/integrations/providers/konlpy.mdx",
    "filename": "konlpy.mdx",
    "size_bytes": 397,
    "line_count": 28,
    "preview": "---\ntitle: KoNLPY\n---\n\n>[KoNLPy](https://konlpy.org/) is a Python package for natural language processing (NLP)\n> of the Korean language.\n\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/yandex.mdx",
    "filename": "yandex.mdx",
    "size_bytes": 1111,
    "line_count": 64,
    "preview": "---\ntitle: Yandex\n---\n\nAll functionality related to Yandex Cloud\n\n>[Yandex Cloud](https://cloud.yandex.com/en/) is a public cloud platform.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/lancedb.mdx",
    "filename": "lancedb.mdx",
    "size_bytes": 684,
    "line_count": 25,
    "preview": "---\ntitle: LanceDB\n---\n\nThis page covers how to use [LanceDB](https://github.com/lancedb/lancedb) within LangChain.\nIt is broken into two parts: installation and setup, and then references to specific LanceDB wrappers.\n\n## Installation and setup\n\n- Install the Python SDK with `pip install lancedb`\n"
  }
,
  {
    "path": "python/integrations/providers/jenkins.mdx",
    "filename": "jenkins.mdx",
    "size_bytes": 539,
    "line_count": 25,
    "preview": "---\ntitle: Jenkins\n---\n\n[Jenkins](https://www.jenkins.io/) is an open-source automation platform that enables\nsoftware teams to streamline their development workflows. It's widely adopted in the\nDevOps community as a tool for automating the building, testing, and deployment of\napplications through CI/CD pipelines.\n\n\n"
  }
,
  {
    "path": "python/integrations/providers/upstash.mdx",
    "filename": "upstash.mdx",
    "size_bytes": 7077,
    "line_count": 216,
    "preview": "---\ntitle: Upstash\n---\n\nUpstash offers developers serverless databases and messaging\nplatforms to build powerful applications without having to worry\nabout the operational complexity of running databases at scale.\n\nOne significant advantage of Upstash is that their databases support HTTP and all of their SDKs use HTTP.\nThis means that you can run this in serverless platforms, edge or any platform that does not support TCP connections.\n"
  }
,
  {
    "path": "python/integrations/providers/ragatouille.mdx",
    "filename": "ragatouille.mdx",
    "size_bytes": 4653,
    "line_count": 135,
    "preview": "---\ntitle: RAGatouille\n---\n\n>[RAGatouille](https://github.com/bclavie/RAGatouille) makes it as simple as can be to use `ColBERT`! [ColBERT](https://github.com/stanford-futuredata/ColBERT) is a fast and accurate retrieval model, enabling scalable BERT-based search over large text collections in tens of milliseconds.\n>\n>See the [ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction](https://arxiv.org/abs/2112.01488) paper.\n\nThere are multiple ways that we can use RAGatouille.\n\n"
  }
,
  {
    "path": "python/integrations/providers/marqo.mdx",
    "filename": "marqo.mdx",
    "size_bytes": 2156,
    "line_count": 35,
    "preview": "---\ntitle: Marqo\n---\n\nThis page covers how to use the Marqo ecosystem within LangChain.\n\n### **What is marqo?**\n\nMarqo is a tensor search engine that uses embeddings stored in in-memory HNSW indexes to achieve cutting edge search speeds. Marqo can scale to hundred-million document indexes with horizontal index sharding and allows for async and non-blocking data upload and search. Marqo uses the latest machine learning models from PyTorch, Huggingface, OpenAI and more. You can start with a pre-configured model or bring your own. The built in ONNX support and conversion allows for faster inference and higher throughput on both CPU and GPU.\n\n"
  }
,
  {
    "path": "python/integrations/providers/nuclia.mdx",
    "filename": "nuclia.mdx",
    "size_bytes": 2078,
    "line_count": 92,
    "preview": "---\ntitle: Nuclia\n---\n\n>[Nuclia](https://nuclia.com) automatically indexes your unstructured data from any internal\n> and external source, providing optimized search results and generative answers.\n> It can handle video and audio transcription, image content extraction, and document parsing.\n\n\n\n"
  }
,
  {
    "path": "python/integrations/providers/konko.mdx",
    "filename": "konko.mdx",
    "size_bytes": 2617,
    "line_count": 74,
    "preview": "---\ntitle: Konko\n---\n\nAll functionality related to Konko\n\n>[Konko AI](https://www.konko.ai/) provides a fully managed API to help application developers\n\n>1. **Select** the right open source or proprietary LLMs for their application\n>2. **Build** applications faster with integrations to leading application frameworks and fully managed APIs\n"
  }
,
  {
    "path": "python/integrations/providers/runpod.mdx",
    "filename": "runpod.mdx",
    "size_bytes": 2960,
    "line_count": 91,
    "preview": "---\ntitle: Runpod\n---\n\n[RunPod](https://www.runpod.io/) provides GPU cloud infrastructure, including Serverless endpoints optimized for deploying and scaling AI models.\n\nThis guide covers how to use the `langchain-runpod` integration package to connect LangChain applications to models hosted on [RunPod Serverless](https://www.runpod.io/serverless-gpu).\n\nThe integration offers interfaces for both standard Language Models (LLMs) and Chat Models.\n\n"
  }
,
  {
    "path": "python/integrations/providers/gel.mdx",
    "filename": "gel.mdx",
    "size_bytes": 1517,
    "line_count": 64,
    "preview": "---\ntitle: Gel\n---\n\n[Gel](https://www.geldata.com/) is a powerful data platform built on top of PostgreSQL.\n\n- Think in objects and graphs instead of tables and JOINs.\n- Use the advanced Python SDK, integrated GUI, migrations engine, Auth and AI layers, and much more.\n- Run locally, remotely, or in a [fully managed cloud](https://www.geldata.com/cloud).\n\n"
  }
,
  {
    "path": "python/integrations/providers/everlyai.mdx",
    "filename": "everlyai.mdx",
    "size_bytes": 495,
    "line_count": 19,
    "preview": "---\ntitle: Everly AI\n---\n\n> [Everly AI](https://everlyai.xyz/) allows you to run your ML models at scale in the cloud.\n> It also provides API access to [several LLM models](https://everlyai.xyz/).\n\n## Installation and setup\n\nTo use `Everly AI`, you will need an API key. Visit\n"
  }
,
  {
    "path": "python/integrations/providers/analyticdb.mdx",
    "filename": "analyticdb.mdx",
    "size_bytes": 1223,
    "line_count": 39,
    "preview": "---\ntitle: AnalyticDB\n---\n\n>[AnalyticDB for PostgreSQL](https://www.alibabacloud.com/help/en/analyticdb-for-postgresql/latest/product-introduction-overview)\n> is a massively parallel processing (MPP) data warehousing service\n> from [Alibaba Cloud](https://www.alibabacloud.com/)\n>that is designed to analyze large volumes of data online.\n\n>`AnalyticDB for PostgreSQL` is developed based on the open-source `Greenplum Database`\n"
  }
,
  {
    "path": "python/integrations/providers/oxylabs.mdx",
    "filename": "oxylabs.mdx",
    "size_bytes": 604,
    "line_count": 26,
    "preview": "---\ntitle: Oxylabs\n---\n\n[Oxylabs](https://oxylabs.io/) is a market-leading web intelligence collection platform, driven by the highest business,\nethics, and compliance standards, enabling companies worldwide to unlock data-driven insights.\n\n[langchain-oxylabs](https://pypi.org/project/langchain-oxylabs/) implements\ntools enabling LLMs to interact with Oxylabs Web Scraper API.\n\n"
  }
,
  {
    "path": "python/integrations/providers/valthera.mdx",
    "filename": "valthera.mdx",
    "size_bytes": 2712,
    "line_count": 71,
    "preview": "---\ntitle: Valthera\n---\n\n> [Valthera](https://github.com/valthera/valthera) is an open-source framework that empowers LLM Agents to drive meaningful, context-aware user engagement. It evaluates user motivation and ability in real time, ensuring that notifications and actions are triggered only when users are most receptive.\n>\n> **langchain-valthera** integrates Valthera with LangChain, enabling developers to build smarter, behavior-driven engagement systems that deliver personalized interactions.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/youtube.mdx",
    "filename": "youtube.mdx",
    "size_bytes": 699,
    "line_count": 31,
    "preview": "---\ntitle: YouTube\n---\n\n>[YouTube](https://www.youtube.com/) is an online video sharing and social media platform by Google.\n> We download the `YouTube` transcripts and video information.\n\n## Installation and setup\n\n<CodeGroup>\n"
  }
,
  {
    "path": "python/integrations/providers/promptlayer.mdx",
    "filename": "promptlayer.mdx",
    "size_bytes": 1297,
    "line_count": 56,
    "preview": "---\ntitle: PromptLayer\n---\n\n>[PromptLayer](https://docs.promptlayer.com/introduction) is a platform for prompt engineering.\n> It also helps with the LLM observability to visualize requests, version prompts, and track usage.\n>\n>While `PromptLayer` does have LLMs that integrate directly with LangChain (e.g.\n> [`PromptLayerOpenAI`](https://docs.promptlayer.com/languages/langchain)),\n> using a callback is the recommended way to integrate `PromptLayer` with LangChain.\n"
  }
,
  {
    "path": "python/integrations/providers/usearch.mdx",
    "filename": "usearch.mdx",
    "size_bytes": 886,
    "line_count": 33,
    "preview": "---\ntitle: USearch\n---\n\n>[USearch](https://unum-cloud.github.io/usearch/) is a Smaller & Faster Single-File Vector Search Engine.\n\n>`USearch's` base functionality is identical to `FAISS`, and the interface should look\n> familiar if you have ever investigated Approximate Nearest Neighbors search.\n> `USearch` and `FAISS` both employ `HNSW` algorithm, but they differ significantly\n> in their design principles. `USearch` is compact and broadly compatible with FAISS without\n"
  }
,
  {
    "path": "python/integrations/providers/trubrics.mdx",
    "filename": "trubrics.mdx",
    "size_bytes": 602,
    "line_count": 31,
    "preview": "---\ntitle: Trubrics\n---\n\n>[Trubrics](https://trubrics.com) is an LLM user analytics platform that lets you collect, analyse and manage user\nprompts & feedback on AI models.\n>\n>Check out [Trubrics repo](https://github.com/trubrics/trubrics-sdk) for more information on `Trubrics`.\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/etherscan.mdx",
    "filename": "etherscan.mdx",
    "size_bytes": 483,
    "line_count": 20,
    "preview": "---\ntitle: Etherscan\n---\n\n>[Etherscan](https://docs.etherscan.io/) is the leading blockchain explorer,\n> search, API and analytics platform for `Ethereum`, a decentralized smart contracts platform.\n\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/deeplake.mdx",
    "filename": "deeplake.mdx",
    "size_bytes": 385,
    "line_count": 24,
    "preview": "---\ntitle: Deeplake\n---\n\n[Deeplake](https://www.deeplake.ai/) is a database optimized for AI and deep learning\napplications.\n\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/arcee.mdx",
    "filename": "arcee.mdx",
    "size_bytes": 746,
    "line_count": 32,
    "preview": "---\ntitle: Arcee\n---\n\n>[Arcee](https://www.arcee.ai/about/about-us) enables the development and advancement\n> of what we coin as SLMs—small, specialized, secure, and scalable language models.\n> By offering a SLM Adaptation System and a seamless, secure integration,\n> `Arcee` empowers enterprises to harness the full potential of\n> domain-adapted language models, driving the transformative\n> innovation in operations.\n"
  }
,
  {
    "path": "python/integrations/providers/trulens.mdx",
    "filename": "trulens.mdx",
    "size_bytes": 2930,
    "line_count": 90,
    "preview": "---\ntitle: TruLens\n---\n\n>[TruLens](https://trulens.org) is an [open-source](https://github.com/truera/trulens) package that provides instrumentation and evaluation tools for large language model (LLM) based applications.\n\nThis page covers how to use [TruLens](https://trulens.org) to evaluate and track LLM apps built on langchain.\n\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/psychic.mdx",
    "filename": "psychic.mdx",
    "size_bytes": 1840,
    "line_count": 38,
    "preview": "---\ntitle: Psychic\n---\n\n<Warning>\nThis provider is no longer maintained, and may not work. Use with caution.\n</Warning>\n\n>[Psychic](https://www.psychic.dev/) is a platform for integrating with SaaS tools like `Notion`, `Zendesk`,\n> `Confluence`, and `Google Drive` via OAuth and syncing documents from these applications to your SQL or vector\n"
  }
,
  {
    "path": "python/integrations/providers/yi.mdx",
    "filename": "yi.mdx",
    "size_bytes": 826,
    "line_count": 25,
    "preview": "---\ntitle: 01.AI\n---\n\n>[01.AI](https://www.lingyiwanwu.com/en), founded by Dr. Kai-Fu Lee, is a global company at the forefront of AI 2.0. They offer cutting-edge large language models, including the Yi series, which range from 6B to hundreds of billions of parameters. 01.AI also provides multimodal models, an open API platform, and open-source options like Yi-34B/9B/6B and Yi-VL.\n\n## Installation and setup\n\nRegister and get an API key from either the China site [here](https://platform.lingyiwanwu.com/apikeys) or the global site [here](https://platform.01.ai/apikeys).\n\n"
  }
,
  {
    "path": "python/integrations/providers/iugu.mdx",
    "filename": "iugu.mdx",
    "size_bytes": 530,
    "line_count": 21,
    "preview": "---\ntitle: Iugu\n---\n\n>[Iugu](https://www.iugu.com/) is a Brazilian services and software as a service (SaaS)\n> company. It offers payment-processing software and application programming\n> interfaces for e-commerce websites and mobile applications.\n\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/couchbase.mdx",
    "filename": "couchbase.mdx",
    "size_bytes": 7000,
    "line_count": 225,
    "preview": "---\ntitle: Couchbase\n---\n\n>[Couchbase](http://couchbase.com/) is an award-winning distributed NoSQL cloud database\n> that delivers unmatched versatility, performance, scalability, and financial value\n> for all of your cloud, mobile, AI, and edge computing applications.\n\nIf you want to see a detailed usage example, see [Couchbase Vector Store](/oss/integrations/vectorstores/couchbase).\n\n"
  }
,
  {
    "path": "python/integrations/providers/flyte.mdx",
    "filename": "flyte.mdx",
    "size_bytes": 6162,
    "line_count": 155,
    "preview": "---\ntitle: Flyte\n---\n\n> [Flyte](https://github.com/flyteorg/flyte) is an open-source orchestrator that facilitates building production-grade data and ML pipelines.\n> It is built for scalability and reproducibility, leveraging Kubernetes as its underlying platform.\n\nThe purpose of this notebook is to demonstrate the integration of a `FlyteCallback` into your Flyte task, enabling you to effectively monitor and track your LangChain experiments.\n\n## Installation & setup\n"
  }
,
  {
    "path": "python/integrations/providers/yahoo.mdx",
    "filename": "yahoo.mdx",
    "size_bytes": 641,
    "line_count": 33,
    "preview": "---\ntitle: Yahoo\n---\n\n>[Yahoo (Wikipedia)](https://en.wikipedia.org/wiki/Yahoo) is an American web services provider.\n>\n> It provides a web portal, search engine Yahoo Search, and related\n> services, including `My Yahoo`, `Yahoo Mail`, `Yahoo News`,\n> `Yahoo Finance`, `Yahoo Sports` and its advertising platform, `Yahoo Native`.\n\n"
  }
,
  {
    "path": "python/integrations/providers/argilla.mdx",
    "filename": "argilla.mdx",
    "size_bytes": 698,
    "line_count": 33,
    "preview": "---\ntitle: Argilla\n---\n\n>[Argilla](https://argilla.io/) is an open-source data curation platform for LLMs.\n> Using `Argilla`, everyone can build robust language models through faster data curation\n> using both human and machine feedback. `Argilla` provides support for each step in the MLOps cycle,\n> from data labeling to model monitoring.\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/anthropic.mdx",
    "filename": "anthropic.mdx",
    "size_bytes": 768,
    "line_count": 24,
    "preview": "---\ntitle: Anthropic (Claude)\n---\n\nThis page covers all LangChain integrations with [Anthropic](https://www.anthropic.com/), the makers of Claude.\n\n## Model interfaces\n\n<Columns cols={2}>\n    <Card title=\"ChatAnthropic\" href=\"/oss/integrations/chat/anthropic\" cta=\"Get started\" icon=\"message\" arrow>\n"
  }
,
  {
    "path": "python/integrations/providers/hazy_research.mdx",
    "filename": "hazy_research.mdx",
    "size_bytes": 625,
    "line_count": 21,
    "preview": "---\ntitle: Hazy Research\n---\n\nThis page covers how to use the Hazy Research ecosystem within LangChain.\nIt is broken into two parts: installation and setup, and then references to specific Hazy Research wrappers.\n\n## Installation and setup\n- To use the `manifest`, install it with `pip install manifest-ml`\n\n"
  }
,
  {
    "path": "python/integrations/providers/ontotext_graphdb.mdx",
    "filename": "ontotext_graphdb.mdx",
    "size_bytes": 659,
    "line_count": 30,
    "preview": "---\ntitle: Ontotext GraphDB\n---\n\n>[Ontotext GraphDB](https://graphdb.ontotext.com/) is a graph database and knowledge discovery tool compliant with RDF and SPARQL.\n\n## Dependencies\n\nInstall the [rdflib](https://github.com/RDFLib/rdflib) package with\n\n"
  }
,
  {
    "path": "python/integrations/providers/pebblo/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 1207,
    "line_count": 24,
    "preview": "---\ntitle: Pebblo\n---\n\n[Pebblo](https://www.daxa.ai/pebblo) enables developers to safely load and retrieve data to promote their Gen AI app to deployment without\nworrying about the organization’s compliance and security requirements. The Pebblo SafeLoader identifies semantic topics and entities found in the\nloaded data and the Pebblo SafeRetriever enforces identity and semantic controls on the retrieved context. The results are\nsummarized on the UI or a PDF report.\n\n## Pebblo overview\n"
  }
,
  {
    "path": "python/integrations/providers/pebblo/pebblo_retrieval_qa.mdx",
    "filename": "pebblo_retrieval_qa.mdx",
    "size_bytes": 13122,
    "line_count": 371,
    "preview": "---\ntitle: Identity-enabled RAG using PebbloRetrievalQA\n---\n\n> PebbloRetrievalQA is a Retrieval chain with Identity & Semantic Enforcement for question-answering\nagainst a vector database.\n\nThis notebook covers how to retrieve documents using Identity & Semantic Enforcement (Deny Topics/Entities).\nFor more details on Pebblo and its SafeRetriever feature visit [Pebblo documentation](https://daxa-ai.github.io/pebblo/retrieval_chain/)\n\n"
  }
,
  {
    "path": "python/integrations/providers/chaindesk.mdx",
    "filename": "chaindesk.mdx",
    "size_bytes": 587,
    "line_count": 19,
    "preview": "---\ntitle: Chaindesk\n---\n\n>[Chaindesk](https://chaindesk.ai) is an [open-source](https://github.com/gmpetrov/databerry) document retrieval platform that helps to connect your personal data with Large Language Models.\n\n\n## Installation and setup\n\nWe need to sign up for Chaindesk, create a datastore, add some data and get your datastore api endpoint url.\n"
  }
,
  {
    "path": "python/integrations/providers/xata.mdx",
    "filename": "xata.mdx",
    "size_bytes": 819,
    "line_count": 35,
    "preview": "---\ntitle: Xata\n---\n\n> [Xata](https://xata.io) is a serverless data platform, based on `PostgreSQL`.\n> It provides a Python SDK for interacting with your database, and a UI\n> for managing your data.\n> `Xata` has a native vector type, which can be added to any table, and\n> supports similarity search. LangChain inserts vectors directly to `Xata`,\n> and queries it for the nearest neighbors of a given vector, so that you can\n"
  }
,
  {
    "path": "python/integrations/providers/datadog.mdx",
    "filename": "datadog.mdx",
    "size_bytes": 3929,
    "line_count": 96,
    "preview": "---\ntitle: Datadog Tracing\n---\n\n>[ddtrace](https://github.com/DataDog/dd-trace-py) is a Datadog application performance monitoring (APM) library which provides an integration to monitor your LangChain application.\n\nKey features of the ddtrace integration for LangChain:\n- Traces: Capture LangChain requests, parameters, prompt-completions, and help visualize LangChain operations.\n- Metrics: Capture LangChain request latency, errors, and token/cost usage (for OpenAI LLMs and chat models).\n- Logs: Store prompt completion data for each LangChain operation.\n"
  }
,
  {
    "path": "python/integrations/providers/outline.mdx",
    "filename": "outline.mdx",
    "size_bytes": 635,
    "line_count": 24,
    "preview": "---\ntitle: Outline\n---\n\n> [Outline](https://www.getoutline.com/) is an open-source collaborative knowledge base platform designed for team information sharing.\n\n## Setup\n\nYou first need to [create an api key](https://www.getoutline.com/developers#section/Authentication) for your Outline instance. Then you need to set the following environment variables:\n\n"
  }
,
  {
    "path": "python/integrations/providers/imsdb.mdx",
    "filename": "imsdb.mdx",
    "size_bytes": 322,
    "line_count": 18,
    "preview": "---\ntitle: IMSDb\n---\n\n>[IMSDb](https://imsdb.com/) is the `Internet Movie Script Database`.\n>\n## Installation and setup\n\nThere isn't any special setup for it.\n\n"
  }
,
  {
    "path": "python/integrations/providers/tidb.mdx",
    "filename": "tidb.mdx",
    "size_bytes": 1062,
    "line_count": 30,
    "preview": "---\ntitle: TiDB\n---\n\n> [TiDB Cloud](https://www.pingcap.com/tidb-serverless), is a comprehensive Database-as-a-Service (DBaaS) solution,\n> that provides dedicated and serverless options. `TiDB Serverless` is now integrating\n> a built-in vector search into the MySQL landscape. With this enhancement, you can seamlessly\n> develop AI applications using `TiDB Serverless` without the need for a new database or additional\n> technical stacks. Create a free TiDB Serverless cluster and start using the vector search feature at https://pingcap.com/ai.\n\n"
  }
,
  {
    "path": "python/integrations/providers/pgvector.mdx",
    "filename": "pgvector.mdx",
    "size_bytes": 1030,
    "line_count": 31,
    "preview": "---\ntitle: PGVector\n---\n\nThis page covers how to use the Postgres [PGVector](https://github.com/pgvector/pgvector) ecosystem within LangChain\nIt is broken into two parts: installation and setup, and then references to specific PGVector wrappers.\n\n## Installation\n- Install the Python package with `pip install pgvector`\n\n"
  }
,
  {
    "path": "python/integrations/providers/deepinfra.mdx",
    "filename": "deepinfra.mdx",
    "size_bytes": 1764,
    "line_count": 55,
    "preview": "---\ntitle: DeepInfra\n---\n\n>[DeepInfra](https://deepinfra.com/docs) allows us to run the\n> [latest machine learning models](https://deepinfra.com/models) with ease.\n> DeepInfra takes care of all the heavy lifting related to running, scaling and monitoring\n> the models. Users can focus on your application and integrate the models with simple REST API calls.\n\n>DeepInfra provides [examples](https://deepinfra.com/docs/advanced/langchain) of integration with LangChain.\n"
  }
,
  {
    "path": "python/integrations/providers/jina.mdx",
    "filename": "jina.mdx",
    "size_bytes": 1018,
    "line_count": 40,
    "preview": "---\ntitle: Jina AI\n---\n\n>[Jina AI](https://jina.ai/about-us) is a search AI company. `Jina` helps businesses and developers unlock multimodal data with a better search.\n\n<Warning>\nFor proper compatibility, please ensure you are using the `openai` SDK at version **0.x**.\n</Warning>\n\n"
  }
,
  {
    "path": "python/integrations/providers/memcached.mdx",
    "filename": "memcached.mdx",
    "size_bytes": 1181,
    "line_count": 42,
    "preview": "---\ntitle: Memcached\n---\n\n> [Memcached](https://www.memcached.org/) is a free & open source, high-performance, distributed memory object caching system,\n> generic in nature, but intended for use in speeding up dynamic web applications by alleviating database load.\n\nThis page covers how to use Memcached with langchain, using [pymemcache](https://github.com/pinterest/pymemcache) as\na client to connect to an already running Memcached instance.\n\n"
  }
,
  {
    "path": "python/integrations/providers/reddit.mdx",
    "filename": "reddit.mdx",
    "size_bytes": 592,
    "line_count": 30,
    "preview": "---\ntitle: Reddit\n---\n\n>[Reddit](https://www.reddit.com) is an American social news aggregation, content rating, and discussion website.\n\n## Installation and setup\n\nFirst, you need to install a python package.\n\n"
  }
,
  {
    "path": "python/integrations/providers/breebs.mdx",
    "filename": "breebs.mdx",
    "size_bytes": 677,
    "line_count": 17,
    "preview": "---\ntitle: Breebs (Open Knowledge)\n---\n\n>[Breebs](https://www.breebs.com/) is an open collaborative knowledge platform.\n>Anybody can create a `Breeb`, a knowledge capsule based on PDFs stored on a Google Drive folder.\n>A `Breeb` can be used by any LLM/chatbot to improve its expertise, reduce hallucinations and give access to sources.\n>Behind the scenes, `Breebs` implements several `Retrieval Augmented Generation (RAG)` models\n> to seamlessly provide useful context at each iteration.\n\n"
  }
,
  {
    "path": "python/integrations/providers/trello.mdx",
    "filename": "trello.mdx",
    "size_bytes": 800,
    "line_count": 30,
    "preview": "---\ntitle: Trello\n---\n\n>[Trello](https://www.atlassian.com/software/trello) is a web-based project management and collaboration tool that allows individuals and teams to organize and track their tasks and projects. It provides a visual interface known as a \"board\" where users can create lists and cards to represent their tasks and activities.\n>The TrelloLoader allows us to load cards from a `Trello` board.\n\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/vlite.mdx",
    "filename": "vlite.mdx",
    "size_bytes": 886,
    "line_count": 45,
    "preview": "---\ntitle: vlite\n---\n\nThis page covers how to use [vlite](https://github.com/sdan/vlite) within LangChain. vlite is a simple and fast vector database for storing and retrieving embeddings.\n\n## Installation and setup\n\nTo install vlite, run the following command:\n\n"
  }
,
  {
    "path": "python/integrations/providers/predibase.mdx",
    "filename": "predibase.mdx",
    "size_bytes": 3949,
    "line_count": 117,
    "preview": "---\ntitle: Predibase\n---\n\nLearn how to use LangChain with models on Predibase.\n\n## Setup\n\n- Create a [Predibase](https://predibase.com/) account and [API key](https://docs.predibase.com/sdk-guide/intro).\n- Install the Predibase Python client with `pip install predibase`\n"
  }
,
  {
    "path": "python/integrations/providers/friendli.mdx",
    "filename": "friendli.mdx",
    "size_bytes": 1130,
    "line_count": 49,
    "preview": "---\ntitle: Friendli AI\n---\n\n> [FriendliAI](https://friendli.ai/) enhances AI application performance and optimizes\n> cost savings with scalable, efficient deployment options, tailored for high-demand AI workloads.\n\n## Installation and setup\n\nInstall the `friendli-client` python package.\n"
  }
,
  {
    "path": "python/integrations/providers/atlas.mdx",
    "filename": "atlas.mdx",
    "size_bytes": 470,
    "line_count": 21,
    "preview": "---\ntitle: Atlas\n---\n\n>[Nomic Atlas](https://docs.nomic.ai/index.html) is a platform for interacting with both\n> small and internet scale unstructured datasets.\n\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/together.mdx",
    "filename": "together.mdx",
    "size_bytes": 1426,
    "line_count": 53,
    "preview": "---\ntitle: Together AI\n---\n\n[Together AI](https://www.together.ai/) offers an API to query [50+ leading open-source models](https://docs.together.ai/docs/inference-models) in a couple lines of code.\n\nThis example goes over how to use LangChain to interact with Together AI models.\n\n## Installation\n\n"
  }
,
  {
    "path": "python/integrations/providers/tavily.mdx",
    "filename": "tavily.mdx",
    "size_bytes": 720,
    "line_count": 25,
    "preview": "---\ntitle: Tavily\n---\n\n[Tavily](https://tavily.com) is a search engine, specifically designed for AI agents.\nTavily provides both a search and extract API, AI developers can effortlessly integrate their\napplications with realtime online information. Tavily’s primary mission is to provide factual\nand reliable information from trusted sources, enhancing the accuracy and reliability of AI\ngenerated content and reasoning.\n\n"
  }
,
  {
    "path": "python/integrations/providers/vespa.mdx",
    "filename": "vespa.mdx",
    "size_bytes": 478,
    "line_count": 29,
    "preview": "---\ntitle: Vespa\n---\n\n>[Vespa](https://vespa.ai/) is a fully featured search engine and vector database.\n> It supports vector search (ANN), lexical search, and search in structured data, all in the same query.\n\n## Installation and setup\n\n\n"
  }
,
  {
    "path": "python/integrations/providers/sklearn.mdx",
    "filename": "sklearn.mdx",
    "size_bytes": 1227,
    "line_count": 36,
    "preview": "---\ntitle: scikit-learn\n---\n\n>[scikit-learn](https://scikit-learn.org/stable/) is an open-source collection of machine learning algorithms,\n> including some implementations of the [k nearest neighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html). `SKLearnVectorStore` wraps this implementation and adds the possibility to persist the vector store in json, bson (binary json) or Apache Parquet format.\n\n## Installation and setup\n\n- Install the Python package with `pip install scikit-learn`\n"
  }
,
  {
    "path": "python/integrations/providers/zilliz.mdx",
    "filename": "zilliz.mdx",
    "size_bytes": 600,
    "line_count": 30,
    "preview": "---\ntitle: Zilliz\n---\n\n>[Zilliz Cloud](https://zilliz.com/doc/quick_start) is a fully managed service on cloud for `LF AI Milvus®`,\n\n\n## Installation and setup\n\nInstall the Python SDK:\n"
  }
,
  {
    "path": "python/integrations/providers/evernote.mdx",
    "filename": "evernote.mdx",
    "size_bytes": 658,
    "line_count": 29,
    "preview": "---\ntitle: EverNote\n---\n\n>[EverNote](https://evernote.com/) is intended for archiving and creating notes in which photos, audio and saved web content can be embedded. Notes are stored in virtual \"notebooks\" and can be tagged, annotated, edited, searched, and exported.\n\n## Installation and setup\n\nFirst, you need to install `lxml` and `html2text` python packages.\n\n"
  }
,
  {
    "path": "python/integrations/providers/vdms.mdx",
    "filename": "vdms.mdx",
    "size_bytes": 1194,
    "line_count": 50,
    "preview": "---\ntitle: VDMS\n---\n\n> [VDMS](https://github.com/IntelLabs/vdms/blob/master/README.md) is a storage solution for efficient access\n> of big-”visual”-data that aims to achieve cloud scale by searching for relevant visual data via visual metadata\n> stored as a graph and enabling machine friendly enhancements to visual data for faster access.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/intel.mdx",
    "filename": "intel.mdx",
    "size_bytes": 6987,
    "line_count": 130,
    "preview": "---\ntitle: Intel\n---\n\n>[Optimum Intel](https://github.com/huggingface/optimum-intel?tab=readme-ov-file#optimum-intel) is the interface between the 🤗 Transformers and Diffusers libraries and the different tools and libraries provided by Intel to accelerate end-to-end pipelines on Intel architectures.\n\n>[Intel® Extension for Transformers](https://github.com/intel/intel-extension-for-transformers?tab=readme-ov-file#intel-extension-for-transformers) (ITREX) is an innovative toolkit designed to accelerate GenAI/LLM everywhere with the optimal performance of Transformer-based models on various Intel platforms, including Intel Gaudi2, Intel CPU, and Intel GPU.\n\nThis page covers how to use optimum-intel and ITREX with LangChain.\n\n"
  }
,
  {
    "path": "python/integrations/providers/langfair.mdx",
    "filename": "langfair.mdx",
    "size_bytes": 5870,
    "line_count": 137,
    "preview": "---\ntitle: LangFair\n---\n\nLangFair is a comprehensive Python library designed for conducting bias and fairness assessments of large language model (LLM) use cases. The LangFair [repository](https://github.com/cvs-health/langfair) includes a comprehensive framework for [choosing bias and fairness metrics](https://github.com/cvs-health/langfair/tree/main#-choosing-bias-and-fairness-metrics-for-an-llm-use-case), along with [demo notebooks](https://github.com/cvs-health/langfair/tree/main/examples) and a [technical playbook](https://arxiv.org/abs/2407.10853) that discusses LLM bias and fairness risks, evaluation metrics, and best practices.\n\nExplore our [documentation site](https://cvs-health.github.io/langfair/) for detailed instructions on using LangFair.\n\n## ⚡ Quickstart guide\n### (Optional) Create a virtual environment for using LangFair\n"
  }
,
  {
    "path": "python/integrations/providers/surrealdb.mdx",
    "filename": "surrealdb.mdx",
    "size_bytes": 1976,
    "line_count": 30,
    "preview": "---\ntitle: SurrealDB\n---\n\n[SurrealDB](https://surrealdb.com) is a unified, multi-model database purpose-built for AI systems. It combines structured and unstructured data (including vector search, graph traversal, relational queries, full-text search, document storage, and time-series data) into a single ACID-compliant engine, scaling from a 3 MB edge binary to petabyte-scale clusters in the cloud. By eliminating the need for multiple specialized stores, SurrealDB simplifies architectures, reduces latency, and ensures consistency for AI workloads.\n\n**Why SurrealDB Matters for GenAI Systems**\n- **One engine for storage and memory:** Combine durable storage and fast, agent-friendly memory in a single system, providing all the data your agent needs and removing the need to sync multiple systems.\n- **One-hop memory for agents:** Run vector search, graph traversal, semantic joins, and transactional writes in a single query, giving LLM agents fast, consistent memory access without stitching relational, graph and vector databases together.\n- **In-place inference and real-time updates:** SurrealDB enables agents to run inference next to data and receive millisecond-fresh updates, critical for real-time reasoning and collaboration.\n"
  }
,
  {
    "path": "python/integrations/providers/discord-shikenso.mdx",
    "filename": "discord-shikenso.mdx",
    "size_bytes": 2155,
    "line_count": 73,
    "preview": "---\ntitle: Discord\n---\n\n> [Discord](https://discord.com/) is an instant messaging, voice, and video communication platform widely used by communities of all types.\n\n## Installation and setup\n\nInstall the `langchain-discord-shikenso` package:\n\n"
  }
,
  {
    "path": "python/integrations/providers/twitter.mdx",
    "filename": "twitter.mdx",
    "size_bytes": 592,
    "line_count": 33,
    "preview": "---\ntitle: Twitter\n---\n\n>[Twitter](https://twitter.com/) is an online social media and social networking service.\n\n\n## Installation and setup\n\n<CodeGroup>\n"
  }
,
  {
    "path": "python/integrations/providers/contextual.mdx",
    "filename": "contextual.mdx",
    "size_bytes": 7306,
    "line_count": 153,
    "preview": "---\ntitle: Contextual AI\n---\n\nContextual AI provides state-of-the-art RAG components designed specifically for accurate and reliable enterprise AI applications. Our LangChain integration exposes standalone API endpoints for our specialized models:\n\n- Grounded Language Model (GLM): The world's most grounded language model, engineered to minimize hallucinations by prioritizing faithfulness to retrieved knowledge. GLM delivers exceptional factual accuracy with inline attributions, making it ideal for enterprise RAG and agentic applications where reliability is critical.\n\n- Instruction-Following Reranker: The first reranker that follows custom instructions to intelligently prioritize documents based on specific criteria like recency, source, or document type. Outperforming competitors on industry benchmarks, our reranker resolves conflicting information challenges in enterprise knowledge bases.\n\n"
  }
,
  {
    "path": "python/integrations/providers/modelscope.mdx",
    "filename": "modelscope.mdx",
    "size_bytes": 1635,
    "line_count": 58,
    "preview": "---\ntitle: ModelScope\n---\n\n>[ModelScope](https://www.modelscope.cn/home) is a big repository of the models and datasets.\n\nThis page covers how to use the modelscope ecosystem within LangChain.\nIt is broken into two parts: installation and setup, and then references to specific modelscope wrappers.\n\n## Installation\n"
  }
,
  {
    "path": "python/integrations/providers/bagel.mdx",
    "filename": "bagel.mdx",
    "size_bytes": 613,
    "line_count": 29,
    "preview": "---\ntitle: Bagel\n---\n\n> [Bagel](https://www.bagel.net/) (`Open Vector Database for AI`), is like GitHub for AI data.\nIt is a collaborative platform where users can create,\nshare, and manage vector datasets. It can support private projects for independent developers,\ninternal collaborations for enterprises, and public contributions for data DAOs.\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/brightdata.mdx",
    "filename": "brightdata.mdx",
    "size_bytes": 1711,
    "line_count": 45,
    "preview": "---\ntitle: Bright Data\n---\n\n[Bright Data](https://brightdata.com) is a web data platform that provides tools for web scraping, SERP collection, and accessing geo-restricted content.\n\nBright Data allows developers to extract structured data from websites, perform search engine queries, and access content that might be otherwise blocked or geo-restricted. The platform is designed to help overcome common web scraping challenges including anti-bot systems, CAPTCHAs, and IP blocks.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/falkordb.mdx",
    "filename": "falkordb.mdx",
    "size_bytes": 1092,
    "line_count": 33,
    "preview": "---\ntitle: FalkorDB\n---\n\n>What is `FalkorDB`?\n\n>- FalkorDB is an `open-source database management system` that specializes in graph database technology.\n>- FalkorDB allows you to represent and store data in nodes and edges, making it ideal for handling connected data and relationships.\n>- FalkorDB Supports OpenCypher query language with proprietary extensions, making it easy to interact with and query your graph data.\n>- With FalkorDB, you can achieve high-performance `graph traversals and queries`, suitable for production-level systems.\n"
  }
,
  {
    "path": "python/integrations/providers/discord.mdx",
    "filename": "discord.mdx",
    "size_bytes": 1325,
    "line_count": 41,
    "preview": "---\ntitle: Discord (community loader)\n---\n\n>[Discord](https://discord.com/) is a VoIP and instant messaging social platform. Users have the ability to communicate\n> with voice calls, video calls, text messaging, media and files in private chats or as part of communities called\n> \"servers\". A server is a collection of persistent chat rooms and voice channels which can be accessed via invite links.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/assemblyai.mdx",
    "filename": "assemblyai.mdx",
    "size_bytes": 1286,
    "line_count": 50,
    "preview": "---\ntitle: AssemblyAI\n---\n\n>[AssemblyAI](https://www.assemblyai.com/) builds `Speech AI` models for tasks like\nspeech-to-text, speaker diarization, speech summarization, and more.\n> `AssemblyAI’s` Speech AI models include accurate speech-to-text for voice data\n> (such as calls, virtual meetings, and podcasts), speaker detection, sentiment analysis,\n> chapter detection, PII redaction.\n\n"
  }
,
  {
    "path": "python/integrations/providers/greennode.mdx",
    "filename": "greennode.mdx",
    "size_bytes": 3546,
    "line_count": 73,
    "preview": "---\ntitle: GreenNode\n---\n\n>**GreenNode** is a global AI solutions provider and a **NVIDIA Preferred Partner**, delivering full-stack AI capabilities—from infrastructure to application—for enterprises across the US, MENA, and APAC regions.\n>Operating on **world-class infrastructure** (LEED Gold, TIA‑942, Uptime Tier III), **GreenNode** empowers enterprises, startups, and researchers with a comprehensive suite of AI services:\n>\n>- [Powerful AI Infrastructure:](https://greennode.ai/) As one of the first hyperscale AI clusters in APAC, powered by NVIDIA H100 GPUs, GreenNode's infrastructure is optimized for high-throughput machine learning and deep learning workloads.\n>- [GreenNode AI Platform:](https://greennode.ai/product/ai-platform) Designed for technical teams, GreenNode’s self-service AI platform enables fast deployment of Jupyter notebook environments, preconfigured with optimized compute instances. From this portal, developers can launch ML training, fine-tuning, hyperparameter optimization, and inference workflows with minimal setup time. The platform includes access to 100+ curated open-source models and supports integrations with common MLOps tools and storage frameworks.\n>- [GreenNode Serverless AI:](https://greennode.ai/product/model-as-a-service) GreenNode Serverless AI features a library of pre-trained production-ready models across domains such as text gen, code gen, text to speech, speech to text, embedding and reranking models. This service is ideal for teams looking to prototype or deploy AI solutions without managing model infrastructure.\n"
  }
,
  {
    "path": "python/integrations/providers/galaxia.mdx",
    "filename": "galaxia.mdx",
    "size_bytes": 1401,
    "line_count": 40,
    "preview": "---\ntitle: Smabbler\n---\n\n> Smabbler’s graph-powered platform boosts AI development by transforming data into a structured knowledge foundation.\n\n## Galaxia\n\n> Galaxia Knowledge Base is an integrated knowledge base and retrieval mechanism for RAG. In contrast to standard solution, it is based on Knowledge Graphs built using symbolic NLP and Knowledge Representation solutions. Provided texts are analysed and transformed into Graphs containing text, language and semantic information. This rich structure allows for retrieval that is based on semantic information, not on vector similarity/distance.\n\n"
  }
,
  {
    "path": "python/integrations/providers/vearch.mdx",
    "filename": "vearch.mdx",
    "size_bytes": 489,
    "line_count": 17,
    "preview": "---\ntitle: Vearch\n---\n\n[Vearch](https://github.com/vearch/vearch) is a scalable distributed system for efficient similarity search of deep learning vectors.\n\n# Installation and setup\n\nVearch Python SDK enables vearch to use locally. Vearch python sdk can be installed easily by pip install vearch.\n\n"
  }
,
  {
    "path": "python/integrations/providers/redis.mdx",
    "filename": "redis.mdx",
    "size_bytes": 4538,
    "line_count": 133,
    "preview": "---\ntitle: Redis\n---\n\n>[Redis (Remote Dictionary Server)](https://en.wikipedia.org/wiki/Redis) is an open-source in-memory storage,\n> used as a distributed, in-memory key–value database, cache and message broker, with optional durability.\n> Because it holds all data in memory and because of its design, `Redis` offers low-latency reads and writes,\n> making it particularly suitable for use cases that require a cache. Redis is the most popular NoSQL database,\n> and one of the most popular databases overall.\n\n"
  }
,
  {
    "path": "python/integrations/providers/chroma.mdx",
    "filename": "chroma.mdx",
    "size_bytes": 694,
    "line_count": 35,
    "preview": "---\ntitle: Chroma\n---\n\n>[Chroma](https://docs.trychroma.com/getting-started) is a database for building AI applications with embeddings.\n\n## Installation and setup\n\n<CodeGroup>\n```bash pip\n"
  }
,
  {
    "path": "python/integrations/providers/mlx.mdx",
    "filename": "mlx.mdx",
    "size_bytes": 772,
    "line_count": 42,
    "preview": "---\ntitle: MLX\n---\n\n>[MLX](https://ml-explore.github.io/mlx/build/html/index.html) is a `NumPy`-like array framework\n> designed for efficient and flexible machine learning on `Apple` silicon,\n> brought to you by `Apple machine learning research`.\n\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/kinetica.mdx",
    "filename": "kinetica.mdx",
    "size_bytes": 1479,
    "line_count": 46,
    "preview": "---\ntitle: Kinetica\n---\n\n[Kinetica](https://www.kinetica.com/) is a real-time database purpose built for enabling\nanalytics and generative AI on time-series & spatial data.\n\n## Chat model\n\nThe Kinetica LLM wrapper uses the [Kinetica SqlAssist\n"
  }
,
  {
    "path": "python/integrations/providers/beautiful_soup.mdx",
    "filename": "beautiful_soup.mdx",
    "size_bytes": 692,
    "line_count": 28,
    "preview": "---\ntitle: Beautiful Soup\n---\n\n>[Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/) is a Python package for parsing\n> HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup).\n> It creates a parse tree for parsed pages that can be used to extract data from HTML,[3] which\n> is useful for web scraping.\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/comet_tracking.mdx",
    "filename": "comet_tracking.mdx",
    "size_bytes": 8062,
    "line_count": 222,
    "preview": "---\ntitle: Comet\n---\n\n>[Comet](https://www.comet.com/) machine learning platform integrates with your existing infrastructure\n>and tools so you can manage, visualize, and optimize models—from training runs to production monitoring\n\n![230328046 a8b18c51 12e3 4617 9b39 97614a571a2d](https://user-images.githubusercontent.com/7529846/230328046-a8b18c51-12e3-4617-9b39-97614a571a2d.png)\n\nIn this guide we will demonstrate how to track your LangChain Experiments, Evaluation Metrics, and LLM Sessions with [Comet](https://www.comet.com/site/?utm_source=langchain&utm_medium=referral&utm_campaign=comet_notebook).\n"
  }
,
  {
    "path": "python/integrations/providers/cohere.mdx",
    "filename": "cohere.mdx",
    "size_bytes": 5050,
    "line_count": 165,
    "preview": "---\ntitle: Cohere\n---\n\n>[Cohere](https://cohere.ai/about) is a Canadian startup that provides natural language processing models\n> that help companies improve human-machine interactions.\n\n## Installation and setup\n- Install the Python SDK :\n<CodeGroup>\n"
  }
,
  {
    "path": "python/integrations/providers/permit.mdx",
    "filename": "permit.mdx",
    "size_bytes": 981,
    "line_count": 40,
    "preview": "---\ntitle: Permit\n---\n\n[Permit.io](https://permit.io/) offers fine-grained access control and policy\nenforcement. With LangChain, you can integrate Permit checks to ensure only authorized\nusers can access or retrieve data in your LLM applications.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/slack.mdx",
    "filename": "slack.mdx",
    "size_bytes": 624,
    "line_count": 34,
    "preview": "---\ntitle: Slack\n---\n\n>[Slack](https://slack.com/) is an instant messaging program.\n\n## Installation and setup\n\nThere isn't any special setup for it.\n\n"
  }
,
  {
    "path": "python/integrations/providers/ollama.mdx",
    "filename": "ollama.mdx",
    "size_bytes": 900,
    "line_count": 28,
    "preview": "---\ntitle: Ollama\n---\n\nThis page covers all LangChain integrations with [Ollama](https://ollama.com/).\n\nOllama allows you to run open-source models (like [`gpt-oss`](https://ollama.com/library/gpt-oss)) locally.\n\nFor a complete list of supported models and variants, see the [Ollama model library](https://ollama.ai/library).\n\n"
  }
,
  {
    "path": "python/integrations/providers/motorhead.mdx",
    "filename": "motorhead.mdx",
    "size_bytes": 360,
    "line_count": 9,
    "preview": "---\ntitle: Motörhead\n---\n\n>[Motörhead](https://github.com/getmetal/motorhead) is a memory server implemented in Rust. It automatically handles incremental summarization in the background and allows for stateless applications.\n\n## Installation and setup\n\nSee instructions at [Motörhead](https://github.com/getmetal/motorhead) for running the server locally.\n"
  }
,
  {
    "path": "python/integrations/providers/oceanbase.mdx",
    "filename": "oceanbase.mdx",
    "size_bytes": 1309,
    "line_count": 38,
    "preview": "---\ntitle: OceanBase\n---\n\n[OceanBase Database](https://github.com/oceanbase/oceanbase) is a distributed relational database.\nIt is developed entirely by Ant Group. The OceanBase Database is built on a common server cluster.\nBased on the Paxos protocol and its distributed structure, the OceanBase Database provides high availability and linear scalability.\n\nOceanBase currently has the ability to store vectors. Users can easily perform the following operations with SQL:\n\n"
  }
,
  {
    "path": "python/integrations/providers/replicate.mdx",
    "filename": "replicate.mdx",
    "size_bytes": 1976,
    "line_count": 49,
    "preview": "---\ntitle: Replicate\n---\n\nThis page covers how to run models on Replicate within LangChain.\n\n## Installation and setup\n- Create a [Replicate](https://replicate.com) account. Get your API key and set it as an environment variable (`REPLICATE_API_TOKEN`)\n- Install the [Replicate python client](https://github.com/replicate/replicate-python) with `pip install replicate`\n\n"
  }
,
  {
    "path": "python/integrations/providers/hacker_news.mdx",
    "filename": "hacker_news.mdx",
    "size_bytes": 614,
    "line_count": 20,
    "preview": "---\ntitle: Hacker News\n---\n\n>[Hacker News](https://en.wikipedia.org/wiki/Hacker_News) (sometimes abbreviated as `HN`) is a social news\n> website focusing on computer science and entrepreneurship. It is run by the investment fund and startup\n> incubator `Y Combinator`. In general, content that can be submitted is defined as \"anything that gratifies\n> one's intellectual curiosity.\"\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/ctranslate2.mdx",
    "filename": "ctranslate2.mdx",
    "size_bytes": 977,
    "line_count": 38,
    "preview": "---\ntitle: CTranslate2\n---\n\n>[CTranslate2](https://opennmt.net/CTranslate2/quickstart.html) is a C++ and Python library\n> for efficient inference with Transformer models.\n>\n>The project implements a custom runtime that applies many performance optimization\n> techniques such as weights quantization, layers fusion, batch reordering, etc.,\n> to accelerate and reduce the memory usage of Transformer models on CPU and GPU.\n"
  }
,
  {
    "path": "python/integrations/providers/baidu.mdx",
    "filename": "baidu.mdx",
    "size_bytes": 1764,
    "line_count": 76,
    "preview": "---\ntitle: Baidu\n---\n\n>[Baidu Cloud](https://cloud.baidu.com/) is a cloud service provided by `Baidu, Inc.`,\n> headquartered in Beijing. It offers a cloud storage service, client software,\n> file management, resource sharing, and Third Party Integration.\n\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/pygmalionai.mdx",
    "filename": "pygmalionai.mdx",
    "size_bytes": 499,
    "line_count": 29,
    "preview": "---\ntitle: PygmalionAI\n---\n\n>[PygmalionAI](https://pygmalion.chat/) is a company supporting the\n> open-source models by serving the inference endpoint\n> for the [Aphrodite Engine](https://github.com/PygmalionAI/aphrodite-engine).\n\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/alibaba_cloud.mdx",
    "filename": "alibaba_cloud.mdx",
    "size_bytes": 2965,
    "line_count": 117,
    "preview": "---\ntitle: Alibaba Cloud\n---\n\n>[Alibaba Group Holding Limited (Wikipedia)](https://en.wikipedia.org/wiki/Alibaba_Group), or `Alibaba`\n> (Chinese: 阿里巴巴), is a Chinese multinational technology company specializing in e-commerce, retail,\n> Internet, and technology.\n>\n> [Alibaba Cloud (Wikipedia)](https://en.wikipedia.org/wiki/Alibaba_Cloud), also known as `Aliyun`\n> (Chinese: 阿里云; pinyin: Ālǐyún; lit. 'Ali Cloud'), is a cloud computing company, a subsidiary\n"
  }
,
  {
    "path": "python/integrations/providers/docusaurus.mdx",
    "filename": "docusaurus.mdx",
    "size_bytes": 469,
    "line_count": 28,
    "preview": "---\ntitle: Docusaurus\n---\n\n>[Docusaurus](https://docusaurus.io/) is a static-site generator which provides\n> out-of-the-box documentation features.\n\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/annoy.mdx",
    "filename": "annoy.mdx",
    "size_bytes": 617,
    "line_count": 29,
    "preview": "---\ntitle: Annoy\n---\n\n> [Annoy](https://github.com/spotify/annoy) (`Approximate Nearest Neighbors Oh Yeah`)\n> is a C++ library with Python bindings to search for points in space that are\n> close to a given query point. It also creates large read-only file-based data\n> structures that are mapped into memory so that many processes may share the same data.\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/asknews.mdx",
    "filename": "asknews.mdx",
    "size_bytes": 902,
    "line_count": 41,
    "preview": "---\ntitle: AskNews\n---\n\n[AskNews](https://asknews.app/) enhances language models with up-to-date global or historical news\nby processing and indexing over 300,000 articles daily, providing prompt-optimized responses\nthrough a low-latency endpoint, and ensuring transparency and diversity in its news coverage.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/abso.mdx",
    "filename": "abso.mdx",
    "size_bytes": 400,
    "line_count": 15,
    "preview": "---\ntitle: Abso\n---\n\n[Abso](https://abso.ai/#router) is an open-source LLM proxy that automatically routes requests between fast and slow models based on prompt complexity. It uses various heuristics to chose the proper model. It's very fast and has low latency.\n\n## Installation and setup\n\n```bash\npip install langchain-abso\n"
  }
,
  {
    "path": "python/integrations/providers/whylabs_profiling.mdx",
    "filename": "whylabs_profiling.mdx",
    "size_bytes": 4018,
    "line_count": 76,
    "preview": "---\ntitle: WhyLabs\n---\n\n>[WhyLabs](https://docs.whylabs.ai/docs/) is an observability platform designed to monitor data pipelines and ML applications for data quality regressions, data drift, and model performance degradation. Built on top of an open-source package called `whylogs`, the platform enables Data Scientists and Engineers to:\n>\n>- Set up in minutes: Begin generating statistical profiles of any dataset using whylogs, the lightweight open-source library.\n>- Upload dataset profiles to the WhyLabs platform for centralized and customizable monitoring/alerting of dataset features as well as model inputs, outputs, and performance.\n>- Integrate seamlessly: interoperable with any data pipeline, ML infrastructure, or framework. Generate real-time insights into your existing data flow. See more about our integrations here.\n>- Scale to terabytes: handle your large-scale data, keeping compute requirements low. Integrate with either batch or streaming data pipelines.\n"
  }
,
  {
    "path": "python/integrations/providers/bibtex.mdx",
    "filename": "bibtex.mdx",
    "size_bytes": 648,
    "line_count": 28,
    "preview": "---\ntitle: BibTeX\n---\n\n>[BibTeX](https://www.ctan.org/pkg/bibtex) is a file format and reference management system commonly used in conjunction with `LaTeX` typesetting. It serves as a way to organize and store bibliographic information for academic and research documents.\n\n## Installation and setup\n\nWe have to install the `bibtexparser` and `pymupdf` packages.\n\n"
  }
,
  {
    "path": "python/integrations/providers/ieit_systems.mdx",
    "filename": "ieit_systems.mdx",
    "size_bytes": 932,
    "line_count": 40,
    "preview": "---\ntitle: IEIT Systems\n---\n\n>[IEIT Systems](https://en.ieisystem.com/) is a Chinese information technology company\n> established in 1999. It provides the IT infrastructure products, solutions,\n> and services, innovative IT products and solutions across cloud computing,\n> big data, and artificial intelligence.\n\n\n"
  }
,
  {
    "path": "python/integrations/providers/all_providers.mdx",
    "filename": "all_providers.mdx",
    "size_bytes": 69388,
    "line_count": 3137,
    "preview": "---\ntitle: \"All integration providers\"\nsidebarTitle: \"All providers\"\nmode: wide\n---\n\nBrowse the complete collection of integrations available for Python. LangChain Python offers the most extensive ecosystem with 1000+ integrations across LLMs, chat models, retrievers, vector stores, document loaders, and more.\n\n## Providers\n\n"
  }
,
  {
    "path": "python/integrations/providers/wandb_tracing.mdx",
    "filename": "wandb_tracing.mdx",
    "size_bytes": 2399,
    "line_count": 83,
    "preview": "---\ntitle: Weights & Biases tracing\n---\n\nThere are two recommended ways to trace your LangChains:\n\n1. Setting the `LANGCHAIN_WANDB_TRACING` environment variable to \"true\".\n1. Using a context manager with tracing_enabled() to trace a particular block of code.\n\n**Note** if the environment variable is set, all code will be traced, regardless of whether or not it's within the context manager.\n"
  }
,
  {
    "path": "python/integrations/providers/dashvector.mdx",
    "filename": "dashvector.mdx",
    "size_bytes": 1509,
    "line_count": 47,
    "preview": "---\ntitle: DashVector\n---\n\n> [DashVector](https://help.aliyun.com/document_detail/2510225.html) is a fully-managed vectorDB service that supports high-dimension dense and sparse vectors, real-time insertion and filtered search. It is built to scale automatically and can adapt to different application requirements.\n\nThis document demonstrates to leverage DashVector within the LangChain ecosystem. In particular, it shows how to install DashVector, and how to use it as a VectorStore plugin in LangChain.\nIt is broken into two parts: installation and setup, and then references to specific DashVector wrappers.\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/cassandra.mdx",
    "filename": "cassandra.mdx",
    "size_bytes": 2509,
    "line_count": 104,
    "preview": "---\ntitle: Cassandra\n---\n\n> [Apache Cassandra®](https://cassandra.apache.org/) is a NoSQL, row-oriented, highly scalable and highly available database.\n> Starting with version 5.0, the database ships with [vector search capabilities](https://cassandra.apache.org/doc/trunk/cassandra/vector-search/overview.html).\n\nThe integrations outlined in this page can be used with `Cassandra` as well as other CQL-compatible databases,\ni.e. those using the `Cassandra Query Language` protocol.\n\n"
  }
,
  {
    "path": "python/integrations/providers/cerebras.mdx",
    "filename": "cerebras.mdx",
    "size_bytes": 1724,
    "line_count": 38,
    "preview": "---\ntitle: Cerebras\n---\n\nAt Cerebras, we've developed the world's largest and fastest AI processor, the Wafer-Scale Engine-3 (WSE-3). The Cerebras CS-3 system, powered by the WSE-3, represents a new class of AI supercomputer that sets the standard for generative AI training and inference with unparalleled performance and scalability.\n\nWith Cerebras as your inference provider, you can:\n- Achieve unprecedented speed for AI inference workloads\n- Build commercially with high throughput\n- Effortlessly scale your AI workloads with our seamless clustering technology\n"
  }
,
  {
    "path": "python/integrations/providers/graph_rag.mdx",
    "filename": "graph_rag.mdx",
    "size_bytes": 644,
    "line_count": 30,
    "preview": "---\ntitle: Graph RAG\n---\n\n## Overview\n\n[Graph RAG](https://datastax.github.io/graph-rag/) provides a retriever interface\nthat combines **unstructured** similarity search on vectors with **structured**\ntraversal of metadata properties. This enables graph-based retrieval over **existing**\nvector stores.\n"
  }
,
  {
    "path": "python/integrations/providers/outlines.mdx",
    "filename": "outlines.mdx",
    "size_bytes": 5646,
    "line_count": 205,
    "preview": "---\ntitle: Outlines\n---\n\n>[Outlines](https://github.com/dottxt-ai/outlines) is a Python library for constrained language generation. It provides a unified interface to various language models and allows for structured generation using techniques like regex matching, type constraints, JSON schemas, and context-free grammars.\n\nOutlines supports multiple backends, including:\n- Hugging Face Transformers\n- llama.cpp\n- vLLM\n"
  }
,
  {
    "path": "python/integrations/providers/open_agent_spec.mdx",
    "filename": "open_agent_spec.mdx",
    "size_bytes": 3102,
    "line_count": 83,
    "preview": "---\ntitle: Open Agent Spec\n---\n\nThis page covers all LangChain integrations with [Open Agent Specification](https://oracle.github.io/agent-spec).\n\nOpen Agent Spec is a framework-agnostic declarative language from Oracle for defining agentic systems. It allows to define agents and workflows in a portable JSON/YAML format that can be executed across different runtimes.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/embedchain.mdx",
    "filename": "embedchain.mdx",
    "size_bytes": 657,
    "line_count": 33,
    "preview": "---\ntitle: Embedchain\n---\n\n> [Embedchain](https://github.com/embedchain/embedchain) is a RAG framework to create\n> data pipelines. It loads, indexes, retrieves and syncs all the data.\n>\n>It is available as an [open source package](https://github.com/embedchain/embedchain)\n> and as a [hosted platform solution](https://app.embedchain.ai/).\n\n"
  }
,
  {
    "path": "python/integrations/providers/fmp-data.mdx",
    "filename": "fmp-data.mdx",
    "size_bytes": 676,
    "line_count": 23,
    "preview": "---\ntitle: FMP Data (Financial Data Prep)\n---\n\n> [FMP-Data](https://pypi.org/project/fmp-data/) is a python package for connecting to\n> Financial Data Prep API. It simplifies how you can access production quality data.\n\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/joplin.mdx",
    "filename": "joplin.mdx",
    "size_bytes": 496,
    "line_count": 21,
    "preview": "---\ntitle: Joplin\n---\n\n>[Joplin](https://joplinapp.org/) is an open-source note-taking app. It captures your thoughts\n> and securely accesses them from any device.\n\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/netmind.mdx",
    "filename": "netmind.mdx",
    "size_bytes": 765,
    "line_count": 25,
    "preview": "---\ntitle: Netmind\n---\n\n[Netmind AI](https://www.netmind.ai/) Build AI Faster, Smarter, and More Affordably.\nTrain, Fine-tune, Run Inference, and Scale with our Global GPU Network—Your all-in-one AI Engine.\n\nThis example goes over how to use LangChain to interact with Netmind AI models.\n\n## Installation and setup\n"
  }
,
  {
    "path": "python/integrations/providers/context.mdx",
    "filename": "context.mdx",
    "size_bytes": 457,
    "line_count": 28,
    "preview": "---\ntitle: Context\n---\n\n>[Context](https://context.ai/) provides user analytics for LLM-powered products and features.\n\n## Installation and setup\n\nWe need to install the  `context-python` Python package:\n\n"
  }
,
  {
    "path": "python/integrations/providers/acreom.mdx",
    "filename": "acreom.mdx",
    "size_bytes": 349,
    "line_count": 17,
    "preview": "---\ntitle: Acreom\n---\n\n[acreom](https://acreom.com) is a dev-first knowledge base with tasks running on local `markdown` files.\n\n## Installation and setup\n\nNo installation is required.\n\n"
  }
,
  {
    "path": "python/integrations/providers/pipeshift.mdx",
    "filename": "pipeshift.mdx",
    "size_bytes": 1300,
    "line_count": 52,
    "preview": "---\ntitle: Pipeshift\n---\n\n> [Pipeshift](https://pipeshift.com) is a fine-tuning and inference platform for open-source LLMs\n\n- You bring your datasets. Fine-tune multiple LLMs. Start inferencing in one-click and watch them scale to millions.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/kdbai.mdx",
    "filename": "kdbai.mdx",
    "size_bytes": 640,
    "line_count": 30,
    "preview": "---\ntitle: KDB.AI\n---\n\n>[KDB.AI](https://kdb.ai) is a powerful knowledge-based vector database and search engine that allows you to build scalable, reliable AI applications, using real-time data, by providing advanced search, recommendation and personalization.\n\n\n## Installation and setup\n\nInstall the Python SDK:\n"
  }
,
  {
    "path": "python/integrations/providers/truefoundry.mdx",
    "filename": "truefoundry.mdx",
    "size_bytes": 4029,
    "line_count": 106,
    "preview": "---\ntitle: TrueFoundry\n---\n\nTrueFoundry provides an enterprise-ready [AI Gateway](https://www.truefoundry.com/ai-gateway) to provide governance and observability to agentic frameworks like LangChain. TrueFoundry AI Gateway serves as a unified interface for LLM access, providing:\n\n- **Unified API Access**: Connect to 250+ LLMs (OpenAI, Claude, Gemini, Groq, Mistral) through one API\n- **Low Latency**: Sub-3ms internal latency with intelligent routing and load balancing\n- **Enterprise Security**: SOC 2, HIPAA, GDPR compliance with RBAC and audit logging\n- **Quota and cost management**: Token-based quotas, rate limiting, and comprehensive usage tracking\n"
  }
,
  {
    "path": "python/integrations/providers/duckdb.mdx",
    "filename": "duckdb.mdx",
    "size_bytes": 376,
    "line_count": 25,
    "preview": "---\ntitle: DuckDB\n---\n\n>[DuckDB](https://duckdb.org/) is an in-process SQL OLAP database management system.\n\n## Installation and setup\n\nFirst, you need to install `duckdb` python package.\n\n"
  }
,
  {
    "path": "python/integrations/providers/petals.mdx",
    "filename": "petals.mdx",
    "size_bytes": 489,
    "line_count": 19,
    "preview": "---\ntitle: Petals\n---\n\nThis page covers how to use the Petals ecosystem within LangChain.\nIt is broken into two parts: installation and setup, and then references to specific Petals wrappers.\n\n## Installation and setup\n- Install with `pip install petals`\n- Get a Hugging Face api key and set it as an environment variable (`HUGGINGFACE_API_KEY`)\n"
  }
,
  {
    "path": "python/integrations/providers/momento.mdx",
    "filename": "momento.mdx",
    "size_bytes": 1949,
    "line_count": 57,
    "preview": "---\ntitle: Momento\n---\n\n> [Momento Cache](https://docs.momentohq.com/) is the world's first truly serverless caching service, offering instant elasticity, scale-to-zero\n> capability, and blazing-fast performance.\n>\n> [Momento Vector Index](https://docs.momentohq.com/vector-index) stands out as the most productive, easiest-to-use, fully serverless vector index.\n>\n> For both services, simply grab the SDK, obtain an API key, input a few lines into your code, and you're set to go. Together, they provide a comprehensive solution for your LLM data needs.\n"
  }
,
  {
    "path": "python/integrations/providers/labelstudio.mdx",
    "filename": "labelstudio.mdx",
    "size_bytes": 881,
    "line_count": 30,
    "preview": "---\ntitle: Label Studio\n---\n\n>[Label Studio](https://labelstud.io/guide/get_started) is an open-source data labeling platform that provides LangChain with flexibility when it comes to labeling data for fine-tuning large language models (LLMs). It also enables the preparation of custom training data and the collection and evaluation of responses through human feedback.\n\n## Installation and setup\n\nSee the [Label Studio installation guide](https://labelstud.io/guide/install) for installation options.\n\n"
  }
,
  {
    "path": "python/integrations/providers/nimble.mdx",
    "filename": "nimble.mdx",
    "size_bytes": 1177,
    "line_count": 29,
    "preview": "---\ntitle: Nimble\n---\n\n[Nimble](https://www.linkedin.com/company/nimbledata) is the first business external data platform, making data decision-making easier than ever, with our award-winning AI-powered data structuring technology Nimble connects business users with the public web knowledge.\n\nWe empower businesses with mission-critical real-time external data to unlock advanced business intelligence, price comparison, and other public data for sales and marketing. We translate data into immediate business value.\n\nIf you'd like to learn more about Nimble, visit us at [nimbleway.com](https://www.nimbleway.com/).\n\n"
  }
,
  {
    "path": "python/integrations/providers/bittensor.mdx",
    "filename": "bittensor.mdx",
    "size_bytes": 423,
    "line_count": 19,
    "preview": "---\ntitle: Bittensor\n---\n\n>[Neural Internet Bittensor](https://neuralinternet.ai/) network, an open source protocol\n> that powers a decentralized, blockchain-based, machine learning network.\n\n## Installation and setup\n\nGet your API_KEY from [Neural Internet](https://neuralinternet.ai/).\n"
  }
,
  {
    "path": "python/integrations/providers/apple.mdx",
    "filename": "apple.mdx",
    "size_bytes": 692,
    "line_count": 24,
    "preview": "---\ntitle: Apple\n---\n\n>[Apple Inc. (Wikipedia)](https://en.wikipedia.org/wiki/Apple_Inc.) is an American\n> multinational corporation and technology company.\n>\n> [iMessage (Wikipedia)](https://en.wikipedia.org/wiki/IMessage) is an instant\n> messaging service developed by Apple Inc. and launched in 2011.\n> `iMessage` functions exclusively on Apple platforms.\n"
  }
,
  {
    "path": "python/integrations/providers/neo4j.mdx",
    "filename": "neo4j.mdx",
    "size_bytes": 2005,
    "line_count": 51,
    "preview": "---\ntitle: Neo4j\n---\n\n>- Neo4j is an `open-source database management system` that specializes in graph database technology.\n>- Neo4j allows you to represent and store data in nodes and edges, making it ideal for handling connected data and relationships.\n>- Neo4j provides a `Cypher Query Language`, making it easy to interact with and query your graph data.\n>- With Neo4j, you can achieve high-performance `graph traversals and queries`, suitable for production-level systems.\n\n>Get started with Neo4j by visiting [their website](https://neo4j.com/).\n"
  }
,
  {
    "path": "python/integrations/providers/polaris_ai_datainsight.mdx",
    "filename": "polaris_ai_datainsight.mdx",
    "size_bytes": 587,
    "line_count": 21,
    "preview": "---\ntitle: PolarisAIDataInsightLoader\n---\n\n> [Polaris AI DataInsight](https://datainsight.polarisoffice.com/playground) is a document parser\n> that extracts document elements (text, images, complex tables, charts, etc.) from various file formats\n> into structured JSON, making them easy to integrate into RAG systems.\n\n## Installation and setup\n\n"
  }
,
  {
    "path": "python/integrations/providers/dedoc.mdx",
    "filename": "dedoc.mdx",
    "size_bytes": 1868,
    "line_count": 64,
    "preview": "---\ntitle: Dedoc\n---\n\n>[Dedoc](https://dedoc.readthedocs.io) is an [open-source](https://github.com/ispras/dedoc)\nlibrary/service that extracts texts, tables, attached files and document structure\n(e.g., titles, list items, etc.) from files of various formats.\n\n`Dedoc` supports `DOCX`, `XLSX`, `PPTX`, `EML`, `HTML`, `PDF`, images and more.\nFull list of supported formats can be found [here](https://dedoc.readthedocs.io/en/latest/#id1).\n"
  }
,
  {
    "path": "python/integrations/providers/byte_dance.mdx",
    "filename": "byte_dance.mdx",
    "size_bytes": 786,
    "line_count": 32,
    "preview": "---\ntitle: ByteDance\n---\n\n>[ByteDance](https://bytedance.com/) is a Chinese internet technology company.\n\n## Installation and setup\n\nGet the access token.\nYou can find the access instructions [here](https://open.larksuite.com/document)\n"
  }
,
  {
    "path": "python/integrations/providers/javelin_ai_gateway.mdx",
    "filename": "javelin_ai_gateway.mdx",
    "size_bytes": 2532,
    "line_count": 98,
    "preview": "---\ntitle: Javelin AI Gateway\n---\n\n[The Javelin AI Gateway](https://www.getjavelin.io) service is a high-performance, enterprise grade API Gateway for AI applications.\nIt is designed to streamline the usage and access of various large language model (LLM) providers,\nsuch as OpenAI, Cohere, Anthropic and custom large language models within an organization by incorporating\nrobust access security for all interactions with LLMs.\n\nJavelin offers a high-level interface that simplifies the interaction with LLMs by providing a unified endpoint\n"
  }
,
  {
    "path": "python/integrations/providers/tensorflow_datasets.mdx",
    "filename": "tensorflow_datasets.mdx",
    "size_bytes": 1056,
    "line_count": 45,
    "preview": "---\ntitle: TensorFlow Datasets\n---\n\n>[TensorFlow Datasets](https://www.tensorflow.org/datasets) is a collection of datasets ready to use,\n> with TensorFlow or other Python ML frameworks, such as Jax. All datasets are exposed\n> as [tf.data.Datasets](https://www.tensorflow.org/api_docs/python/tf/data/Dataset),\n> enabling easy-to-use and high-performance input pipelines. To get started see\n> the [guide](https://www.tensorflow.org/datasets/overview) and\n> the [list of datasets](https://www.tensorflow.org/datasets/catalog/overview#all_datasets).\n"
  }
,
  {
    "path": "python/integrations/providers/clarifai.mdx",
    "filename": "clarifai.mdx",
    "size_bytes": 4520,
    "line_count": 61,
    "preview": "---\ntitle: Clarifai\n---\n\n>[Clarifai](https://clarifai.com) is one of first deep learning platforms having been founded in 2013. Clarifai provides an AI platform with the full AI lifecycle for data exploration, data labeling, model training, evaluation and inference around images, video, text and audio data. In the LangChain ecosystem, as far as we're aware, Clarifai is the only provider that supports LLMs, embeddings and a vector store in one production scale platform, making it an excellent choice to operationalize your LangChain implementations.\n>\n> `Clarifai` provides 1,000s of AI models for many different use cases. You can [explore them here](https://clarifai.com/explore) to find the one most suited for your use case. These models include those created by other providers such as OpenAI, Anthropic, Cohere, AI21, etc. as well as state of the art from open source such as Falcon, InstructorXL, etc. so that you build the best in AI into your products. You'll find these organized by the creator's user_id and into projects we call applications denoted by their app_id. Those IDs will be needed in additional to the model_id and optionally the version_id, so make note of all these IDs once you found the best model for your use case!\n>\n>Also note that given there are many models for images, video, text and audio understanding, you can build some interested AI agents that utilize the variety of AI models as experts to understand those data types.\n\n"
  }
,
  {
    "path": "python/integrations/providers/transwarp.mdx",
    "filename": "transwarp.mdx",
    "size_bytes": 1106,
    "line_count": 42,
    "preview": "---\ntitle: Transwarp\n---\n\n>[Transwarp](https://www.transwarp.cn/en/introduction) aims to build\n> enterprise-level big data and AI infrastructure software,\n> to shape the future of data world. It provides enterprises with\n> infrastructure software and services around the whole data lifecycle,\n> including integration, storage, governance, modeling, analysis,\n> mining and circulation.\n"
  }
,
  {
    "path": "python/integrations/providers/dataherald.mdx",
    "filename": "dataherald.mdx",
    "size_bytes": 2186,
    "line_count": 72,
    "preview": "---\ntitle: Dataherald\n---\n\n>[Dataherald](https://www.dataherald.com) is a natural language-to-SQL.\n\nThis page covers how to use the `Dataherald API` within LangChain.\n\n## Installation and setup\n- Install requirements with\n"
  }
,
  {
    "path": "python/integrations/providers/roam.mdx",
    "filename": "roam.mdx",
    "size_bytes": 375,
    "line_count": 19,
    "preview": "---\ntitle: Roam\n---\n\n>[ROAM](https://roamresearch.com/) is a note-taking tool for networked thought, designed to create a personal knowledge base.\n\n## Installation and setup\n\nThere isn't any special setup for it.\n\n"
  }
,
  {
    "path": "python/integrations/providers/perplexity.mdx",
    "filename": "perplexity.mdx",
    "size_bytes": 760,
    "line_count": 33,
    "preview": "---\ntitle: Perplexity\n---\n\n>[Perplexity](https://www.perplexity.ai/pro) is the most powerful way to search\n> the internet with unlimited Pro Search, upgraded AI models, unlimited file upload,\n> image generation, and API credits.\n>\n> You can check a [list of available models](https://docs.perplexity.ai/docs/model-cards).\n\n"
  }
,
  {
    "path": "python/integrations/stores/file_system.mdx",
    "filename": "file_system.mdx",
    "size_bytes": 2541,
    "line_count": 99,
    "preview": "---\ntitle: LocalFileStore\n---\n\nThis will help you get started with local filesystem [key-value stores](/oss/integrations/stores). For detailed documentation of all LocalFileStore features and configurations head to the [API reference](https://python.langchain.com/api_reference/langchain/storage/langchain.storage.file_system.LocalFileStore.html).\n\n## Overview\n\nThe `LocalFileStore` is a persistent implementation of `ByteStore` that stores everything in a folder of your choosing. It's useful if you're using a single machine and are tolerant of files being added or deleted.\n\n"
  }
,
  {
    "path": "python/integrations/stores/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 2299,
    "line_count": 41,
    "preview": "---\ntitle: \"Key-value stores\"\n---\n\n## Overview\n\nLangChain provides a key-value store interface for storing and retrieving data by key. The key-value store interface in LangChain is primarily used for caching [embeddings](/oss/integrations/text_embedding).\n\n## Interface\n\n"
  }
,
  {
    "path": "python/integrations/stores/in_memory.mdx",
    "filename": "in_memory.mdx",
    "size_bytes": 2330,
    "line_count": 85,
    "preview": "---\ntitle: InMemoryByteStore\n---\n\nThis guide will help you get started with in-memory [key-value stores](/oss/integrations/stores). For detailed documentation of all `InMemoryByteStore` features and configurations head to the [API reference](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.InMemoryByteStore.html).\n\n## Overview\n\nThe `InMemoryByteStore` is a non-persistent implementation of a `ByteStore` that stores everything in a Python dictionary. It's intended for demos and cases where you don't need persistence past the lifetime of the Python process.\n\n"
  }
,
  {
    "path": "python/integrations/stores/elasticsearch.mdx",
    "filename": "elasticsearch.mdx",
    "size_bytes": 4203,
    "line_count": 132,
    "preview": "---\ntitle: ElasticsearchEmbeddingsCache\n---\n\nThis will help you get started with Elasticsearch [key-value stores](/oss/integrations/stores). For detailed documentation of all `ElasticsearchEmbeddingsCache` features and configurations head to the [API reference](https://python.langchain.com/api_reference/elasticsearch/cache/langchain_elasticsearch.cache.ElasticsearchEmbeddingsCache.html).\n\n## Overview\n\nThe `ElasticsearchEmbeddingsCache` is a `ByteStore` implementation that uses your Elasticsearch instance for efficient storage and retrieval of embeddings.\n\n"
  }
,
  {
    "path": "python/integrations/stores/astradb.mdx",
    "filename": "astradb.mdx",
    "size_bytes": 2839,
    "line_count": 108,
    "preview": "---\ntitle: AstraDBByteStore\n---\n\nThis will help you get started with Astra DB [key-value stores](/oss/integrations/stores). For detailed documentation of all `AstraDBByteStore` features and configurations head to the [API reference](https://python.langchain.com/api_reference/astradb/storage/langchain_astradb.storage.AstraDBByteStore.html).\n\n## Overview\n\n> [DataStax Astra DB](https://docs.datastax.com/en/astra-db-serverless/index.html) is a serverless\n> AI-ready database built on `Apache Cassandra®` and made conveniently available\n"
  }
,
  {
    "path": "python/integrations/stores/upstash_redis.mdx",
    "filename": "upstash_redis.mdx",
    "size_bytes": 3154,
    "line_count": 104,
    "preview": "---\ntitle: UpstashRedisByteStore\n---\n\nThis will help you get started with Upstash redis [key-value stores](/oss/integrations/stores). For detailed documentation of all `UpstashRedisByteStore` features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/storage/langchain_community.storage.upstash_redis.UpstashRedisByteStore.html).\n\n## Overview\n\nThe `UpstashRedisStore` is an implementation of `ByteStore` that stores everything in your [Upstash](https://upstash.com/)-hosted Redis instance.\n\n"
  }
,
  {
    "path": "python/integrations/stores/redis.mdx",
    "filename": "redis.mdx",
    "size_bytes": 2473,
    "line_count": 89,
    "preview": "---\ntitle: RedisStore\n---\n\nThis will help you get started with Redis [key-value stores](/oss/integrations/stores). For detailed documentation of all `RedisStore` features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/storage/langchain_community.storage.redis.RedisStore.html).\n\n## Overview\n\nThe `RedisStore` is an implementation of `ByteStore` that stores everything in your Redis instance.\n\n"
  }
,
  {
    "path": "python/integrations/stores/cassandra.mdx",
    "filename": "cassandra.mdx",
    "size_bytes": 4048,
    "line_count": 117,
    "preview": "---\ntitle: CassandraByteStore\n---\n\nThis will help you get started with Cassandra [key-value stores](/oss/integrations/stores). For detailed documentation of all `CassandraByteStore` features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/storage/langchain_community.storage.cassandra.CassandraByteStore.html).\n\n## Overview\n\n[Cassandra](https://cassandra.apache.org/) is a NoSQL, row-oriented, highly scalable and highly available database.\n\n"
  }
,
  {
    "path": "python/integrations/splitters/recursive_json_splitter.mdx",
    "filename": "recursive_json_splitter.mdx",
    "size_bytes": 5595,
    "line_count": 135,
    "preview": "---\ntitle: Split JSON data\n---\n\nThis json splitter [splits](/oss/integrations/splitters/) json data while allowing control over chunk sizes. It traverses json data depth first and builds smaller json chunks. It attempts to keep nested json objects whole but will split them if needed to keep chunks between a min_chunk_size and the max_chunk_size.\n\nIf the value is not a nested json, but rather a very large string the string will not be split. If you need a hard cap on the chunk size consider composing this with a Recursive Text splitter on those chunks. There is an optional pre-processing step to split lists, by first converting them to json (dict) and then splitting them as such.\n\n1. How the text is split: json value.\n2. How the chunk size is measured: by number of characters.\n"
  }
,
  {
    "path": "python/integrations/splitters/split_html.mdx",
    "filename": "split_html.mdx",
    "size_bytes": 34180,
    "line_count": 656,
    "preview": "---\ntitle: Split HTML\n---\n\nSplitting HTML documents into manageable chunks is essential for various text processing tasks such as natural language processing, search indexing, and more. In this guide, we will explore three different text splitters provided by LangChain that you can use to split HTML content effectively:\n\n- [**HTMLHeaderTextSplitter**](#using-htmlheadertextsplitter)\n- [**HTMLSectionSplitter**](#using-htmlsectionsplitter)\n- [**HTMLSemanticPreservingSplitter**](#using-htmlsemanticpreservingsplitter)\n\n"
  }
,
  {
    "path": "python/integrations/splitters/markdown_header_metadata_splitter.mdx",
    "filename": "markdown_header_metadata_splitter.mdx",
    "size_bytes": 8446,
    "line_count": 158,
    "preview": "---\ntitle: Split markdown\n---\n\nMany chat or Q&A applications involve chunking input documents prior to embedding and vector storage.\n\n[These notes](https://www.pinecone.io/learn/chunking-strategies/) from Pinecone provide some useful tips:\n\n```wrap\nWhen a full paragraph or document is embedded, the embedding process considers both the overall context and the relationships between the sentences and phrases within the text. This can result in a more comprehensive vector representation that captures the broader meaning and themes of the text.\n"
  }
,
  {
    "path": "python/integrations/vectorstores/myscale.mdx",
    "filename": "myscale.mdx",
    "size_bytes": 6276,
    "line_count": 187,
    "preview": "---\ntitle: MyScale\n---\n\n>[MyScale](https://docs.myscale.com/en/overview/) is a cloud-based database optimized for AI applications and solutions, built on the open-source [ClickHouse](https://github.com/ClickHouse/ClickHouse).\n\nThis notebook shows how to use functionality related to the `MyScale` vector database.\n\n## Setting up environments\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/opengauss.mdx",
    "filename": "opengauss.mdx",
    "size_bytes": 6508,
    "line_count": 195,
    "preview": "---\ntitle: openGauss VectorStore\n---\n\nThis notebook covers how to get started with the openGauss VectorStore. [openGauss](https://opengauss.org/en/) is a high-performance relational database with native vector storage and retrieval capabilities. This integration enables ACID-compliant vector operations within LangChain applications, combining traditional SQL functionality with modern AI-driven similarity search.\n vector store.\n\n## Setup\n\n### Launch openGauss Container\n"
  }
,
  {
    "path": "python/integrations/vectorstores/memorydb.mdx",
    "filename": "memorydb.mdx",
    "size_bytes": 10071,
    "line_count": 289,
    "preview": "---\ntitle: Amazon MemoryDB\n---\n\n>[Vector Search](https://docs.aws.amazon.com/memorydb/latest/devguide/vector-search.html/) introduction and langchain integration guide.\n\n## What is Amazon MemoryDB?\n\nMemoryDB is compatible with Redis OSS, a popular open source data store, enabling you to quickly build applications using the same flexible and friendly Redis OSS data structures, APIs, and commands that they already use today. With MemoryDB, all of your data is stored in memory, which enables you to achieve microsecond read and single-digit millisecond write latency and high throughput. MemoryDB also stores data durably across multiple Availability Zones (AZs) using a Multi-AZ transactional log to enable fast failover, database recovery, and node restarts.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/tair.mdx",
    "filename": "tair.mdx",
    "size_bytes": 2340,
    "line_count": 77,
    "preview": "---\ntitle: Tair\n---\n\n>[Tair](https://www.alibabacloud.com/help/en/tair/latest/what-is-tair) is a cloud native in-memory database service developed by `Alibaba Cloud`.\nIt provides rich data models and enterprise-grade capabilities to support your real-time online scenarios while maintaining full compatibility with open-source `Redis`. `Tair` also introduces persistent memory-optimized instances that are based on the new non-volatile memory (NVM) storage medium.\n\nThis notebook shows how to use functionality related to the `Tair` vector database.\n\nYou'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration\n"
  }
,
  {
    "path": "python/integrations/vectorstores/lindorm.mdx",
    "filename": "lindorm.mdx",
    "size_bytes": 4028,
    "line_count": 146,
    "preview": "---\ntitle: LindormVectorStore\n---\n\nThis notebook covers how to get started with the Lindorm vector store.\n\n## Setup\n\nTo access Lindorm vector stores you'll need to create a Lindorm account, get the ak/sk, and install the `langchain-lindorm-integration` integration package.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/sap_hanavector.mdx",
    "filename": "sap_hanavector.mdx",
    "size_bytes": 30851,
    "line_count": 891,
    "preview": "---\ntitle: SAP HANA Cloud Vector Engine\n---\n\n>[SAP HANA Cloud Vector Engine](https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-vector-engine-guide/sap-hana-cloud-sap-hana-database-vector-engine-guide) is a vector store fully integrated into the `SAP HANA Cloud` database.\n\n## Setup\n\nInstall the `langchain-hana` external integration package, as well as the other packages used throughout this notebook.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/teradata.mdx",
    "filename": "teradata.mdx",
    "size_bytes": 15793,
    "line_count": 384,
    "preview": "---\ntitle: TeradataVectorStore\n---\n\n>Teradata Vector Store is designed to store, index, and search high-dimensional vector embeddings efficiently within your enterprise data platform.\n\nThis guide shows you how to quickly get up and running with TeradataVectorStore for your semantic search and RAG applications. Whether you're new to Teradata or looking to add AI capabilities to your existing data workflows, this guide will walk you through everything you need to know.\n\n**What makes TeradataVectorStore special?**\n- Built on enterprise-grade Teradata Vantage platform. \n"
  }
,
  {
    "path": "python/integrations/vectorstores/lantern.mdx",
    "filename": "lantern.mdx",
    "size_bytes": 15986,
    "line_count": 364,
    "preview": "---\ntitle: Lantern\n---\n\n>[Lantern](https://github.com/lanterndata/lantern) is an open-source vector similarity search for `Postgres`\n\nIt supports:\n\n- Exact and approximate nearest neighbor search\n- L2 squared distance, hamming distance, and cosine distance\n"
  }
,
  {
    "path": "python/integrations/vectorstores/google_memorystore_redis.mdx",
    "filename": "google_memorystore_redis.mdx",
    "size_bytes": 8993,
    "line_count": 224,
    "preview": "---\ntitle: Google Memorystore for Redis\n---\n\n> [Google Memorystore for Redis](https://cloud.google.com/memorystore/docs/redis/memorystore-for-redis-overview) is a fully-managed service that is powered by the Redis in-memory data store to build application caches that provide sub-millisecond data access. Extend your database application to build AI-powered experiences leveraging Memorystore for Redis's LangChain integrations.\n\nThis notebook goes over how to use [Memorystore for Redis](https://cloud.google.com/memorystore/docs/redis/memorystore-for-redis-overview) to store vector embeddings with the `MemorystoreVectorStore` class.\n\nLearn more about the package on [GitHub](https://github.com/googleapis/langchain-google-memorystore-redis-python/).\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/activeloop_deeplake.mdx",
    "filename": "activeloop_deeplake.mdx",
    "size_bytes": 7804,
    "line_count": 266,
    "preview": "---\ntitle: Activeloop Deep Lake\n---\n\n>[Activeloop Deep Lake](https://docs.deeplake.ai/) as a Multi-Modal Vector Store that stores embeddings and their metadata including text, jsons, images, audio, video, and more. It saves the data locally, in your cloud, or on Activeloop storage. It performs hybrid search including embeddings and their attributes.\n\nThis notebook showcases basic functionality related to `Activeloop Deep Lake`. While `Deep Lake` can store embeddings, it is capable of storing any type of data. It is a serverless data lake with version control, query engine and streaming dataloaders to deep learning frameworks.\n\nFor more information, please see the Deep Lake [documentation](https://docs.deeplake.ai/)\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/ydb.mdx",
    "filename": "ydb.mdx",
    "size_bytes": 8023,
    "line_count": 268,
    "preview": "---\ntitle: YDB\n---\n\n> [YDB](https://ydb.tech/) is a versatile open source Distributed SQL Database that combines high availability and scalability with strong consistency and ACID transactions. It accommodates transactional (OLTP), analytical (OLAP), and streaming workloads simultaneously.\n\nThis notebook shows how to use functionality related to the `YDB` vector store.\n\n## Setup\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/pgembedding.mdx",
    "filename": "pgembedding.mdx",
    "size_bytes": 5606,
    "line_count": 173,
    "preview": "---\ntitle: Postgres Embedding\n---\n\n> [Postgres Embedding](https://github.com/neondatabase/pg_embedding) is an open-source vector similarity search for `Postgres` that uses  `Hierarchical Navigable Small Worlds (HNSW)` for approximate nearest neighbor search.\n\n>It supports:\n>\n>- exact and approximate nearest neighbor search using HNSW\n>- L2 distance\n"
  }
,
  {
    "path": "python/integrations/vectorstores/hippo.mdx",
    "filename": "hippo.mdx",
    "size_bytes": 6328,
    "line_count": 161,
    "preview": "---\ntitle: Hippo\n---\n\n>[Transwarp Hippo](https://www.transwarp.cn/en/subproduct/hippo) is an enterprise-level cloud-native distributed vector database that supports storage, retrieval, and management of massive vector-based datasets. It efficiently solves problems such as vector similarity search and high-density vector clustering. `Hippo` features high availability, high performance, and easy scalability. It has many functions, such as multiple vector search indexes, data partitioning and sharding, data persistence, incremental data ingestion, vector scalar field filtering, and mixed queries. It can effectively meet the high real-time search demands of enterprises for massive vector data\n\n## Getting started\n\nThe only prerequisite here is an API key from the OpenAI website. Make sure you have already started a Hippo instance.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/yellowbrick.mdx",
    "filename": "yellowbrick.mdx",
    "size_bytes": 13639,
    "line_count": 378,
    "preview": "---\ntitle: Yellowbrick\n---\n\n[Yellowbrick](https://yellowbrick.com/yellowbrick-data-warehouse/) is an elastic, massively parallel processing (MPP) SQL database that runs in the cloud and on-premises, using kubernetes for scale, resilience and cloud portability. Yellowbrick is designed to address the largest and most complex business-critical data warehousing use cases. The efficiency at scale that Yellowbrick provides also enables it to be used as a high performance and scalable vector database to store and search vectors with SQL.\n\n## Using yellowbrick as the vector store for ChatGpt\n\nThis tutorial demonstrates how to create a simple chatbot backed by ChatGpt that uses Yellowbrick as a vector store to support Retrieval Augmented Generation (RAG). What you'll need:\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/mongodb_atlas.mdx",
    "filename": "mongodb_atlas.mdx",
    "size_bytes": 10625,
    "line_count": 318,
    "preview": "---\ntitle: MongoDB Atlas\n---\n\nThis notebook covers how to MongoDB Atlas vector search in LangChain, using the `langchain-mongodb` package.\n\n>[MongoDB Atlas](https://www.mongodb.com/docs/atlas/) is a fully-managed cloud database available in AWS, Azure, and GCP.  It supports native Vector Search, full text search (BM25), and hybrid search on your MongoDB document data.\n\n>[MongoDB Atlas Vector Search](https://www.mongodb.com/products/platform/atlas-vector-search) allows to store your embeddings in MongoDB documents, create a vector search index, and perform KNN search with an approximate nearest neighbor algorithm (`Hierarchical Navigable Small Worlds`). It uses the [$vectorSearch MQL Stage](https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-overview/).\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/supabase.mdx",
    "filename": "supabase.mdx",
    "size_bytes": 10848,
    "line_count": 285,
    "preview": "---\ntitle: Supabase (Postgres)\n---\n\n>[Supabase](https://supabase.com/docs) is an open-source Firebase alternative. `Supabase` is built on top of `PostgreSQL`, which offers strong SQL querying capabilities and enables a simple interface with already-existing tools and frameworks.\n\n>[PostgreSQL](https://en.wikipedia.org/wiki/PostgreSQL) also known as `Postgres`, is a free and open-source relational database management system (RDBMS) emphasizing extensibility and SQL compliance.\n\nThis notebook shows how to use `Supabase` and `pgvector` as your VectorStore.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/azure_db_for_postgresql.mdx",
    "filename": "azure_db_for_postgresql.mdx",
    "size_bytes": 17003,
    "line_count": 451,
    "preview": "---\ntitle: Azure Database for PostgreSQL - Flexible Server\n---\n\n[Azure Database for PostgreSQL - Flexible Server](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/service-overview) is a relational database service based on the open-source Postgres database engine. It's a fully managed database-as-a-service that can handle mission-critical workloads with predictable performance, security, high availability, and dynamic scalability.\n\nThis guide shows you how to leverage this integrated [vector database](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/how-to-use-pgvector) to store documents in collections, create indices and perform vector search queries using approximate nearest neighbor algorithms such as Cosine Distance, L2 (Euclidean distance), and IP (inner product) to locate documents close to the query vectors.\n\n## Vector support\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/google_bigquery_vector_search.mdx",
    "filename": "google_bigquery_vector_search.mdx",
    "size_bytes": 208592,
    "line_count": 190,
    "preview": "---\ntitle: Google BigQuery Vector Search\n---\n\n> [Google Cloud BigQuery Vector Search](https://cloud.google.com/bigquery/docs/vector-search-intro) lets you use GoogleSQL to do semantic search, using vector indexes for fast approximate results, or using brute force for exact results.\n\nThis tutorial illustrates how to work with an end-to-end data and embedding management system in LangChain, and provides a scalable semantic search in BigQuery using the`BigQueryVectorStore` class. This class is part of a set of 2 classes capable of providing a unified data storage and flexible vector search in Google Cloud:\n\n- **BigQuery Vector Search**: with `BigQueryVectorStore` class, which is ideal for rapid prototyping with no infrastructure setup and batch retrieval.\n- **Feature Store Online Store**: with `VertexFSVectorStore` class, enables low-latency retrieval with manual or scheduled data sync. Perfect for production-ready user-facing GenAI applications.\n"
  }
,
  {
    "path": "python/integrations/vectorstores/moorcheh.mdx",
    "filename": "moorcheh.mdx",
    "size_bytes": 5263,
    "line_count": 169,
    "preview": "---\ntitle: \"Moorcheh\"\ndescription: \"Lightning-fast semantic search engine and vector store using Maximally Informative Binarization (MIB) and Information-Theoretic Score (ITS)\"\n---\n\n# Moorcheh\n\n[Moorcheh](https://www.moorcheh.ai/) is a lightning-fast semantic search engine and vector store. Instead of using simple distance metrics like L2 or Cosine, Moorcheh uses Maximally Informative Binarization (MIB) and Information-Theoretic Score (ITS) to retrieve accurate document chunks.\n\nThe following tutorial will allow you to use Moorcheh and LangChain to upload and store text documents and vector embeddings as well as retrieve relevant chunks for all of your queries.\n"
  }
,
  {
    "path": "python/integrations/vectorstores/documentdb.mdx",
    "filename": "documentdb.mdx",
    "size_bytes": 8862,
    "line_count": 231,
    "preview": "---\ntitle: Amazon Document DB\n---\n\n>[Amazon DocumentDB (with MongoDB Compatibility)](https://docs.aws.amazon.com/documentdb/) makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud.\n> With Amazon DocumentDB, you can run the same application code and use the same drivers and tools that you use with MongoDB.\n> Vector search for Amazon DocumentDB combines the flexibility and rich querying capability of a JSON-based document database with the power of vector search.\n\nThis notebook shows you how to use [Amazon Document DB Vector Search](https://docs.aws.amazon.com/documentdb/latest/developerguide/vector-search.html) to store documents in collections, create indicies and perform vector search queries using approximate nearest neighbor algorithms such \"cosine\", \"euclidean\", and \"dotProduct\". By default, DocumentDB creates Hierarchical Navigable Small World (HNSW) indexes. To learn about other supported vector index types, please refer to the document linked above.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/neo4jvector.mdx",
    "filename": "neo4jvector.mdx",
    "size_bytes": 14022,
    "line_count": 455,
    "preview": "---\ntitle: Neo4j Vector Index\n---\n\n>[Neo4j](https://neo4j.com/) is an open-source graph database with integrated support for vector similarity search\n\nIt supports:\n\n- approximate nearest neighbor search\n- Euclidean similarity and cosine similarity\n"
  }
,
  {
    "path": "python/integrations/vectorstores/oracle.mdx",
    "filename": "oracle.mdx",
    "size_bytes": 21676,
    "line_count": 463,
    "preview": "---\ntitle: Oracle AI Vector Search - Vector Store\n---\n\nOracle AI Vector Search is designed for Artificial Intelligence (AI) workloads that allows you to query data based on semantics, rather than keywords.\nOne of the biggest benefits of Oracle AI Vector Search is that semantic search on unstructured data can be combined with relational search on business data in one single system.\nThis is not only powerful but also significantly more effective because you don't need to add a specialized vector database, eliminating the pain of data fragmentation between multiple systems.\n\nIn addition, your vectors can benefit from all of Oracle Database’s most powerful features, like the following:\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/google_alloydb.mdx",
    "filename": "google_alloydb.mdx",
    "size_bytes": 9227,
    "line_count": 266,
    "preview": "---\ntitle: Google AlloyDB for PostgreSQL\n---\n\n> [AlloyDB](https://cloud.google.com/alloydb) is a fully managed relational database service that offers high performance, seamless integration, and impressive scalability. AlloyDB is 100% compatible with PostgreSQL. Extend your database application to build AI-powered experiences leveraging AlloyDB's LangChain integrations.\n\nThis notebook goes over how to use `AlloyDB for PostgreSQL` to store vector embeddings with the `AlloyDBVectorStore` class.\n\nLearn more about the package on [GitHub](https://github.com/googleapis/langchain-google-alloydb-pg-python/).\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/pgvecto_rs.mdx",
    "filename": "pgvecto_rs.mdx",
    "size_bytes": 3342,
    "line_count": 124,
    "preview": "---\ntitle: PGVecto.rs\n---\n\nThis notebook shows how to use functionality related to the Postgres vector database ([pgvecto.rs](https://github.com/tensorchord/pgvecto.rs)).\n\n```python\npip install \"pgvecto_rs[sdk]\" langchain-community\n```\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/falkordbvector.mdx",
    "filename": "falkordbvector.mdx",
    "size_bytes": 4516,
    "line_count": 161,
    "preview": "---\ntitle: FalkorDBVectorStore\n---\n\n<a href=\"https://docs.falkordb.com/\" target=\"_blank\">FalkorDB</a> is an open-source graph database with integrated support for vector similarity search\n\nit supports:\n\n- approximate nearest neighbor search\n- Euclidean similarity & Cosine Similarity\n"
  }
,
  {
    "path": "python/integrations/vectorstores/google_cloud_sql_pg.mdx",
    "filename": "google_cloud_sql_pg.mdx",
    "size_bytes": 9716,
    "line_count": 267,
    "preview": "---\ntitle: Google Cloud SQL for PostgreSQL\n---\n\n> [Cloud SQL](https://cloud.google.com/sql) is a fully managed relational database service that offers high performance, seamless integration, and impressive scalability. It offers PostgreSQL, PostgreSQL, and SQL Server database engines. Extend your database application to build AI-powered experiences leveraging Cloud SQL's LangChain integrations.\n\nThis notebook goes over how to use `Cloud SQL for PostgreSQL` to store vector embeddings with the `PostgresVectorStore` class.\n\nLearn more about the package on [GitHub](https://github.com/googleapis/langchain-google-cloud-sql-pg-python/).\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/llm_rails.mdx",
    "filename": "llm_rails.mdx",
    "size_bytes": 15526,
    "line_count": 186,
    "preview": "---\ntitle: LLMRails\n---\n\n>[LLMRails](https://www.llmrails.com/) is a API platform for building GenAI applications. It provides an easy-to-use API for document indexing and querying that is managed by LLMRails and is optimized for performance and accuracy.\nSee the [LLMRails API documentation](https://docs.llmrails.com/) for more information on how to use the API.\n\nYou'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration\n\nThis notebook shows how to use functionality related to the `LLMRails`'s integration with langchain.\n"
  }
,
  {
    "path": "python/integrations/vectorstores/zeusdb.mdx",
    "filename": "zeusdb.mdx",
    "size_bytes": 5589,
    "line_count": 225,
    "preview": "---\ntitle: ZeusDB\n---\n\n>[ZeusDB](https://www.zeusdb.com) is a high-performance vector database powered by Rust, offering advanced features like product quantization, persistent storage, and enterprise-grade logging.\n\nThis documentation shows how to use ZeusDB to bring enterprise-grade vector search capabilities to your LangChain applications.\n\n---\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/db2.mdx",
    "filename": "db2.mdx",
    "size_bytes": 8574,
    "line_count": 224,
    "preview": "---\ntitle: IBM Db2 Vector Store and Vector Search\n---\n\nLangChain's Db2 integration (langchain-db2) provides vector store and vector search capabilities for working with IBM relational database Db2 version v12.1.2 and above, distributed under the MIT license. Users can use the provided implementations as-is or customize them for specific needs.\n Key features include:\n\n* Vector storage with metadata\n* Vector similarity search and max marginal relevance search, with metadata filtering options\n* Support for dot production, cosine, and euclidean distance metrics\n"
  }
,
  {
    "path": "python/integrations/vectorstores/vectara.mdx",
    "filename": "vectara.mdx",
    "size_bytes": 23598,
    "line_count": 251,
    "preview": "---\ntitle: Vectara\n---\n\n[Vectara](https://vectara.com/) is the trusted AI Assistant and Agent platform which focuses on enterprise readiness for mission-critical applications.\nVectara serverless RAG-as-a-service provides all the components of RAG behind an easy-to-use API, including:\n\n1. A way to extract text from files (PDF, PPT, DOCX, etc)\n2. ML-based chunking that provides state of the art performance.\n3. The [Boomerang](https://vectara.com/how-boomerang-takes-retrieval-augmented-generation-to-the-next-level-via-grounded-generation/) embeddings model.\n"
  }
,
  {
    "path": "python/integrations/vectorstores/zep_cloud.mdx",
    "filename": "zep_cloud.mdx",
    "size_bytes": 62486,
    "line_count": 290,
    "preview": "---\ntitle: Zep Cloud\n---\n\n> Recall, understand, and extract data from chat histories. Power personalized AI experiences.\n\n> [Zep](https://www.getzep.com) is a long-term memory service for AI Assistant apps.\n> With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant,\n> while also reducing hallucinations, latency, and cost.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/typesense.mdx",
    "filename": "typesense.mdx",
    "size_bytes": 2488,
    "line_count": 84,
    "preview": "---\ntitle: Typesense\n---\n\n> [Typesense](https://typesense.org) is an open-source, in-memory search engine, that you can either [self-host](https://typesense.org/docs/guide/install-typesense#option-2-local-machine-self-hosting) or run on [Typesense Cloud](https://cloud.typesense.org/).\n>\n> Typesense focuses on performance by storing the entire index in RAM (with a backup on disk) and also focuses on providing an out-of-the-box developer experience by simplifying available options and setting good defaults.\n>\n> It also lets you combine attribute-based filtering together with vector queries, to fetch the most relevant documents.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/hologres.mdx",
    "filename": "hologres.mdx",
    "size_bytes": 3764,
    "line_count": 87,
    "preview": "---\ntitle: Hologres\n---\n\n>[Hologres](https://www.alibabacloud.com/help/en/hologres/latest/introduction) is a unified real-time data warehousing service developed by Alibaba Cloud. You can use Hologres to write, update, process, and analyze large amounts of data in real time.\n>Hologres supports standard SQL syntax, is compatible with PostgreSQL, and supports most PostgreSQL functions. Hologres supports online analytical processing (OLAP) and ad hoc analysis for up to petabytes of data, and provides high-concurrency and low-latency online data services.\n\n>Hologres provides **vector database** functionality by adopting [Proxima](https://www.alibabacloud.com/help/en/hologres/latest/vector-processing).\n>Proxima is a high-performance software library developed by Alibaba DAMO Academy. It allows you to search for the nearest neighbors of vectors. Proxima provides higher stability and performance than similar open-source software such as Faiss. Proxima allows you to search for similar text or image embeddings with high throughput and low latency. Hologres is deeply integrated with Proxima to provide a high-performance vector search service.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/awadb.mdx",
    "filename": "awadb.mdx",
    "size_bytes": 2121,
    "line_count": 75,
    "preview": "---\ntitle: AwaDB\n---\n\n>[AwaDB](https://github.com/awa-ai/awadb) is an AI Native database for the search and storage of embedding vectors used by LLM Applications.\n\nYou'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration\n\nThis notebook shows how to use functionality related to the `AwaDB`.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 31930,
    "line_count": 825,
    "preview": "---\ntitle: \"Vector stores\"\n---\n\n## Overview\n\nA vector store stores [embedded](/oss/integrations/text_embedding) data and performs similarity search.\n\n```mermaid\nflowchart LR\n"
  }
,
  {
    "path": "python/integrations/vectorstores/baiducloud_vector_search.mdx",
    "filename": "baiducloud_vector_search.mdx",
    "size_bytes": 3084,
    "line_count": 73,
    "preview": "---\ntitle: Baidu Cloud ElasticSearch VectorSearch\n---\n\n>[Baidu Cloud VectorSearch](https://cloud.baidu.com/doc/BES/index.html?from=productToDoc) is a fully managed, enterprise-level distributed search and analysis service which is 100% compatible to open source. Baidu Cloud VectorSearch provides low-cost, high-performance, and reliable retrieval and analysis platform level product services for structured/unstructured data. As a vector database , it supports multiple index types and similarity distance methods.\n\n>`Baidu Cloud ElasticSearch` provides a privilege management mechanism, for you to  configure the cluster privileges freely, so as to further ensure data security.\n\nThis notebook shows how to use functionality related to the `Baidu Cloud ElasticSearch VectorStore`.\nTo run, you should have an [Baidu Cloud ElasticSearch](https://cloud.baidu.com/product/bes.html) instance up and running:\n"
  }
,
  {
    "path": "python/integrations/vectorstores/tiledb.mdx",
    "filename": "tiledb.mdx",
    "size_bytes": 2230,
    "line_count": 72,
    "preview": "---\ntitle: TileDB\n---\n\n> [TileDB](https://github.com/TileDB-Inc/TileDB) is a powerful engine for indexing and querying dense and sparse multi-dimensional arrays.\n\n> TileDB offers ANN search capabilities using the [TileDB-Vector-Search](https://github.com/TileDB-Inc/TileDB-Vector-Search) module. It provides serverless execution of ANN queries and storage of vector indexes both on local disk and cloud object stores (i.e. AWS S3).\n\nMore details in:\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/apache_doris.mdx",
    "filename": "apache_doris.mdx",
    "size_bytes": 3632,
    "line_count": 133,
    "preview": "---\ntitle: Apache Doris\n---\n\n>[Apache Doris](https://doris.apache.org/) is a modern data warehouse for real-time analytics.\nIt delivers lightning-fast analytics on real-time data at scale.\n\n>Usually `Apache Doris` is categorized into OLAP, and it has showed excellent performance in [ClickBench — a Benchmark For Analytical DBMS](https://benchmark.clickhouse.com/). Since it has a super-fast vectorized execution engine, it could also be used as a fast vectordb.\n\nYou'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration\n"
  }
,
  {
    "path": "python/integrations/vectorstores/zep.mdx",
    "filename": "zep.mdx",
    "size_bytes": 12825,
    "line_count": 328,
    "preview": "---\ntitle: Zep\n---\n\n> Recall, understand, and extract data from chat histories. Power personalized AI experiences.\n\n> [Zep](https://www.getzep.com) is a long-term memory service for AI Assistant apps.\n> With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant,\n> while also reducing hallucinations, latency, and cost.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/dingo.mdx",
    "filename": "dingo.mdx",
    "size_bytes": 3625,
    "line_count": 117,
    "preview": "---\ntitle: DingoDB\n---\n\n>[DingoDB](https://dingodb.readthedocs.io/en/latest/) is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.\n\nYou'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration\n\nThis notebook shows how to use functionality related to the DingoDB vector database.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/sqlitevec.mdx",
    "filename": "sqlitevec.mdx",
    "size_bytes": 5478,
    "line_count": 159,
    "preview": "---\ntitle: SQLiteVec\n---\n\nThis notebook covers how to get started with the SQLiteVec vector store.\n\n>[SQLite-Vec](https://alexgarcia.xyz/sqlite-vec/) is an `SQLite` extension designed for vector search, emphasizing local-first operations and easy integration into applications without external servers. It is the successor to [SQLite-VSS](https://alexgarcia.xyz/sqlite-vss/) by the same author. It is written in zero-dependency C and designed to be easy to build and use.\n\nThis notebook shows how to use the `SQLiteVec` vector database.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/google_vertex_ai_vector_search.mdx",
    "filename": "google_vertex_ai_vector_search.mdx",
    "size_bytes": 81073,
    "line_count": 903,
    "preview": "---\ntitle: Google Vertex AI Vector Search\n---\n\nThis page covers how to use Google Vertex AI Vector Search as a vector store in LangChain.\n\n## Overview\n\n[Google Vertex AI Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview) is a fully managed, high-scale, low-latency solution for finding similar vectors. It supports both exact and approximate nearest neighbor (ANN) search using Google's ScaNN (Scalable Nearest Neighbors) technology.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/tigris.mdx",
    "filename": "tigris.mdx",
    "size_bytes": 2730,
    "line_count": 76,
    "preview": "---\ntitle: Tigris\n---\n\n> [Tigris](https://tigrisdata.com) is an open-source Serverless NoSQL Database and Search Platform designed to simplify building high-performance vector search applications.\n> `Tigris` eliminates the infrastructure complexity of managing, operating, and synchronizing multiple tools, allowing you to focus on building great applications instead.\n\nThis notebook guides you how to use Tigris as your VectorStore\n\n**Pre requisites**\n"
  }
,
  {
    "path": "python/integrations/vectorstores/meilisearch.mdx",
    "filename": "meilisearch.mdx",
    "size_bytes": 6907,
    "line_count": 193,
    "preview": "---\ntitle: Meilisearch\n---\n\n> [Meilisearch](https://meilisearch.com) is an open-source, lightning-fast, and hyper relevant search engine. It comes with great defaults to help developers build snappy search experiences.\n>\n> You can [self-host Meilisearch](https://www.meilisearch.com/docs/learn/getting_started/installation#local-installation) or run on [Meilisearch Cloud](https://www.meilisearch.com/pricing).\n\nMeilisearch v1.3 supports vector search. This page guides you through integrating Meilisearch as a vector store and using it to perform vector search.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/aperturedb.mdx",
    "filename": "aperturedb.mdx",
    "size_bytes": 5761,
    "line_count": 155,
    "preview": "---\ntitle: ApertureDB\n---\n\n[ApertureDB](https://docs.aperturedata.io) is a database that stores, indexes, and manages multi-modal data like text, images, videos, bounding boxes, and embeddings, together with their associated metadata.\n\nThis notebook explains how to use the embeddings functionality of ApertureDB.\n\n## Install ApertureDB Python SDK\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/weaviate.mdx",
    "filename": "weaviate.mdx",
    "size_bytes": 26189,
    "line_count": 453,
    "preview": "---\ntitle: Weaviate\n---\n\nThis notebook covers how to get started with the Weaviate vector store in LangChain, using the `langchain-weaviate` package.\n\n> [Weaviate](https://weaviate.io/) is an open-source vector database. It allows you to store data objects and vector embeddings from your favorite ML-models, and scale seamlessly into billions of data objects.\n\nTo use this integration, you need to have a running Weaviate database instance.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/clickhouse.mdx",
    "filename": "clickhouse.mdx",
    "size_bytes": 7241,
    "line_count": 221,
    "preview": "---\ntitle: ClickHouse\n---\n\n> [ClickHouse](https://clickhouse.com/) is the fastest and most resource efficient open-source database for real-time apps and analytics with full SQL support and a wide range of functions to assist users in writing analytical queries. Lately added data structures and distance search functions (like `L2Distance`) as well as [approximate nearest neighbor search indexes](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/annindexes) enable ClickHouse to be used as a high performance and scalable vector database to store and search vectors with SQL.\n\nThis notebook shows how to use functionality related to the `ClickHouse` vector store.\n\n## Setup\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/google_spanner.mdx",
    "filename": "google_spanner.mdx",
    "size_bytes": 7190,
    "line_count": 207,
    "preview": "---\ntitle: Google Spanner\n---\n\n> [Spanner](https://cloud.google.com/spanner) is a highly scalable database that combines unlimited scalability with relational semantics, such as secondary indexes, strong consistency, schemas, and SQL providing 99.999% availability in one easy solution.\n\nThis notebook goes over how to use `Spanner` for Vector Search with `SpannerVectorStore` class.\n\nLearn more about the package on [GitHub](https://github.com/googleapis/langchain-google-spanner-python/).\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/databricks_vector_search.mdx",
    "filename": "databricks_vector_search.mdx",
    "size_bytes": 8397,
    "line_count": 248,
    "preview": "---\ntitle: Databricks Vector Search\n---\n\n\n[Databricks Vector Search](https://docs.databricks.com/en/generative-ai/vector-search.html) is a serverless similarity search engine that allows you to store a vector representation of your data, including metadata, in a vector database. With Vector Search, you can create auto-updating vector search indexes from Delta tables managed by Unity Catalog and query them with a simple API to return the most similar vectors.\n\nThis notebook shows how to use LangChain with Databricks Vector Search.\n\n## Setup\n"
  }
,
  {
    "path": "python/integrations/vectorstores/singlestore.mdx",
    "filename": "singlestore.mdx",
    "size_bytes": 16051,
    "line_count": 328,
    "preview": "---\ntitle: SingleStoreVectorStore\n---\n\n>[SingleStore](https://singlestore.com/) is a robust, high-performance distributed SQL database solution designed to excel in both [cloud](https://www.singlestore.com/cloud/) and on-premises environments. Boasting a versatile feature set, it offers seamless deployment options while delivering unparalleled performance.\n\nA standout feature of SingleStore is its advanced support for vector storage and operations, making it an ideal choice for applications requiring intricate AI capabilities such as text similarity matching. With built-in vector functions like [dot_product](https://docs.singlestore.com/managed-service/en/reference/sql-reference/vector-functions/dot_product.html) and [euclidean_distance](https://docs.singlestore.com/managed-service/en/reference/sql-reference/vector-functions/euclidean_distance.html), SingleStore empowers developers to implement sophisticated algorithms efficiently.\n\nFor developers keen on leveraging vector data within SingleStore, a comprehensive tutorial is available, guiding them through the intricacies of [working with vector data](https://docs.singlestore.com/managed-service/en/developer-resources/functional-extensions/working-with-vector-data.html). This tutorial delves into the Vector Store within SingleStoreDB, showcasing its ability to facilitate searches based on vector similarity. Leveraging vector indexes, queries can be executed with remarkable speed, enabling swift retrieval of relevant data.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/pinecone_sparse.mdx",
    "filename": "pinecone_sparse.mdx",
    "size_bytes": 10798,
    "line_count": 281,
    "preview": "---\ntitle: Pinecone (sparse)\n---\n\n>[Pinecone](https://docs.pinecone.io/docs/overview) is a vector database with broad functionality.\n\nThis notebook shows how to use functionality related to the `Pinecone` vector database.\n\n## Setup\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/starrocks.mdx",
    "filename": "starrocks.mdx",
    "size_bytes": 6402,
    "line_count": 154,
    "preview": "---\ntitle: StarRocks\n---\n\n>[StarRocks](https://www.starrocks.io/) is a High-Performance Analytical Database.\n`StarRocks` is a next-gen sub-second MPP database for full analytics scenarios, including multi-dimensional analytics, real-time analytics and ad-hoc query.\n\n>Usually `StarRocks` is categorized into OLAP, and it has showed excellent performance in [ClickBench — a Benchmark For Analytical DBMS](https://benchmark.clickhouse.com/). Since it has a super-fast vectorized execution engine, it could also be used as a fast vectordb.\n\nHere we'll show how to use the StarRocks Vector Store.\n"
  }
,
  {
    "path": "python/integrations/vectorstores/elasticsearch.mdx",
    "filename": "elasticsearch.mdx",
    "size_bytes": 23903,
    "line_count": 746,
    "preview": "---\ntitle: Elasticsearch\n---\n\n>[Elasticsearch](https://www.elastic.co/elasticsearch/) is a distributed, RESTful search and analytics engine, capable of performing both vector and lexical search. It is built on top of the Apache Lucene library.\n\nThis notebook shows how to use functionality related to the `Elasticsearch` vector store.\n\n## Setup\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/nucliadb.mdx",
    "filename": "nucliadb.mdx",
    "size_bytes": 1266,
    "line_count": 48,
    "preview": "---\ntitle: NucliaDB\n---\n\nYou can use a local NucliaDB instance or use [Nuclia Cloud](https://nuclia.cloud).\n\nWhen using a local instance, you need a Nuclia Understanding API key, so your texts are properly vectorized and indexed. You can get a key by creating a free account at [https://nuclia.cloud](https://nuclia.cloud), and then [create a NUA key](https://docs.nuclia.dev/docs/docs/using/understanding/intro).\n\n```python\npip install -qU  langchain langchain-community nuclia\n"
  }
,
  {
    "path": "python/integrations/vectorstores/milvus.mdx",
    "filename": "milvus.mdx",
    "size_bytes": 15202,
    "line_count": 416,
    "preview": "---\ntitle: Milvus\n---\n\n>[Milvus](https://milvus.io/docs/overview.md) is a database that stores, indexes, and manages massive embedding vectors generated by deep neural networks and other machine learning (ML) models.\n\nThis notebook shows how to use functionality related to the Milvus vector database.\n\n## Setup\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/qdrant.mdx",
    "filename": "qdrant.mdx",
    "size_bytes": 17597,
    "line_count": 507,
    "preview": "---\ntitle: Qdrant\n---\n\n>[Qdrant](https://qdrant.tech/documentation/) (read: quadrant) is a vector similarity search engine. It provides a production-ready service with a convenient API to store, search, and manage vectors with additional payload and extended filtering support. It makes it useful for all sorts of neural network or semantic-based matching, faceted search, and other applications.\n\nThis documentation demonstrates how to use Qdrant with LangChain for dense (i.e., embedding-based), sparse (i.e., text search) and hybrid retrieval. The `QdrantVectorStore` class supports multiple retrieval modes via Qdrant's new [Query API](https://qdrant.tech/blog/qdrant-1.10.x/). It requires you to run Qdrant v1.10.0 or above.\n\n## Setup\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/semadb.mdx",
    "filename": "semadb.mdx",
    "size_bytes": 4285,
    "line_count": 133,
    "preview": "---\ntitle: SemaDB\n---\n\n> [SemaDB](https://www.semafind.com/products/semadb) from [SemaFind](https://www.semafind.com) is a no fuss vector similarity database for building AI applications. The hosted `SemaDB Cloud` offers a no fuss developer experience to get started.\n\nThe full documentation of the API along with examples and an interactive playground is available on [RapidAPI](https://rapidapi.com/semafind-semadb/api/semadb).\n\nThis notebook demonstrates usage of the `SemaDB Cloud` vector store.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/opensearch.mdx",
    "filename": "opensearch.mdx",
    "size_bytes": 7292,
    "line_count": 252,
    "preview": "---\ntitle: OpenSearch\n---\n\n> [OpenSearch](https://opensearch.org/) is a scalable, flexible, and extensible open-source software suite for search, analytics, and observability applications licensed under Apache 2.0. `OpenSearch` is a distributed search and analytics engine based on `Apache Lucene`.\n\nThis notebook shows how to use functionality related to the `OpenSearch` database.\n\nTo run, you should have an OpenSearch instance up and running: [see here for an easy Docker installation](https://hub.docker.com/r/opensearchproject/opensearch).\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/azure_cosmos_db_mongo_vcore.mdx",
    "filename": "azure_cosmos_db_mongo_vcore.mdx",
    "size_bytes": 11646,
    "line_count": 267,
    "preview": "---\ntitle: Azure Cosmos DB Mongo vCore\n---\n\nThis notebook shows you how to leverage this integrated [vector database](https://learn.microsoft.com/en-us/azure/cosmos-db/vector-database) to store documents in collections, create indicies and perform vector search queries using approximate nearest neighbor algorithms such as COS (cosine distance), L2 (Euclidean distance), and IP (inner product) to locate documents close to the query vectors.\n\nAzure Cosmos DB is the database that powers OpenAI's ChatGPT service. It offers single-digit millisecond response times, automatic and instant scalability, along with guaranteed speed at any scale.\n\n[Azure Cosmos DB for MongoDB vCore](https://learn.microsoft.com/en-us/azure/cosmos-db/mongodb/vcore/) provides developers with a fully managed MongoDB-compatible database service for building modern applications with a familiar architecture. You can apply your MongoDB experience and continue to use your favorite MongoDB drivers, SDKs, and tools by pointing your application to the API for MongoDB vCore account's connection string.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/thirdai_neuraldb.mdx",
    "filename": "thirdai_neuraldb.mdx",
    "size_bytes": 3937,
    "line_count": 100,
    "preview": "---\ntitle: ThirdAI NeuralDB\n---\n\n>[NeuralDB](https://www.thirdai.com/neuraldb-enterprise/) is a CPU-friendly and fine-tunable vector store developed by [ThirdAI](https://www.thirdai.com/).\n\n## Initialization\n\nThere are two initialization methods:\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/pinecone.mdx",
    "filename": "pinecone.mdx",
    "size_bytes": 6520,
    "line_count": 219,
    "preview": "---\ntitle: Pinecone\n---\n\n>[Pinecone](https://docs.pinecone.io/docs/overview) is a vector database with broad functionality.\n\nThis notebook shows how to use functionality related to the `Pinecone` vector database.\n\n## Setup\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/rockset.mdx",
    "filename": "rockset.mdx",
    "size_bytes": 5656,
    "line_count": 144,
    "preview": "---\ntitle: Rockset\n---\n\n>[Rockset](https://rockset.com/) is a real-time search and analytics database built for the cloud. Rockset uses a [Converged Index™](https://rockset.com/blog/converged-indexing-the-secret-sauce-behind-rocksets-fast-queries/) with an efficient store for vector embeddings to serve low latency, high concurrency search queries at scale. Rockset has full support for metadata filtering and  handles real-time ingestion for constantly updating, streaming data.\n\nThis notebook demonstrates how to use `Rockset` as a vector store in LangChain. Before getting started, make sure you have access to a `Rockset` account and an API key available. [Start your free trial today.](https://rockset.com/create/)\n\nYou'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/astradb.mdx",
    "filename": "astradb.mdx",
    "size_bytes": 16328,
    "line_count": 398,
    "preview": "---\ntitle: Astra DB Vector Store\n---\n\nThis page provides a quickstart for using Astra DB as a Vector Store.\n\n> [DataStax Astra DB](https://docs.datastax.com/en/astra-db-serverless/index.html) is a serverless\n> AI-ready database built on `Apache Cassandra®` and made conveniently available\n> through an easy-to-use JSON API.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/jaguar.mdx",
    "filename": "jaguar.mdx",
    "size_bytes": 6646,
    "line_count": 230,
    "preview": "---\ntitle: Jaguar Vector Database\n---\n\n1. It is a distributed vector database\n2. The “ZeroMove” feature of JaguarDB enables instant horizontal scalability\n3. Multimodal: embeddings, text, images, videos, PDFs, audio, time series, and geospatial\n4. All-masters: allows both parallel reads and writes\n5. Anomaly detection capabilities\n6. RAG support: combines LLM with proprietary and real-time data\n"
  }
,
  {
    "path": "python/integrations/vectorstores/baiduvectordb.mdx",
    "filename": "baiduvectordb.mdx",
    "size_bytes": 2417,
    "line_count": 57,
    "preview": "---\ntitle: Baidu VectorDB\n---\n\n>[Baidu VectorDB](https://cloud.baidu.com/product/vdb.html) is a robust, enterprise-level distributed database service, meticulously developed and fully managed by Baidu Intelligent Cloud. It stands out for its exceptional ability to store, retrieve, and analyze multi-dimensional vector data. At its core, VectorDB operates on Baidu's proprietary \"Mochow\" vector database kernel, which ensures high performance, availability, and security, alongside remarkable scalability and user-friendliness.\n\n>This database service supports a diverse range of index types and similarity calculation methods, catering to various use cases. A standout feature of VectorDB is its capacity to manage an immense vector scale of up to 10 billion, while maintaining impressive query performance, supporting millions of queries per second (QPS) with millisecond-level query latency.\n\nYou'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/sqlitevss.mdx",
    "filename": "sqlitevss.mdx",
    "size_bytes": 3868,
    "line_count": 112,
    "preview": "---\ntitle: SQLite-VSS\n---\n\n>[SQLite-VSS](https://alexgarcia.xyz/sqlite-vss/) is an `SQLite` extension designed for vector search, emphasizing local-first operations and easy integration into applications without external servers. Leveraging the `Faiss` library, it offers efficient similarity search and clustering capabilities.\n\nYou'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration\n\nThis notebook shows how to use the `SQLiteVSS` vector database.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/relyt.mdx",
    "filename": "relyt.mdx",
    "size_bytes": 3211,
    "line_count": 86,
    "preview": "---\ntitle: Relyt\n---\n\n>[Relyt](https://docs.relyt.cn/docs/vector-engine/use/) is a cloud native data warehousing service that is designed to analyze large volumes of data online.\n\n>`Relyt` is compatible with the ANSI SQL 2003 syntax and the PostgreSQL and Oracle database ecosystems. Relyt also supports row store and column store. Relyt processes petabytes of data offline at a high performance level and supports highly concurrent online queries.\n\nThis notebook shows how to use functionality related to the `Relyt` vector database.\nTo run, you should have an [Relyt](https://docs.relyt.cn/) instance up and running:\n"
  }
,
  {
    "path": "python/integrations/vectorstores/mariadb.mdx",
    "filename": "mariadb.mdx",
    "size_bytes": 3490,
    "line_count": 148,
    "preview": "---\ntitle: MariaDB\n---\n\nLangChain's MariaDB integration (langchain-mariadb) provides vector capabilities for working with MariaDB version 11.7.1 and above, distributed under the MIT license. Users can use the provided implementations as-is or customize them for specific needs.\n Key features include:\n\n* Built-in vector similarity search\n* Support for cosine and euclidean distance metrics\n* Robust metadata filtering options\n"
  }
,
  {
    "path": "python/integrations/vectorstores/epsilla.mdx",
    "filename": "epsilla.mdx",
    "size_bytes": 2913,
    "line_count": 80,
    "preview": "---\ntitle: Epsilla\n---\n\n>[Epsilla](https://www.epsilla.com) is an open-source vector database that leverages the advanced parallel graph traversal techniques for vector indexing. Epsilla is licensed under GPL-3.0.\n\nYou'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration\n\nThis notebook shows how to use the functionalities related to the `Epsilla` vector database.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/vald.mdx",
    "filename": "vald.mdx",
    "size_bytes": 4234,
    "line_count": 148,
    "preview": "---\ntitle: Vald\n---\n\n> [Vald](https://github.com/vdaas/vald) is a highly scalable distributed fast approximate nearest neighbor (ANN) dense vector search engine.\n\nThis notebook shows how to use functionality related to the `Vald` database.\n\nTo run this notebook you need a running Vald cluster.\nCheck [Get Started](https://github.com/vdaas/vald#get-started) for more information.\n"
  }
,
  {
    "path": "python/integrations/vectorstores/alibabacloud_opensearch.mdx",
    "filename": "alibabacloud_opensearch.mdx",
    "size_bytes": 7577,
    "line_count": 177,
    "preview": "---\ntitle: Alibaba Cloud OpenSearch\n---\n\n>[Alibaba Cloud Opensearch](https://www.alibabacloud.com/product/opensearch) is a one-stop platform to develop intelligent search services. `OpenSearch` was built on the large-scale distributed search engine developed by `Alibaba`. `OpenSearch` serves more than 500 business cases in Alibaba Group and thousands of Alibaba Cloud customers. `OpenSearch` helps develop search services in different search scenarios, including e-commerce, O2O, multimedia, the content industry, communities and forums, and big data query in enterprises.\n\n>`OpenSearch` helps you develop high-quality, maintenance-free, and high-performance intelligent search services to provide your users with high search efficiency and accuracy.\n\n>`OpenSearch` provides the vector search feature. In specific scenarios, especially test question search and image search scenarios, you can use the vector search feature together with the multimodal search feature to improve the accuracy of search results.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/docarray_hnsw.mdx",
    "filename": "docarray_hnsw.mdx",
    "size_bytes": 3783,
    "line_count": 94,
    "preview": "---\ntitle: DocArray HnswSearch\n---\n\n>[DocArrayHnswSearch](https://docs.docarray.org/user_guide/storing/index_hnswlib/) is a lightweight Document Index implementation provided by [Docarray](https://github.com/docarray/docarray) that runs fully locally and is best suited for small- to medium-sized datasets. It stores vectors on disk in [hnswlib](https://github.com/nmslib/hnswlib), and stores all other data in [SQLite](https://www.sqlite.org/index.html).\n\nYou'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration\n\nThis notebook shows how to use functionality related to the `DocArrayHnswSearch`.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/pathway.mdx",
    "filename": "pathway.mdx",
    "size_bytes": 5216,
    "line_count": 86,
    "preview": "---\ntitle: Pathway\n---\n\n> [Pathway](https://pathway.com/) is an open data processing framework. It allows you to easily develop data transformation pipelines and Machine Learning applications that work with live data sources and changing data.\n\nThis notebook demonstrates how to use a live `Pathway` data indexing pipeline with `LangChain`. You can query the results of this pipeline from your chains in the same manner as you would a regular vector store. However, under the hood, Pathway updates the index on each data change giving you always up-to-date answers.\n\nIn this notebook, we will use a [public demo document processing pipeline](https://pathway.com/solutions/ai-pipelines#try-it-out) that:\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/momento_vector_index.mdx",
    "filename": "momento_vector_index.mdx",
    "size_bytes": 5663,
    "line_count": 159,
    "preview": "---\ntitle: Momento Vector Index (MVI)\n---\n\n>[MVI](https://gomomento.com): the most productive, easiest to use, serverless vector index for your data. To get started with MVI, simply sign up for an account. There's no need to handle infrastructure, manage servers, or be concerned about scaling. MVI is a service that scales automatically to meet your needs.\n\nTo sign up and access MVI, visit the [Momento Console](https://console.gomomento.com).\n\n# Setup\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/bageldb.mdx",
    "filename": "bageldb.mdx",
    "size_bytes": 3642,
    "line_count": 145,
    "preview": "---\ntitle: BagelDB\n---\n\n> [BagelDB](https://www.bageldb.ai/) (`Open Vector Database for AI`), is like GitHub for AI data.\nIt is a collaborative platform where users can create,\nshare, and manage vector datasets. It can support private projects for independent developers,\ninternal collaborations for enterprises, and public contributions for data DAOs.\n\n### Installation and setup\n"
  }
,
  {
    "path": "python/integrations/vectorstores/lancedb.mdx",
    "filename": "lancedb.mdx",
    "size_bytes": 11046,
    "line_count": 335,
    "preview": "---\ntitle: LanceDB\n---\n\n>[LanceDB](https://lancedb.com/) is an open-source database for vector-search built with persistent storage, which greatly simplifies retrevial, filtering and management of embeddings. Fully open source.\n\nThis notebook shows how to use functionality related to the `LanceDB` vector database based on the Lance data format.\n\n```python\n! pip install tantivy\n"
  }
,
  {
    "path": "python/integrations/vectorstores/upstash.mdx",
    "filename": "upstash.mdx",
    "size_bytes": 19983,
    "line_count": 231,
    "preview": "---\ntitle: Upstash Vector\n---\n\n> [Upstash Vector](https://upstash.com/docs/vector/overall/whatisvector) is a serverless vector database designed for working with vector embeddings.\n>\n> The vector langchain integration is a wrapper around the [upstash-vector](https://github.com/upstash/vector-py) package.\n>\n> The python package uses the [vector rest api](https://upstash.com/docs/vector/api/get-started) behind the scenes.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/tablestore.mdx",
    "filename": "tablestore.mdx",
    "size_bytes": 5260,
    "line_count": 165,
    "preview": "---\ntitle: Tablestore\n---\n\n[Tablestore](https://www.aliyun.com/product/ots) is a fully managed NoSQL cloud database service.\n\nTablestore enables storage of a massive amount of structured and semi-structured data.\n\nThis notebook shows how to use functionality related to the `Tablestore` vector database.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/google_scann.mdx",
    "filename": "google_scann.mdx",
    "size_bytes": 2799,
    "line_count": 85,
    "preview": "---\ntitle: ScaNN\n---\n\nScaNN (Scalable Nearest Neighbors) is a method for efficient vector similarity search at scale.\n\nScaNN includes search space pruning and quantization for Maximum Inner Product Search and also supports other distance functions such as Euclidean distance. The implementation is optimized for x86 processors with AVX2 support. See its [Google Research github](https://github.com/google-research/google-research/tree/master/scann) for more details.\n\nYou'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/marqo.mdx",
    "filename": "marqo.mdx",
    "size_bytes": 10478,
    "line_count": 325,
    "preview": "---\ntitle: Marqo\n---\n\nThis notebook shows how to use functionality related to the Marqo vectorstore.\n\n>[Marqo](https://www.marqo.ai/) is an open-source vector search engine. Marqo allows you to store and query multi-modal data such as text and images. Marqo creates the vectors for you using a huge selection of open-source models, you can also provide your own fine-tuned models and Marqo will handle the loading and inference for you.\n\nYou'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/vikingdb.mdx",
    "filename": "vikingdb.mdx",
    "size_bytes": 2455,
    "line_count": 93,
    "preview": "---\ntitle: viking DB\n---\n\n>[viking DB](https://www.volcengine.com/docs/6459/1163946) is a database that stores, indexes, and manages massive embedding vectors generated by deep neural networks and other machine learning (ML) models.\n\nThis notebook shows how to use functionality related to the VikingDB vector database.\n\nYou'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/timescalevector.mdx",
    "filename": "timescalevector.mdx",
    "size_bytes": 73282,
    "line_count": 1035,
    "preview": "---\ntitle: Timescale Vector (Postgres)\n---\n\n>[Timescale Vector](https://www.timescale.com/ai?utm_campaign=vectorlaunch&utm_source=langchain&utm_medium=referral) is `PostgreSQL++` vector database for AI applications.\n\nThis notebook shows how to use the Postgres vector database `Timescale Vector`. You'll learn how to use TimescaleVector for (1) semantic search, (2) time-based vector search, (3) self-querying, and (4) how to create indexes to speed up queries.\n\n## What is timescale vector?\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/gel.mdx",
    "filename": "gel.mdx",
    "size_bytes": 7618,
    "line_count": 252,
    "preview": "---\ntitle: Gel\n---\n\n> An implementation of LangChain vectorstore abstraction using `gel` as the backend.\n\n[Gel](https://www.geldata.com/) is an open-source PostgreSQL data layer optimized for fast development to production cycle. It comes with a high-level strictly typed graph-like data model, composable hierarchical query language, full SQL support, migrations, Auth and AI modules.\n\nThe code lives in an integration package called [langchain-gel](https://github.com/geldata/langchain-gel).\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/azure_cosmos_db_no_sql.mdx",
    "filename": "azure_cosmos_db_no_sql.mdx",
    "size_bytes": 40829,
    "line_count": 455,
    "preview": "---\ntitle: Azure Cosmos DB No SQL\n---\n\nThis notebook shows you how to leverage this integrated [vector database](https://learn.microsoft.com/en-us/azure/cosmos-db/vector-database) to store documents in collections, create indicies and perform vector search queries using approximate nearest neighbor algorithms such as COS (cosine distance), L2 (Euclidean distance), and IP (inner product) to locate documents close to the query vectors.\n\nAzure Cosmos DB is the database that powers OpenAI's ChatGPT service. It offers single-digit millisecond response times, automatic and instant scalability, along with guaranteed speed at any scale.\n\nAzure Cosmos DB for NoSQL now offers vector indexing and search in preview. This feature is designed to handle high-dimensional vectors, enabling efficient and accurate vector search at any scale. You can now store vectors directly in the documents alongside your data. This means that each document in your database can contain not only traditional schema-free data, but also high-dimensional vectors as other properties of the documents. This colocation of data and vectors allows for efficient indexing and searching, as the vectors are stored in the same logical unit as the data they represent. This simplifies data management, AI application architectures, and the efficiency of vector-based operations.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/analyticdb.mdx",
    "filename": "analyticdb.mdx",
    "size_bytes": 3693,
    "line_count": 85,
    "preview": "---\ntitle: AnalyticDB\n---\n\n>[AnalyticDB for PostgreSQL](https://www.alibabacloud.com/help/en/analyticdb-for-postgresql/latest/product-introduction-overview) is a massively parallel processing (MPP) data warehousing service that is designed to analyze large volumes of data online.\n\n>`AnalyticDB for PostgreSQL` is developed based on the open-source `Greenplum Database` project and is enhanced with in-depth extensions by `Alibaba Cloud`. AnalyticDB for PostgreSQL is compatible with the ANSI SQL 2003 syntax and the PostgreSQL and Oracle database ecosystems. AnalyticDB for PostgreSQL also supports row store and column store. AnalyticDB for PostgreSQL processes petabytes of data offline at a high performance level and supports highly concurrent online queries.\n\nYou'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/usearch.mdx",
    "filename": "usearch.mdx",
    "size_bytes": 3784,
    "line_count": 81,
    "preview": "---\ntitle: USearch\n---\n\n>[USearch](https://unum-cloud.github.io/usearch/) is a Smaller & Faster Single-File Vector Search Engine\n\n>USearch's base functionality is identical to FAISS, and the interface should look familiar if you have ever investigated Approximate Nearest Neigbors search. FAISS is a widely recognized standard for high-performance vector search engines. USearch and FAISS both employ the same HNSW algorithm, but they differ significantly in their design principles. USearch is compact and broadly compatible without sacrificing performance, with a primary focus on user-defined metrics and fewer dependencies.\n\n```python\npip install -qU  usearch langchain-community\n"
  }
,
  {
    "path": "python/integrations/vectorstores/google_vertex_ai_feature_store.mdx",
    "filename": "google_vertex_ai_feature_store.mdx",
    "size_bytes": 207893,
    "line_count": 181,
    "preview": "---\ntitle: Google Vertex AI Feature Store\n---\n\n> [Google Cloud Vertex Feature Store](https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview) streamlines your ML feature management and online serving processes by letting you serve at low-latency your data in [Google Cloud BigQuery](https://cloud.google.com/bigquery?hl=en), including the capacity to perform approximate neighbor retrieval for embeddings\n\nThis tutorial shows you how to easily perform low-latency vector search and approximate nearest neighbor retrieval directly from your BigQuery data, enabling powerful ML applications with minimal setup. We will do that using the `VertexFSVectorStore` class.\n\nThis class is part of a set of 2 classes capable of providing a unified data storage and flexible vector search in Google Cloud:\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/couchbase.mdx",
    "filename": "couchbase.mdx",
    "size_bytes": 33040,
    "line_count": 891,
    "preview": "---\ntitle: Couchbase\n---\n\n[Couchbase](http://couchbase.com/) is an award-winning distributed NoSQL cloud database that delivers unmatched versatility, performance, scalability, and financial value for all of your cloud, mobile, AI, and edge computing applications. Couchbase embraces AI with coding assistance for developers and vector search for their applications.\n\nCouchbase provides two different vector store implementations for LangChain:\n\n| Vector Store | Index Type | Minimum Version | Best For |\n|-------------|-----------|-----------------|----------|\n"
  }
,
  {
    "path": "python/integrations/vectorstores/ecloud_vector_search.mdx",
    "filename": "ecloud_vector_search.mdx",
    "size_bytes": 6082,
    "line_count": 206,
    "preview": "---\ntitle: China Mobile ECloud ElasticSearch VectorSearch\n---\n\n>[China Mobile ECloud VectorSearch](https://ecloud.10086.cn/portal/product/elasticsearch) is a fully managed, enterprise-level distributed search and analysis service. China Mobile ECloud VectorSearch provides low-cost, high-performance, and reliable retrieval and analysis platform level product services for structured/unstructured data. As a vector database , it supports multiple index types and similarity distance methods.\n\nYou'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration\n\nThis notebook shows how to use functionality related to the `ECloud ElasticSearch VectorStore`.\nTo run, you should have an [China Mobile ECloud VectorSearch](https://ecloud.10086.cn/portal/product/elasticsearch) instance up and running:\n"
  }
,
  {
    "path": "python/integrations/vectorstores/docarray_in_memory.mdx",
    "filename": "docarray_in_memory.mdx",
    "size_bytes": 3486,
    "line_count": 87,
    "preview": "---\ntitle: DocArray InMemorySearch\n---\n\n>[DocArrayInMemorySearch](https://docs.docarray.org/user_guide/storing/index_in_memory/) is a document index provided by [Docarray](https://github.com/docarray/docarray) that stores documents in memory. It is a great starting point for small datasets, where you may not want to launch a database server.\n\nThis notebook shows how to use functionality related to the `DocArrayInMemorySearch`.\n\n## Setup\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/xata.mdx",
    "filename": "xata.mdx",
    "size_bytes": 3921,
    "line_count": 97,
    "preview": "---\ntitle: Xata\n---\n\n> [Xata](https://xata.io) is a serverless data platform, based on PostgreSQL. It provides a Python SDK for interacting with your database, and a UI for managing your data.\n> Xata has a native vector type, which can be added to any table, and supports similarity search. LangChain inserts vectors directly to Xata, and queries it for the nearest neighbors of a given vector, so that you can use all the LangChain Embeddings integrations with Xata.\n\nThis notebook guides you how to use Xata as a VectorStore.\n\n## Setup\n"
  }
,
  {
    "path": "python/integrations/vectorstores/pgvector.mdx",
    "filename": "pgvector.mdx",
    "size_bytes": 8937,
    "line_count": 252,
    "preview": "---\ntitle: PGVector\n---\n\n> An implementation of LangChain vectorstore abstraction using `postgres` as the backend and utilizing the `pgvector` extension.\n\nThe code lives in an integration package called: [langchain-postgres](https://github.com/langchain-ai/langchain-postgres/).\n\n## Status\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/pgvectorstore.mdx",
    "filename": "pgvectorstore.mdx",
    "size_bytes": 64152,
    "line_count": 477,
    "preview": "---\ntitle: PGVectorStore\n---\n\n`PGVectorStore` is an implementation of a LangChain vectorstore using `postgres` as the backend.\n\nThis notebook goes over how to use the `PGVectorStore` API.\n\nThe code lives in an integration package called: [langchain-postgres](https://github.com/langchain-ai/langchain-postgres/).\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/faiss.mdx",
    "filename": "faiss.mdx",
    "size_bytes": 9342,
    "line_count": 315,
    "preview": "---\ntitle: Faiss\n---\n\n>[Facebook AI Similarity Search (FAISS)](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also includes supporting code for evaluation and parameter tuning.\n>\n>See [The FAISS Library](https://arxiv.org/pdf/2401.08281) paper.\n\nYou can find the FAISS documentation at [this page](https://faiss.ai/).\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/vlite.mdx",
    "filename": "vlite.mdx",
    "size_bytes": 4267,
    "line_count": 153,
    "preview": "---\ntitle: vlite\n---\n\nVLite is a simple and blazing fast vector database that allows you to store and retrieve data semantically using embeddings. Made with numpy, vlite is a lightweight batteries-included database to implement RAG, similarity search, and embeddings into your projects.\n\nYou'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration\n\n## Installation\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/atlas.mdx",
    "filename": "atlas.mdx",
    "size_bytes": 1855,
    "line_count": 71,
    "preview": "---\ntitle: Atlas\n---\n\n>[Atlas](https://docs.nomic.ai/index.html) is a platform by Nomic made for interacting with both small and internet scale unstructured datasets. It enables anyone to visualize, search, and share massive datasets in their browser.\n\nYou'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration\n\nThis notebook shows you how to use functionality related to the `AtlasDB` vectorstore.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/vespa.mdx",
    "filename": "vespa.mdx",
    "size_bytes": 14956,
    "line_count": 449,
    "preview": "---\ntitle: Vespa\n---\n\n>[Vespa](https://vespa.ai/) is a fully featured search engine and vector database. It supports vector search (ANN), lexical search, and search in structured data, all in the same query.\n\nThis notebook shows how to use `Vespa.ai` as a LangChain vector store.\n\nYou'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/sklearn.mdx",
    "filename": "sklearn.mdx",
    "size_bytes": 4584,
    "line_count": 118,
    "preview": "---\ntitle: scikit-learn\n---\n\n>[scikit-learn](https://scikit-learn.org/stable/) is an open-source collection of machine learning algorithms, including some implementations of the [k nearest neighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html). `SKLearnVectorStore` wraps this implementation and adds the possibility to persist the vector store in json, bson (binary json) or Apache Parquet format.\n\nThis notebook shows how to use the `SKLearnVectorStore` vector database.\n\nYou'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/zilliz.mdx",
    "filename": "zilliz.mdx",
    "size_bytes": 3074,
    "line_count": 86,
    "preview": "---\ntitle: Zilliz\n---\n\n>[Zilliz Cloud](https://zilliz.com/doc/quick_start) is a fully managed service on cloud for `LF AI Milvus®`,\n\nThis notebook shows how to use functionality related to the Zilliz Cloud managed vector database.\n\nYou'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/vdms.mdx",
    "filename": "vdms.mdx",
    "size_bytes": 17466,
    "line_count": 533,
    "preview": "---\ntitle: Intel's Visual Data Management System (VDMS)\n---\n\nThis notebook covers how to get started with VDMS as a vector store.\n\n>Intel's [Visual Data Management System (VDMS)](https://github.com/IntelLabs/vdms) is a storage solution for efficient access of big-”visual”-data that aims to achieve cloud scale by searching for relevant visual data via visual metadata stored as a graph and enabling machine friendly enhancements to visual data for faster access. VDMS is licensed under MIT. For more information on `VDMS`, visit [this page](https://github.com/IntelLabs/vdms/wiki), and find the LangChain API reference [here](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.vdms.VDMS.html).\n\nVDMS supports:\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/surrealdb.mdx",
    "filename": "surrealdb.mdx",
    "size_bytes": 6381,
    "line_count": 174,
    "preview": "---\ntitle: SurrealDBVectorStore\n---\n\n> [SurrealDB](https://surrealdb.com) is a unified, multi-model database purpose-built for AI systems. It combines structured and unstructured data (including vector search, graph traversal, relational queries, full-text search, document storage, and time-series data) into a single ACID-compliant engine, scaling from a 3 MB edge binary to petabyte-scale clusters in the cloud. By eliminating the need for multiple specialized stores, SurrealDB simplifies architectures, reduces latency, and ensures consistency for AI workloads.\n>\n> **Why SurrealDB Matters for GenAI Systems**\n>\n> - **One engine for storage and memory:** Combine durable storage and fast, agent-friendly memory in a single system, providing all the data your agent needs and removing the need to sync multiple systems.\n> - **One-hop memory for agents:** Run vector search, graph traversal, semantic joins, and transactional writes in a single query, giving LLM agents fast, consistent memory access without stitching relational, graph and vector databases together.\n"
  }
,
  {
    "path": "python/integrations/vectorstores/bagel.mdx",
    "filename": "bagel.mdx",
    "size_bytes": 3639,
    "line_count": 145,
    "preview": "---\ntitle: Bagel\n---\n\n> [Bagel](https://www.bagel.net/) (`Open Inference platform for AI`), is like GitHub for AI data.\nIt is a collaborative platform where users can create,\nshare, and manage Inference datasets. It can support private projects for independent developers,\ninternal collaborations for enterprises, and public contributions for data DAOs.\n\n### Installation and setup\n"
  }
,
  {
    "path": "python/integrations/vectorstores/tidb_vector.mdx",
    "filename": "tidb_vector.mdx",
    "size_bytes": 18587,
    "line_count": 447,
    "preview": "---\ntitle: TiDB Vector\n---\n\n> [TiDB Cloud](https://www.pingcap.com/tidb-serverless), is a comprehensive Database-as-a-Service (DBaaS) solution, that provides dedicated and serverless options. TiDB Serverless is now integrating a built-in vector search into the MySQL landscape. With this enhancement, you can seamlessly develop AI applications using TiDB Serverless without the need for a new database or additional technical stacks. Create a free TiDB Serverless cluster and start using the vector search feature at [pingcap.com/ai](https://pingcap.com/ai).\n\nThis guide provides a detailed guide on utilizing the TiDB Vector functionality, showcasing its features and practical applications.\n\n## Setting up environments\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/vearch.mdx",
    "filename": "vearch.mdx",
    "size_bytes": 31171,
    "line_count": 404,
    "preview": "---\ntitle: Vearch\n---\n\n>[Vearch](https://vearch.readthedocs.io) is the vector search infrastructure for deeping learning and AI applications.\n\n## Setting up\n\nFollow [instructions](https://vearch.readthedocs.io/en/latest/quick-start-guide.html#).\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/chroma.mdx",
    "filename": "chroma.mdx",
    "size_bytes": 11218,
    "line_count": 373,
    "preview": "---\ntitle: Chroma\n---\n\nThis notebook covers how to get started with the `Chroma` vector store.\n\n>[Chroma](https://docs.trychroma.com/getting-started) is a AI-native open-source vector database focused on developer productivity and happiness. Chroma is licensed under Apache 2.0. View the full docs of `Chroma` at [this page](https://docs.trychroma.com/reference/py-collection), and find the API reference for the LangChain integration at [this page](https://python.langchain.com/api_reference/chroma/vectorstores/langchain_chroma.vectorstores.Chroma.html).\n\n<Info>\n**Chroma Cloud**\n"
  }
,
  {
    "path": "python/integrations/vectorstores/kinetica.mdx",
    "filename": "kinetica.mdx",
    "size_bytes": 7183,
    "line_count": 290,
    "preview": "---\ntitle: Kinetica Vectorstore API\n---\n\n>[Kinetica](https://www.kinetica.com/) is a database with integrated support for vector similarity search\n\nIt supports:\n\n- exact and approximate nearest neighbor search\n- L2 distance, inner product, and cosine distance\n"
  }
,
  {
    "path": "python/integrations/vectorstores/faiss_async.mdx",
    "filename": "faiss_async.mdx",
    "size_bytes": 8610,
    "line_count": 258,
    "preview": "---\ntitle: Faiss (Async)\n---\n\n>[Facebook AI Similarity Search (Faiss)](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also includes supporting code for evaluation and parameter tuning.\n>\n>See [The FAISS Library](https://arxiv.org/pdf/2401.08281) paper.\n\n[Faiss documentation](https://faiss.ai/).\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/oceanbase.mdx",
    "filename": "oceanbase.mdx",
    "size_bytes": 4487,
    "line_count": 160,
    "preview": "---\ntitle: OceanbaseVectorStore\n---\n\nThis notebook covers how to get started with the Oceanbase vector store.\n\n## Setup\n\nTo access Oceanbase vector stores you'll need to deploy a standalone OceanBase server:\n%docker run --name=ob433 -e MODE=mini -e OB_SERVER_IP=127.0.0.1 -p 2881:2881 -d quay.io/oceanbase/oceanbase-ce:4.3.3.1-101000012024102216\n"
  }
,
  {
    "path": "python/integrations/vectorstores/annoy.mdx",
    "filename": "annoy.mdx",
    "size_bytes": 12094,
    "line_count": 264,
    "preview": "---\ntitle: Annoy\n---\n\n> [Annoy](https://github.com/spotify/annoy) (`Approximate Nearest Neighbors Oh Yeah`) is a C++ library with Python bindings to search for points in space that are close to a given query point. It also creates large read-only file-based data structures that are mapped into memory so that many processes may share the same data.\n\nYou'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration\n\nThis notebook shows how to use functionality related to the `Annoy` vector database.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/dashvector.mdx",
    "filename": "dashvector.mdx",
    "size_bytes": 2795,
    "line_count": 95,
    "preview": "---\ntitle: DashVector\n---\n\n> [DashVector](https://help.aliyun.com/document_detail/2510225.html) is a fully-managed vectorDB service that supports high-dimension dense and sparse vectors, real-time insertion and filtered search. It is built to scale automatically and can adapt to different application requirements.\n\nThis notebook shows how to use functionality related to the `DashVector` vector database.\n\nTo use DashVector, you must have an API key.\nHere are the [installation instructions](https://help.aliyun.com/document_detail/2510223.html).\n"
  }
,
  {
    "path": "python/integrations/vectorstores/cassandra.mdx",
    "filename": "cassandra.mdx",
    "size_bytes": 9697,
    "line_count": 293,
    "preview": "---\ntitle: Apache Cassandra\n---\n\nThis page provides a quickstart for using [Apache Cassandra®](https://cassandra.apache.org/) as a Vector Store.\n\n> [Cassandra](https://cassandra.apache.org/) is a NoSQL, row-oriented, highly scalable and highly available database.Starting with version 5.0, the database ships with [vector search capabilities](https://cassandra.apache.org/doc/trunk/cassandra/vector-search/overview.html).\n\n_Note: in addition to access to the database, an OpenAI API Key is required to run the full example._\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/tencentvectordb.mdx",
    "filename": "tencentvectordb.mdx",
    "size_bytes": 7882,
    "line_count": 189,
    "preview": "---\ntitle: Tencent Cloud VectorDB\n---\n\n>[Tencent Cloud VectorDB](https://cloud.tencent.com/document/product/1709) is a fully managed, self-developed, enterprise-level distributed database service designed for storing, retrieving, and analyzing multi-dimensional vector data. The database supports multiple index types and similarity calculation methods. A single index can support a vector scale of up to 1 billion and can support millions of QPS and millisecond-level query latency. Tencent Cloud Vector Database can not only provide an external knowledge base for large models to improve the accuracy of large model responses but can also be widely used in AI fields such as recommendation systems, NLP services, computer vision, and intelligent customer service.\n\nThis notebook shows how to use functionality related to the Tencent vector database.\n\nTo run, you should have a [Database instance.](https://cloud.tencent.com/document/product/1709/95101).\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/manticore_search.mdx",
    "filename": "manticore_search.mdx",
    "size_bytes": 18753,
    "line_count": 292,
    "preview": "---\ntitle: ManticoreSearch VectorStore\n---\n\n[ManticoreSearch](https://manticoresearch.com/) is an open-source search engine that offers fast, scalable, and user-friendly capabilities. Originating as a fork of [Sphinx Search](http://sphinxsearch.com/), it has evolved to incorporate modern search engine features and improvements. ManticoreSearch distinguishes itself with its robust performance and ease of integration into various applications.\n\nManticoreSearch has recently introduced [vector search capabilities](https://manual.manticoresearch.com/dev/Searching/KNN), starting with search engine version 6.2 and only with [manticore-columnar-lib](https://github.com/manticoresoftware/columnar) package installed. This feature is a considerable advancement, allowing for the execution of searches based on vector similarity.\n\nAs of now, the vector search functionality is only accessible in the developmental (dev) versions of the search engine. Consequently, it is imperative to employ a developmental [manticoresearch-dev](https://pypi.org/project/manticoresearch-dev/) Python client for utilizing this feature effectively.\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/duckdb.mdx",
    "filename": "duckdb.mdx",
    "size_bytes": 1043,
    "line_count": 46,
    "preview": "---\ntitle: DuckDB\n---\n\nThis notebook shows how to use `DuckDB` as a vector store.\n\n```python\n! pip install duckdb langchain langchain-community langchain-openai\n```\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/google_cloud_sql_mysql.mdx",
    "filename": "google_cloud_sql_mysql.mdx",
    "size_bytes": 11578,
    "line_count": 300,
    "preview": "---\ntitle: Google Cloud SQL for MySQL\n---\n\n> [Cloud SQL](https://cloud.google.com/sql) is a fully managed relational database service that offers high performance, seamless integration, and impressive scalability. It offers PostgreSQL, MySQL, and SQL Server database engines. Extend your database application to build AI-powered experiences leveraging Cloud SQL's LangChain integrations.\n\nThis notebook goes over how to use `Cloud SQL for MySQL` to store vector embeddings with the `MySQLVectorStore` class.\n\nLearn more about the package on [GitHub](https://github.com/googleapis/langchain-google-cloud-sql-mysql-python/).\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/azuresearch.mdx",
    "filename": "azuresearch.mdx",
    "size_bytes": 23612,
    "line_count": 528,
    "preview": "---\ntitle: Azure AI Search\n---\n\n[Azure AI Search](https://learn.microsoft.com/azure/search/search-what-is-azure-search) (formerly known as `Azure Search` and `Azure Cognitive Search`) is a cloud search service that gives developers infrastructure, APIs, and tools for information retrieval of vector, keyword, and hybrid queries at scale.\n\nYou'll need to install `langchain-community` with `pip install -qU langchain-community` to use this integration\n\n## Install Azure AI search SDK\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/google_firestore.mdx",
    "filename": "google_firestore.mdx",
    "size_bytes": 6095,
    "line_count": 184,
    "preview": "---\ntitle: Google Firestore\n---\n\n> [Firestore](https://cloud.google.com/firestore) is a serverless document-oriented database that scales to meet any demand. Extend your database application to build AI-powered experiences leveraging Firestore's LangChain integrations.\n\nThis notebook goes over how to use [Firestore](https://cloud.google.com/firestore) to store vectors and query them using the `FirestoreVectorStore` class.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googleapis/langchain-google-firestore-python/blob/main/docs/vectorstores.ipynb)\n\n"
  }
,
  {
    "path": "python/integrations/vectorstores/clarifai.mdx",
    "filename": "clarifai.mdx",
    "size_bytes": 8499,
    "line_count": 206,
    "preview": "---\ntitle: Clarifai\n---\n\n>[Clarifai](https://www.clarifai.com/) is an AI Platform that provides the full AI lifecycle ranging from data exploration, data labeling, model training, evaluation, and inference. A Clarifai application can be used as a vector database after uploading inputs.\n\nThis notebook shows how to use functionality related to the `Clarifai` vector database. Examples are shown to demonstrate text semantic search capabilities. Clarifai also supports semantic search with images, video frames, and localized search (see [Rank](https://docs.clarifai.com/api-guide/search/rank)) and attribute search (see [Filter](https://docs.clarifai.com/api-guide/search/filter)).\n\nTo use Clarifai, you must have an account and a Personal Access Token (PAT) key.\n[Check here](https://clarifai.com/settings/security) to get or create a PAT.\n"
  }
,
  {
    "path": "python/integrations/vectorstores/sqlserver.mdx",
    "filename": "sqlserver.mdx",
    "size_bytes": 34961,
    "line_count": 501,
    "preview": "---\ntitle: SQLServer\n---\n\n>Azure SQL provides a dedicated [Vector data type](https:\\learn.microsoft.com\\sql\\t-sql\\data-types\\vector-data-type?view=azuresqldb-current&viewFallbackFrom=sql-server-ver16&tabs=csharp-sample) that simplifies the creation, storage, and querying of vector embeddings directly within a relational database. This eliminates the need for separate vector databases and related integrations, increasing the security of your solutions while reducing the overall complexity.\n\nAzure SQL is a robust service that combines scalability, security, and high availability, providing all the benefits of a modern database solution. It leverages a sophisticated query optimizer and enterprise features to perform vector similarity searches alongside traditional SQL queries, enhancing data analysis and decision-making.\n\nRead more on using [Intelligent applications with Azure SQL Database](https://learn.microsoft.com/azure/azure-sql/database/ai-artificial-intelligence-intelligent-applications?view=azuresql)\n\n"
  }
,
  {
    "path": "python/integrations/adapters/openai.mdx",
    "filename": "openai.mdx",
    "size_bytes": 3866,
    "line_count": 146,
    "preview": "---\ntitle: OpenAI Adapter\n---\n\n**Please ensure OpenAI library is version 1.0.0 or higher; otherwise, refer to the older doc [OpenAI Adapter(Old)](/oss/integrations/adapters/openai-old/).**\n\nA lot of people get started with OpenAI but want to explore other models. LangChain's integrations with many model providers make this easy to do so. While LangChain has it's own message and model APIs, we've also made it as easy as possible to explore other models by exposing an adapter to adapt LangChain models to the OpenAI api.\n\nAt the moment this only deals with output and does not return other information (token counts, stop reasons, etc).\n\n"
  }
,
  {
    "path": "python/integrations/adapters/openai-old.mdx",
    "filename": "openai-old.mdx",
    "size_bytes": 2838,
    "line_count": 126,
    "preview": "---\ntitle: OpenAI Adapter(Old)\n---\n\n**Please ensure OpenAI library is less than 1.0.0; otherwise, refer to the newer doc [OpenAI Adapter](/oss/integrations/adapters/openai/).**\n\nA lot of people get started with OpenAI but want to explore other models. LangChain's integrations with many model providers make this easy to do so. While LangChain has it's own message and model APIs, we've also made it as easy as possible to explore other models by exposing an adapter to adapt LangChain models to the OpenAI api.\n\nAt the moment this only deals with output and does not return other information (token counts, stop reasons, etc).\n\n"
  }
,
  {
    "path": "python/integrations/document_transformers/google_docai.mdx",
    "filename": "google_docai.mdx",
    "size_bytes": 3448,
    "line_count": 118,
    "preview": "---\ntitle: Google Cloud Document AI\n---\n\nDocument AI is a document understanding platform from Google Cloud to transform unstructured data from documents into structured data, making it easier to understand, analyze, and consume.\n\nLearn more:\n\n- [Document AI overview](https://cloud.google.com/document-ai/docs/overview)\n- [Document AI videos and labs](https://cloud.google.com/document-ai/docs/videos)\n"
  }
,
  {
    "path": "python/integrations/document_transformers/cross_encoder_reranker.mdx",
    "filename": "cross_encoder_reranker.mdx",
    "size_bytes": 6883,
    "line_count": 187,
    "preview": "---\ntitle: Cross Encoder Reranker\n---\n\nThis notebook shows how to implement reranker in a retriever with your own cross encoder from [Hugging Face cross encoder models](https://huggingface.co/cross-encoder) or Hugging Face models that implements cross encoder function ([example: BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)). `SagemakerEndpointCrossEncoder` enables you to use these HuggingFace models loaded on Sagemaker.\n\nThis builds on top of ideas in the ContextualCompressionRetriever. Overall structure of this document came from [Cohere Reranker documentation](/oss/integrations/retrievers/cohere-reranker).\n\nFor more about why cross encoder can be used as reranking mechanism in conjunction with embeddings for better retrieval, refer to [Hugging Face Cross-Encoders documentation](https://www.sbert.net/examples/applications/cross-encoder/README.html).\n\n"
  }
,
  {
    "path": "python/integrations/document_transformers/openvino_rerank.mdx",
    "filename": "openvino_rerank.mdx",
    "size_bytes": 19134,
    "line_count": 382,
    "preview": "---\ntitle: OpenVINO Reranker\n---\n\n[OpenVINO™](https://github.com/openvinotoolkit/openvino) is an open-source toolkit for optimizing and deploying AI inference. The OpenVINO™ Runtime supports various hardware [devices](https://github.com/openvinotoolkit/openvino?tab=readme-ov-file#supported-hardware-matrix) including x86 and ARM CPUs, and Intel GPUs. It can help to boost deep learning performance in Computer Vision, Automatic Speech Recognition, Natural Language Processing and other common tasks.\n\nHugging Face rerank model can be supported by OpenVINO through `OpenVINOReranker` class. If you have an Intel GPU, you can specify `model_kwargs={\"device\": \"GPU\"}` to run inference on it.\n\n```python\npip install -U-strategy eager \"optimum[openvino,nncf]\" --quiet\n"
  }
,
  {
    "path": "python/integrations/document_transformers/doctran_extract_properties.mdx",
    "filename": "doctran_extract_properties.mdx",
    "size_bytes": 9297,
    "line_count": 177,
    "preview": "---\ntitle: Doctran extract properties\n---\n\nWe can extract useful features of documents using the [Doctran](https://github.com/psychic-api/doctran) library, which uses OpenAI's function calling feature to extract specific metadata.\n\nExtracting metadata from documents is helpful for a variety of tasks, including:\n\n* **Classification:** classifying documents into different categories\n* **Data mining:** Extract structured data that can be used for data analysis\n"
  }
,
  {
    "path": "python/integrations/document_transformers/openai_metadata_tagger.mdx",
    "filename": "openai_metadata_tagger.mdx",
    "size_bytes": 5315,
    "line_count": 172,
    "preview": "---\ntitle: OpenAI metadata tagger\n---\n\nIt can often be useful to tag ingested documents with structured metadata, such as the title, tone, or length of a document, to allow for a more targeted similarity search later. However, for large numbers of documents, performing this labelling process manually can be tedious.\n\nThe `OpenAIMetadataTagger` document transformer automates this process by extracting metadata from each provided document according to a provided schema. It uses a configurable `OpenAI Functions`-powered chain under the hood, so if you pass a custom LLM instance, it must be an OpenAI model with functions support.\n\n**Note:** This document transformer works best with complete documents, so it's best to run it first with whole documents before doing any other splitting or processing!\n\n"
  }
,
  {
    "path": "python/integrations/document_transformers/volcengine_rerank.mdx",
    "filename": "volcengine_rerank.mdx",
    "size_bytes": 15976,
    "line_count": 336,
    "preview": "---\ntitle: Volcengine Reranker\n---\n\nThis notebook shows how to use Volcengine Reranker for document compression and retrieval. [Volcengine](https://www.volcengine.com/) is a cloud service platform developed by ByteDance, the parent company of TikTok.\n\nVolcengine's Rerank Service supports reranking up to 50 documents with a maximum of 4000 tokens. For more, please visit [here](https://www.volcengine.com/docs/84313/1254474) and [here](https://www.volcengine.com/docs/84313/1254605).\n\n```python\npip install -qU  volcengine\n"
  }
,
  {
    "path": "python/integrations/document_transformers/markdownify.mdx",
    "filename": "markdownify.mdx",
    "size_bytes": 132222,
    "line_count": 166,
    "preview": "---\ntitle: Markdownify\n---\n\n> [markdownify](https://github.com/matthewwithanm/python-markdownify) is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.\n\n```python\npip install -qU  markdownify\n```\n\n"
  }
,
  {
    "path": "python/integrations/document_transformers/google_cloud_vertexai_rerank.mdx",
    "filename": "google_cloud_vertexai_rerank.mdx",
    "size_bytes": 24711,
    "line_count": 556,
    "preview": "---\ntitle: Google Cloud Vertex AI Reranker\n---\n\n> The [Vertex Search Ranking API](https://cloud.google.com/generative-ai-app-builder/docs/ranking) is one of the standalone APIs in [Vertex AI Agent Builder](https://cloud.google.com/generative-ai-app-builder/docs/builder-apis). It takes a list of documents and reranks those documents based on how relevant the documents are to a query. Compared to embeddings, which look only at the semantic similarity of a document and a query, the ranking API can give you precise scores for how well a document answers a given query. The ranking API can be used to improve the quality of search results after retrieving an initial set of candidate documents.\n\n>The ranking API is stateless so there's no need to index documents before calling the API. All you need to do is pass in the query and documents. This makes the API well suited for reranking documents from any document retrievers.\n\n>For more information, see [Rank and rerank documents](https://cloud.google.com/generative-ai-app-builder/docs/ranking).\n\n"
  }
,
  {
    "path": "python/integrations/document_transformers/html2text.mdx",
    "filename": "html2text.mdx",
    "size_bytes": 3063,
    "line_count": 119,
    "preview": "---\ntitle: HTML to text\n---\n\n>[html2text](https://github.com/Alir3z4/html2text/) is a Python package that converts a page of `HTML` into clean, easy-to-read plain `ASCII text`.\n\nThe ASCII also happens to be a valid `Markdown` (a text-to-HTML format).\n\n```python\npip install -qU html2text\n"
  }
,
  {
    "path": "python/integrations/document_transformers/nuclia_transformer.mdx",
    "filename": "nuclia_transformer.mdx",
    "size_bytes": 1996,
    "line_count": 56,
    "preview": "---\ntitle: Nuclia\n---\n\n>[Nuclia](https://nuclia.com) automatically indexes your unstructured data from any internal and external source, providing optimized search results and generative answers. It can handle video and audio transcription, image content extraction, and document parsing.\n\n`Nuclia Understanding API` document transformer splits text into paragraphs and sentences, identifies entities, provides a summary of the text and generates embeddings for all the sentences.\n\nTo use the Nuclia Understanding API, you need to have a Nuclia account. You can create one for free at [https://nuclia.cloud](https://nuclia.cloud), and then [create a NUA key](https://docs.nuclia.dev/docs/docs/using/understanding/intro).\n\n"
  }
,
  {
    "path": "python/integrations/document_transformers/voyageai-reranker.mdx",
    "filename": "voyageai-reranker.mdx",
    "size_bytes": 15816,
    "line_count": 363,
    "preview": "---\ntitle: VoyageAI Reranker\n---\n\n>[Voyage AI](https://www.voyageai.com/) provides cutting-edge embedding/vectorizations models.\n\nThis notebook shows how to use [Voyage AI's rerank endpoint](https://api.voyageai.com/v1/rerank) in a retriever. This builds on top of ideas in the ContextualCompressionRetriever.\n\n```python\npip install -qU  voyageai\n"
  }
,
  {
    "path": "python/integrations/document_transformers/rankllm-reranker.mdx",
    "filename": "rankllm-reranker.mdx",
    "size_bytes": 31369,
    "line_count": 637,
    "preview": "---\ntitle: RankLLM Reranker\n---\n\n**[RankLLM](https://github.com/castorini/rank_llm)** is a **flexible reranking framework** supporting **listwise, pairwise, and pointwise ranking models**. It includes **RankVicuna, RankZephyr, MonoT5, DuoT5, LiT5, and FirstMistral**, with integration for **FastChat, vLLM, SGLang, and TensorRT-LLM** for efficient inference. RankLLM is optimized for **retrieval and ranking tasks**, leveraging both **open-source LLMs** and proprietary rerankers like **RankGPT and RankGemini**. It supports **batched inference, first-token reranking, and retrieval via BM25 and SPLADE**.\n\n> **Note:** If using the built-in retriever, RankLLM requires **Pyserini, JDK 21, PyTorch, and Faiss** for retrieval functionality.\n\n```python\npip install -qU rank_llm\n"
  }
,
  {
    "path": "python/integrations/document_transformers/localai_rerank.mdx",
    "filename": "localai_rerank.mdx",
    "size_bytes": 1668,
    "line_count": 39,
    "preview": "---\ntitle: LocalAI Reranker\n---\n\n<Info>\n**`langchain-localai` is a 3rd party integration package for LocalAI. It provides a simple way to use LocalAI services in LangChain.**\n\nThe source code is available on [GitHub](https://github.com/mkhludnev/langchain-localai)\n</Info>\n\n"
  }
,
  {
    "path": "python/integrations/document_transformers/dashscope_rerank.mdx",
    "filename": "dashscope_rerank.mdx",
    "size_bytes": 14707,
    "line_count": 310,
    "preview": "---\ntitle: DashScope Reranker\n---\n\nThis notebook shows how to use DashScope Reranker for document compression and retrieval. [DashScope](https://dashscope.aliyun.com/) is the generative AI service from Alibaba Cloud (Aliyun).\n\nDashScope's [Text ReRank Model](https://help.aliyun.com/document_detail/2780058.html?spm=a2c4g.2780059.0.0.6d995024FlrJ12) supports reranking documents with a maximum of 4000 tokens. Moreover, it supports Chinese, English, Japanese, Korean, Thai, Spanish, French, Portuguese, Indonesian, Arabic, and over 50 other languages. For more details, please visit [here](https://help.aliyun.com/document_detail/2780059.html?spm=a2c4g.2780058.0.0.3a9e5b1dWeOQjI).\n\n```python\npip install -qU  dashscope\n"
  }
,
  {
    "path": "python/integrations/document_transformers/jina_rerank.mdx",
    "filename": "jina_rerank.mdx",
    "size_bytes": 3377,
    "line_count": 124,
    "preview": "---\ntitle: Jina Reranker\n---\n\nThis notebook shows how to use Jina Reranker for document compression and retrieval.\n\n```python\npip install -qU langchain langchain-openai langchain-community langchain-text-splitters langchainhub\n\npip install -qU  faiss\n"
  }
,
  {
    "path": "python/integrations/document_transformers/infinity_rerank.mdx",
    "filename": "infinity_rerank.mdx",
    "size_bytes": 14755,
    "line_count": 327,
    "preview": "---\ntitle: Infinity Reranker\n---\n\n`Infinity` is a high-throughput, low-latency REST API for serving text-embeddings, reranking models and clip.\nFor more info, please visit [here](https://github.com/michaelfeil/infinity?tab=readme-ov-file#reranking).\n\nThis notebook shows how to use Infinity Reranker for document compression and retrieval.\n\nYou can launch an Infinity Server with a reranker model in CLI:\n"
  }
,
  {
    "path": "python/integrations/document_transformers/doctran_interrogate_document.mdx",
    "filename": "doctran_interrogate_document.mdx",
    "size_bytes": 10028,
    "line_count": 172,
    "preview": "---\ntitle: Doctran interrogate documents\n---\n\nDocuments used in a vector store knowledge base are typically stored in a narrative or conversational format. However, most user queries are in question format. If we **convert documents into Q&A format** before vectorizing them, we can increase the likelihood of retrieving relevant documents, and decrease the likelihood of retrieving irrelevant documents.\n\nWe can accomplish this using the [Doctran](https://github.com/psychic-api/doctran) library, which uses OpenAI's function calling feature to \"interrogate\" documents.\n\nSee [this notebook](https://github.com/psychic-api/doctran/blob/main/benchmark.ipynb) for benchmarks on vector similarity scores for various queries based on raw documents versus interrogated documents.\n\n"
  }
,
  {
    "path": "python/integrations/document_transformers/beautiful_soup.mdx",
    "filename": "beautiful_soup.mdx",
    "size_bytes": 2158,
    "line_count": 47,
    "preview": "---\ntitle: Beautiful Soup\n---\n\n>[Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/) is a Python package for parsing\n> HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup).\n> It creates a parse tree for parsed pages that can be used to extract data from HTML,[3] which\n> is useful for web scraping.\n\n`Beautiful Soup` offers fine-grained control over HTML content, enabling specific tag extraction, removal, and content cleaning.\n"
  }
,
  {
    "path": "python/integrations/document_transformers/google_translate.mdx",
    "filename": "google_translate.mdx",
    "size_bytes": 6160,
    "line_count": 138,
    "preview": "---\ntitle: Google Translate\n---\n\n[Google Translate](https://translate.google.com/) is a multilingual neural machine translation service developed by Google to translate text, documents and websites from one language into another.\n\nThe `GoogleTranslateTransformer` allows you to translate text and HTML with the [Google Cloud Translation API](https://cloud.google.com/translate).\n\nTo use it, you should have the `google-cloud-translate` python package installed, and a Google Cloud project with the [Translation API enabled](https://cloud.google.com/translate/docs/setup). This transformer uses the [Advanced edition (v3)](https://cloud.google.com/translate/docs/intro-to-v3).\n\n"
  }
,
  {
    "path": "python/integrations/document_transformers/ai21_semantic_text_splitter.mdx",
    "filename": "ai21_semantic_text_splitter.mdx",
    "size_bytes": 13675,
    "line_count": 243,
    "preview": "---\ntitle: AI21SemanticTextSplitter\n---\n\nThis example goes over how to use AI21SemanticTextSplitter in LangChain.\n\n## Installation\n\n```python\npip install langchain-ai21\n"
  }
,
  {
    "path": "python/integrations/document_transformers/doctran_translate_document.mdx",
    "filename": "doctran_translate_document.mdx",
    "size_bytes": 12268,
    "line_count": 173,
    "preview": "---\ntitle: Doctran language translation\n---\n\nComparing documents through embeddings has the benefit of working across multiple languages. \"Harrison says hello\" and \"Harrison dice hola\" will occupy similar positions in the vector space because they have the same meaning semantically.\n\nHowever, it can still be useful to use an LLM to **translate documents into other languages** before vectorizing them. This is especially helpful when users are expected to query the knowledge base in different languages, or when state-of-the-art embedding models are not available for a given language.\n\nWe can accomplish this using the [Doctran](https://github.com/psychic-api/doctran) library, which uses OpenAI's function calling feature to translate documents between languages.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/figma.mdx",
    "filename": "figma.mdx",
    "size_bytes": 4545,
    "line_count": 81,
    "preview": "---\ntitle: Figma\n---\n\n>[Figma](https://www.figma.com/) is a collaborative web application for interface design.\n\nThis notebook covers how to load data from the `Figma` REST API into a format that can be ingested into LangChain, along with example usage for code generation.\n\n```python\nimport os\n"
  }
,
  {
    "path": "python/integrations/document_loaders/subtitle.mdx",
    "filename": "subtitle.mdx",
    "size_bytes": 1305,
    "line_count": 35,
    "preview": "---\ntitle: Subtitle\n---\n\n>[The SubRip file format](https://en.wikipedia.org/wiki/SubRip#SubRip_file_format) is described on the `Matroska` multimedia container format website as \"perhaps the most basic of all subtitle formats.\" `SubRip (SubRip Text)` files are named with the extension `.srt`, and contain formatted lines of plain text in groups separated by a blank line. Subtitles are numbered sequentially, starting at 1. The timecode format used is hours:minutes:seconds,milliseconds with time units fixed to two zero-padded digits and fractions fixed to three zero-padded digits (00:00:00,000). The fractional separator used is the comma, since the program was written in France.\n\nHow to load data from subtitle (`.srt`) files\n\nPlease, download the [example .srt file from here](https://www.opensubtitles.org/en/subtitles/5575150/star-wars-the-clone-wars-crisis-at-the-heart-en).\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/airbyte_gong.mdx",
    "filename": "airbyte_gong.mdx",
    "size_bytes": 3299,
    "line_count": 90,
    "preview": "---\ntitle: Airbyte Gong (Deprecated)\n---\n\nNote: This connector-specific loader is deprecated. Please use [`AirbyteLoader`](/oss/integrations/document_loaders/airbyte) instead.\n\n>[Airbyte](https://github.com/airbytehq/airbyte) is a data integration platform for ELT pipelines from APIs, databases & files to warehouses & lakes. It has the largest catalog of ELT connectors to data warehouses and databases.\n\nThis loader exposes the Gong connector as a document loader, allowing you to load various Gong objects as documents.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/agentql.mdx",
    "filename": "agentql.mdx",
    "size_bytes": 5777,
    "line_count": 117,
    "preview": "---\ntitle: AgentQLLoader\n---\n\n[AgentQL](https://www.agentql.com/)'s document loader provides structured data extraction from any web page using an [AgentQL query](https://docs.agentql.com/agentql-query). AgentQL can be used across multiple languages and web pages without breaking over time and change.\n\n## Overview\n\n`AgentQLLoader` requires the following two parameters:\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/cube_semantic.mdx",
    "filename": "cube_semantic.mdx",
    "size_bytes": 3135,
    "line_count": 84,
    "preview": "---\ntitle: Cube Semantic Layer\n---\n\nThis notebook demonstrates the process of retrieving Cube's data model metadata in a format suitable for passing to LLMs as embeddings, thereby enhancing contextual information.\n\n### About cube\n\n[Cube](https://cube.dev/) is the Semantic Layer for building data apps. It helps data engineers and application developers access data from modern data stores, organize it into consistent definitions, and deliver it to every application.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/weather.mdx",
    "filename": "weather.mdx",
    "size_bytes": 927,
    "line_count": 36,
    "preview": "---\ntitle: Weather\n---\n\n>[OpenWeatherMap](https://openweathermap.org/) is an open-source weather service provider\n\nThis loader fetches the weather data from the OpenWeatherMap's OneCall API, using the pyowm Python package. You must initialize the loader with your OpenWeatherMap API token and the names of the cities you want the weather data for.\n\n```python\nfrom langchain_community.document_loaders import WeatherDataLoader\n"
  }
,
  {
    "path": "python/integrations/document_loaders/async_chromium.mdx",
    "filename": "async_chromium.mdx",
    "size_bytes": 2078,
    "line_count": 52,
    "preview": "---\ntitle: Async Chromium\n---\n\nChromium is one of the browsers supported by Playwright, a library used to control browser automation.\n\nBy running `p.chromium.launch(headless=True)`, we are launching a headless instance of Chromium.\n\nHeadless mode means that the browser is running without a graphical user interface.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/microsoft_word.mdx",
    "filename": "microsoft_word.mdx",
    "size_bytes": 3548,
    "line_count": 92,
    "preview": "---\ntitle: Microsoft Word\n---\n\n>[Microsoft Word](https://www.microsoft.com/en-us/microsoft-365/word) is a word processor developed by Microsoft.\n\nThis covers how to load `Word` documents into a document format that we can use downstream.\n\n## Using Docx2txt\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/college_confidential.mdx",
    "filename": "college_confidential.mdx",
    "size_bytes": 10811,
    "line_count": 33,
    "preview": "---\ntitle: College Confidential\n---\n\n>[College Confidential](https://www.collegeconfidential.com/) gives information on 3,800+ colleges and universities.\n\nThis covers how to load `College Confidential` webpages into a document format that we can use downstream.\n\n```python\nfrom langchain_community.document_loaders import CollegeConfidentialLoader\n"
  }
,
  {
    "path": "python/integrations/document_loaders/lakefs.mdx",
    "filename": "lakefs.mdx",
    "size_bytes": 1126,
    "line_count": 46,
    "preview": "---\ntitle: lakeFS\n---\n\n>[lakeFS](https://docs.lakefs.io/) provides scalable version control over the data lake, and uses Git-like semantics to create and access those versions.\n\nThis notebooks covers how to load document objects from a `lakeFS` path (whether it's an object or a prefix).\n\n## Initializing the lakeFS loader\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/pymupdf4llm.mdx",
    "filename": "pymupdf4llm.mdx",
    "size_bytes": 13199,
    "line_count": 399,
    "preview": "---\ntitle: PyMuPDF4LLMLoader\n---\n\nThis guide provides a quick overview for getting started with PyMuPDF4LLM [document loader](https://python.langchain.com/docs/concepts/#document-loaders). For detailed documentation of all PyMuPDF4LLMLoader features and configurations head to the [GitHub repository](https://github.com/lakinduboteju/langchain-pymupdf4llm).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/fauna.mdx",
    "filename": "fauna.mdx",
    "size_bytes": 992,
    "line_count": 41,
    "preview": "---\ntitle: Fauna\n---\n\n>[Fauna](https://fauna.com/) is a Document Database.\n\nQuery `Fauna` documents\n\n```python\npip install -qU  fauna\n"
  }
,
  {
    "path": "python/integrations/document_loaders/oracleai.mdx",
    "filename": "oracleai.mdx",
    "size_bytes": 9449,
    "line_count": 175,
    "preview": "---\ntitle: Oracle AI Vector Search Document Processing\n---\n\nOracle AI Vector Search is designed for Artificial Intelligence (AI) workloads that allows you to query data based on semantics, rather than keywords.\nOne of the biggest benefits of Oracle AI Vector Search is that semantic search on unstructured data can be combined with relational search on business data in one single system.\nThis is not only powerful but also significantly more effective because you don't need to add a specialized vector database, eliminating the pain of data fragmentation between multiple systems.\n\nIn addition, your vectors can benefit from all of Oracle Database’s most powerful features, like the following:\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/apify_dataset.mdx",
    "filename": "apify_dataset.mdx",
    "size_bytes": 5060,
    "line_count": 138,
    "preview": "---\ntitle: Apify Dataset\n---\n\n> [Apify Dataset](https://docs.apify.com/platform/storage/dataset) is a scalable append-only storage with sequential access built for storing structured web scraping results, such as a list of products or Google SERPs, and then export them to various formats like JSON, CSV, or Excel. Datasets are mainly used to save results of [Apify Actors](https://apify.com/store)—serverless cloud programs for various web scraping, crawling, and data extraction use cases.\n\nThis notebook shows how to load Apify datasets to LangChain.\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/google_drive.mdx",
    "filename": "google_drive.mdx",
    "size_bytes": 12070,
    "line_count": 348,
    "preview": "---\ntitle: Google Drive\n---\n\n>[Google Drive](https://en.wikipedia.org/wiki/Google_Drive) is a file storage and synchronization service developed by Google.\n\nThis notebook covers how to load documents from `Google Drive`. Currently, only `Google Docs` are supported.\n\n## Prerequisites\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/google_memorystore_redis.mdx",
    "filename": "google_memorystore_redis.mdx",
    "size_bytes": 6887,
    "line_count": 173,
    "preview": "---\ntitle: Google Memorystore for Redis\n---\n\n> [Google Memorystore for Redis](https://cloud.google.com/memorystore/docs/redis/memorystore-for-redis-overview) is a fully-managed service that is powered by the Redis in-memory data store to build application caches that provide sub-millisecond data access. Extend your database application to build AI-powered experiences leveraging Memorystore for Redis's LangChain integrations.\n\nThis notebook goes over how to use [Memorystore for Redis](https://cloud.google.com/memorystore/docs/redis/memorystore-for-redis-overview) to [save, load and delete langchain documents](/oss/integrations/document_loaders) with `MemorystoreDocumentLoader` and `MemorystoreDocumentSaver`.\n\nLearn more about the package on [GitHub](https://github.com/googleapis/langchain-google-memorystore-redis-python/).\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/undatasio.mdx",
    "filename": "undatasio.mdx",
    "size_bytes": 3022,
    "line_count": 110,
    "preview": "---\ntitle: UnDatasIO\n---\n\nThis notebook provides a quick overview for getting started with the __UnDatasIO document loader__. UnDatasIO enables efficient loading and parsing of various document formats including PDF, PNG, JPG, JPEG, and JFIF, with features like document lazy loading and native async support, all through UnDatasIO's secure cloud API. These capabilities make the processed data ready for generative AI workflows like RAG.\n\nFor detailed documentation on all features and configurations, refer to the official API reference.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/firecrawl.mdx",
    "filename": "firecrawl.mdx",
    "size_bytes": 23397,
    "line_count": 113,
    "preview": "---\ntitle: FireCrawl\n---\n\n[FireCrawl](https://firecrawl.dev/?ref=langchain) crawls and convert any website into LLM-ready data. It crawls all accessible subpages and give you clean markdown and metadata for each. No sitemap required.\n\nFireCrawl handles complex tasks such as reverse proxies, caching, rate limits, and content blocked by JavaScript. Built by the [mendable.ai](https://mendable.ai) team.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/google_el_carro.mdx",
    "filename": "google_el_carro.mdx",
    "size_bytes": 12599,
    "line_count": 382,
    "preview": "---\ntitle: Google El Carro for Oracle Workloads\n---\n\n> Google [El Carro Oracle Operator](https://github.com/GoogleCloudPlatform/elcarro-oracle-operator)\noffers a way to run Oracle databases in Kubernetes as a portable, open source,\ncommunity driven, no vendor lock-in container orchestration system. El Carro\nprovides a powerful declarative API for comprehensive and consistent\nconfiguration and deployment as well as for real-time operations and\nmonitoring.\n"
  }
,
  {
    "path": "python/integrations/document_loaders/airbyte_json.mdx",
    "filename": "airbyte_json.mdx",
    "size_bytes": 2171,
    "line_count": 102,
    "preview": "---\ntitle: Airbyte JSON (Deprecated)\n---\n\nNote: `AirbyteJSONLoader` is deprecated. Please use [`AirbyteLoader`](/oss/integrations/document_loaders/airbyte) instead.\n\n>[Airbyte](https://github.com/airbytehq/airbyte) is a data integration platform for ELT pipelines from APIs, databases & files to warehouses & lakes. It has the largest catalog of ELT connectors to data warehouses and databases.\n\nThis covers how to load any source from Airbyte into a local JSON file that can be read in as a document\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/pandas_dataframe.mdx",
    "filename": "pandas_dataframe.mdx",
    "size_bytes": 6842,
    "line_count": 165,
    "preview": "---\ntitle: Pandas DataFrame\n---\n\nThis notebook goes over how to load data from a [pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/index) DataFrame.\n\n```python\npip install -qU  pandas\n```\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/dropbox.mdx",
    "filename": "dropbox.mdx",
    "size_bytes": 1897,
    "line_count": 57,
    "preview": "---\ntitle: Dropbox\n---\n\n[Dropbox](https://en.wikipedia.org/wiki/Dropbox) is a file hosting service that brings everything-traditional files, cloud content, and web shortcuts together in one place.\n\nThis notebook covers how to load documents from *Dropbox*. In addition to common files such as text and PDF files, it also supports *Dropbox Paper* files.\n\n## Prerequisites\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/microsoft_powerpoint.mdx",
    "filename": "microsoft_powerpoint.mdx",
    "size_bytes": 3544,
    "line_count": 79,
    "preview": "---\ntitle: Microsoft PowerPoint\n---\n\n>[Microsoft PowerPoint](https://en.wikipedia.org/wiki/Microsoft_PowerPoint) is a presentation program by Microsoft.\n\nThis covers how to load `Microsoft PowerPoint` documents into a document format that we can use downstream.\n\nPlease see [this guide](/oss/integrations/providers/unstructured/) for more instructions on setting up Unstructured locally, including setting up required system dependencies.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/microsoft_sharepoint.mdx",
    "filename": "microsoft_sharepoint.mdx",
    "size_bytes": 9868,
    "line_count": 162,
    "preview": "---\ntitle: Microsoft SharePoint\n---\n\n> [Microsoft SharePoint](https://en.wikipedia.org/wiki/SharePoint) is a website-based collaboration system that uses workflow applications, “list” databases, and other web parts and security features to empower business teams to work together developed by Microsoft.\n\nThis notebook covers how to load documents from the [SharePoint Document Library](https://support.microsoft.com/en-us/office/what-is-a-document-library-3b5976dd-65cf-4c9e-bf5a-713c10ca2872). By default the document loader loads `pdf`, `doc`, `docx` and `txt` files. You can load other file types by providing appropriate parsers (see more below).\n\n## Prerequisites\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/readthedocs_documentation.mdx",
    "filename": "readthedocs_documentation.mdx",
    "size_bytes": 867,
    "line_count": 31,
    "preview": "---\ntitle: ReadTheDocs Documentation\n---\n\n>[Read the Docs](https://readthedocs.org/) is an open-sourced free software documentation hosting platform. It generates documentation written with the `Sphinx` documentation generator.\n\nThis notebook covers how to load content from HTML that was generated as part of a `Read-The-Docs` build.\n\nFor an example of this in the wild, see [here](https://github.com/langchain-ai/chat-langchain).\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/xml.mdx",
    "filename": "xml.mdx",
    "size_bytes": 2878,
    "line_count": 92,
    "preview": "---\ntitle: UnstructuredXMLLoader\n---\n\nThis guide provides a quick overview for getting started with UnstructuredXMLLoader [document loader](https://python.langchain.com/docs/concepts/document_loaders). The `UnstructuredXMLLoader` is used to load `XML` files. The loader works with `.xml` files. The page content will be the text extracted from the XML tags.\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/parsers/azure_openai_whisper_parser.mdx",
    "filename": "azure_openai_whisper_parser.mdx",
    "size_bytes": 2473,
    "line_count": 83,
    "preview": "---\ntitle: Azure OpenAI Whisper Parser\n---\n\n>[Azure OpenAI Whisper Parser](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/whisper-overview) is a wrapper around the Azure OpenAI Whisper API which utilizes machine learning to transcribe audio files to english text.\n>\n>The Parser supports `.mp3`, `.mp4`, `.mpeg`, `.mpga`, `.m4a`, `.wav`, and `.webm`.\n\nThe current implementation follows LangChain core principles and can be used with other loaders to handle both audio downloading and parsing. As a result of this the parser will `yield` an `Iterator[Document]`.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/parsers/writer_pdf_parser.mdx",
    "filename": "writer_pdf_parser.mdx",
    "size_bytes": 4607,
    "line_count": 94,
    "preview": "---\ntitle: WRITER PDF Parser\n---\n\nThis guide provides a quick overview for getting started with the WRITER `PDFParser` [document loader](/oss/integrations/document_loaders/).\n\nWRITER's [PDF Parser](https://dev.writer.com/api-guides/api-reference/tool-api/pdf-parser#parse-pdf) converts PDF documents into other formats like text or Markdown. This is particularly useful when you need to extract and process text content from PDF files for further analysis or integration into your workflow. In `langchain-writer`, we provide usage of WRITER's PDF Parser as a LangChain document parser.\n\n<Warning>\n**Deprecation notice**: The parse PDF tool is deprecated and will be removed on **December 22, 2025**.\n"
  }
,
  {
    "path": "python/integrations/document_loaders/oracleadb_loader.mdx",
    "filename": "oracleadb_loader.mdx",
    "size_bytes": 2715,
    "line_count": 82,
    "preview": "---\ntitle: Oracle Autonomous Database\n---\n\nOracle Autonomous Database is a cloud database that uses machine learning to automate database tuning, security, backups, updates, and other routine management tasks traditionally performed by DBAs.\n\nThis notebook covers how to load documents from Oracle Autonomous Database.\n\n## Prerequisites\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/bilibili.mdx",
    "filename": "bilibili.mdx",
    "size_bytes": 14726,
    "line_count": 50,
    "preview": "---\ntitle: BiliBili\n---\n\n>[Bilibili](https://www.bilibili.com/) is one of the most beloved long-form video sites in China.\n\nThis loader leverages the [bilibili-api](https://github.com/Nemo2011/bilibili-api) to retrieve text transcripts from `Bilibili` videos. To effectively use this loader, it's essential to have the `sessdata`, `bili_jct`, and `buvid3` cookie parameters. These can be obtained by logging into [Bilibili](https://www.bilibili.com/), then extracting the values of `sessdata`, `bili_jct`, and `buvid3` from the browser's developer tools.\n\nIf you choose to leave the cookie parameters blank, the Loader will still function, but it will only retrieve video information for the metadata and will not be able to fetch transcripts.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/opendataloader_pdf.mdx",
    "filename": "opendataloader_pdf.mdx",
    "size_bytes": 4680,
    "line_count": 139,
    "preview": "---\ntitle: OpenDataLoader PDF\n---\n\n**PDF Parsing for RAG** — Convert to Markdown & JSON, Fast, Local, No GPU\n\n[OpenDataLoader PDF](https://github.com/opendataloader-project/opendataloader-pdf) converts PDFs into **LLM-ready Markdown and JSON** with accurate reading order, table extraction, and bounding boxes — all running locally on your machine.\n\n**Why developers choose OpenDataLoader:**\n- **Deterministic** — Same input always produces same output (no LLM hallucinations)\n"
  }
,
  {
    "path": "python/integrations/document_loaders/llmsherpa.mdx",
    "filename": "llmsherpa.mdx",
    "size_bytes": 6332,
    "line_count": 155,
    "preview": "---\ntitle: LLM Sherpa\n---\n\nThis notebook covers how to use `LLM Sherpa` to load files of many types. `LLM Sherpa` supports different file formats including DOCX, PPTX, HTML, TXT, and XML.\n\n`LLMSherpaFileLoader` use LayoutPDFReader, which is part of the LLMSherpa library. This tool is designed to parse PDFs while preserving their layout information, which is often lost when using most PDF to text parsers.\n\nHere are some key features of LayoutPDFReader:\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/diffbot.mdx",
    "filename": "diffbot.mdx",
    "size_bytes": 6343,
    "line_count": 74,
    "preview": "---\ntitle: Diffbot\n---\n\n>[Diffbot](https://docs.diffbot.com/docs/getting-started-with-diffbot) is a suite of ML-based products that make it easy to structure web data.\n\n>Diffbot's [Extract API](https://docs.diffbot.com/reference/extract-introduction) is a service that structures and normalizes data from web pages.\n\n>Unlike traditional web scraping tools, `Diffbot Extract` doesn't require any rules to read the content on a page. It uses a computer vision model to classify a page into one of 20 possible types, and then transforms raw HTML markup into JSON. The resulting structured JSON follows a consistent [type-based ontology](https://docs.diffbot.com/docs/ontology), which makes it easy to extract data from multiple different web sources with the same schema.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/merge_doc.mdx",
    "filename": "merge_doc.mdx",
    "size_bytes": 669,
    "line_count": 37,
    "preview": "---\ntitle: Merge Documents Loader\n---\n\nMerge the documents returned from a set of specified data loaders.\n\n```python\nfrom langchain_community.document_loaders import WebBaseLoader\n\nloader_web = WebBaseLoader(\n"
  }
,
  {
    "path": "python/integrations/document_loaders/modern_treasury.mdx",
    "filename": "modern_treasury.mdx",
    "size_bytes": 2556,
    "line_count": 60,
    "preview": "---\ntitle: Modern Treasury\n---\n\n>[Modern Treasury](https://www.moderntreasury.com/) simplifies complex payment operations. It is a unified platform to power products and processes that move money.\n>\n>- Connect to banks and payment systems\n>- Track transactions and balances in real-time\n>- Automate payment operations for scale\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/google_alloydb.mdx",
    "filename": "google_alloydb.mdx",
    "size_bytes": 6633,
    "line_count": 165,
    "preview": "---\ntitle: Google AlloyDB for PostgreSQL\n---\n\n> [AlloyDB](https://cloud.google.com/alloydb) is a fully managed relational database service that offers high performance, seamless integration, and impressive scalability. AlloyDB is 100% compatible with PostgreSQL. Extend your database application to build AI-powered experiences leveraging AlloyDB's LangChain integrations.\n\nThis notebook goes over how to use `AlloyDB for PostgreSQL` to load Documents with the `AlloyDBLoader` class.\n\nLearn more about the package on [GitHub](https://github.com/googleapis/langchain-google-alloydb-pg-python/).\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/github.mdx",
    "filename": "github.mdx",
    "size_bytes": 3000,
    "line_count": 109,
    "preview": "---\ntitle: GitHub\n---\n\nThis notebooks shows how you can load issues and pull requests (PRs) for a given repository on [GitHub](https://github.com/). Also shows how you can load github files for a given repository on [GitHub](https://github.com/). We will use the LangChain Python repository as an example.\n\n## Setup access token\n\nTo access the GitHub API, you need a personal access token - you can set up yours here: [github.com/settings/tokens?type=beta](https://github.com/settings/tokens?type=beta). You can either set this token as the environment variable `GITHUB_PERSONAL_ACCESS_TOKEN` and it will be automatically pulled in, or you can pass it in directly at initialization as the `access_token` named parameter.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/facebook_chat.mdx",
    "filename": "facebook_chat.mdx",
    "size_bytes": 1493,
    "line_count": 27,
    "preview": "---\ntitle: Facebook Chat\n---\n\n>[Messenger](https://en.wikipedia.org/wiki/Messenger_(software)) is an American proprietary instant messaging app and platform developed by `Meta Platforms`. Originally developed as `Facebook Chat` in 2008, the company revamped its messaging service in 2010.\n\nThis notebook covers how to load data from the [Facebook Chats](https://www.facebook.com/business/help/1646890868956360) into a format that can be ingested into LangChain.\n\n```python\n# pip install pandas\n"
  }
,
  {
    "path": "python/integrations/document_loaders/docugami.mdx",
    "filename": "docugami.mdx",
    "size_bytes": 65263,
    "line_count": 405,
    "preview": "---\ntitle: Docugami\n---\n\nThis notebook covers how to load documents from `Docugami`. It provides the advantages of using this system over alternative data loaders.\n\n## Prerequisites\n\n1. Install necessary python packages.\n2. Grab an access token for your workspace, and make sure it is set as the `DOCUGAMI_API_KEY` environment variable.\n"
  }
,
  {
    "path": "python/integrations/document_loaders/azure_ai_data.mdx",
    "filename": "azure_ai_data.mdx",
    "size_bytes": 2238,
    "line_count": 67,
    "preview": "---\ntitle: Azure AI Data\n---\n\n>[Azure AI Foundry (formerly Azure AI Studio](https://ai.azure.com/) provides the capability to upload data assets to cloud storage and register existing data assets from the following sources:\n>\n>- `Microsoft OneLake`\n>- `Azure Blob Storage`\n>- `Azure Data Lake gen 2`\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/gutenberg.mdx",
    "filename": "gutenberg.mdx",
    "size_bytes": 940,
    "line_count": 35,
    "preview": "---\ntitle: Gutenberg\n---\n\n>[Project Gutenberg](https://www.gutenberg.org/about/) is an online library of free eBooks.\n\nThis notebook covers how to load links to `Gutenberg` e-books into a document format that we can use downstream.\n\n```python\nfrom langchain_community.document_loaders import GutenbergLoader\n"
  }
,
  {
    "path": "python/integrations/document_loaders/airtable.mdx",
    "filename": "airtable.mdx",
    "size_bytes": 1037,
    "line_count": 49,
    "preview": "---\ntitle: Airtable\n---\n\n```python\npip install -qU  pyairtable\n```\n\n```python\nfrom langchain_community.document_loaders import AirtableLoader\n"
  }
,
  {
    "path": "python/integrations/document_loaders/unstructured_markdown.mdx",
    "filename": "unstructured_markdown.mdx",
    "size_bytes": 5162,
    "line_count": 132,
    "preview": "---\ntitle: UnstructuredMarkdownLoader\n---\n\nThis guide provides a quick overview for getting started with UnstructuredMarkdown [document loader](https://python.langchain.com/docs/concepts/document_loaders). For detailed documentation of all __ModuleName__Loader features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.markdown.UnstructuredMarkdownLoader.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/concurrent.mdx",
    "filename": "concurrent.mdx",
    "size_bytes": 400,
    "line_count": 29,
    "preview": "---\ntitle: Concurrent Loader\n---\n\nWorks just like the GenericLoader but concurrently for those who choose to optimize their workflow.\n\n```python\nfrom langchain_community.document_loaders import ConcurrentLoader\n```\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/azure_blob_storage.mdx",
    "filename": "azure_blob_storage.mdx",
    "size_bytes": 7043,
    "line_count": 144,
    "preview": "---\ntitle: Azure Blob Storage Loader\n---\n\n>[Azure Blob Storage](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction) is Microsoft's object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data. Unstructured data is data that doesn't adhere to a particular data model or definition, such as text or binary data.\n\n`Azure Blob Storage` is designed for:\n\n- Serving images or documents directly to a browser.\n- Storing files for distributed access.\n"
  }
,
  {
    "path": "python/integrations/document_loaders/hyperbrowser.mdx",
    "filename": "hyperbrowser.mdx",
    "size_bytes": 4172,
    "line_count": 116,
    "preview": "---\ntitle: HyperbrowserLoader\n---\n\n[Hyperbrowser](https://hyperbrowser.ai) is a platform for running and scaling headless browsers. It lets you launch and manage browser sessions at scale and provides easy to use solutions for any webscraping needs, such as scraping a single page or crawling an entire site.\n\nKey Features:\n\n- Instant Scalability - Spin up hundreds of browser sessions in seconds without infrastructure headaches\n- Simple Integration - Works seamlessly with popular tools like Puppeteer and Playwright\n"
  }
,
  {
    "path": "python/integrations/document_loaders/huawei_obs_file.mdx",
    "filename": "huawei_obs_file.mdx",
    "size_bytes": 1955,
    "line_count": 76,
    "preview": "---\ntitle: Huawei OBS File\n---\n\nThe following code demonstrates how to load an object from the Huawei OBS (Object Storage Service) as document.\n\n```python\n# Install the required package\n# pip install esdk-obs-python\n```\n"
  }
,
  {
    "path": "python/integrations/document_loaders/polars_dataframe.mdx",
    "filename": "polars_dataframe.mdx",
    "size_bytes": 6539,
    "line_count": 115,
    "preview": "---\ntitle: Polars DataFrame\n---\n\nThis notebook goes over how to load data from a [polars](https://pola-rs.github.io/polars-book/user-guide/) DataFrame.\n\n```python\npip install -qU  polars\n```\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/wikipedia.mdx",
    "filename": "wikipedia.mdx",
    "size_bytes": 4257,
    "line_count": 58,
    "preview": "---\ntitle: Wikipedia\n---\n\n>[Wikipedia](https://wikipedia.org/) is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. `Wikipedia` is the largest and most-read reference work in history.\n\nThis notebook shows how to load wiki pages from `wikipedia.org` into the Document format that we use downstream.\n\n## Installation\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/copypaste.mdx",
    "filename": "copypaste.mdx",
    "size_bytes": 693,
    "line_count": 33,
    "preview": "---\ntitle: Copy Paste\n---\n\nThis notebook covers how to load a document object from something you just want to copy and paste. In this case, you don't even need to use a DocumentLoader, but rather can just construct the Document directly.\n\n```python\nfrom langchain_core.documents import Document\n```\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/confluence.mdx",
    "filename": "confluence.mdx",
    "size_bytes": 3838,
    "line_count": 104,
    "preview": "---\ntitle: Confluence\n---\n\n[Confluence](https://www.atlassian.com/software/confluence) is a wiki collaboration platform designed to save and organize all project-related materials. As a knowledge base, Confluence primarily serves content management activities.\n\nThis loader allows you to fetch and process Confluence pages into @[`Document`] objects.\n\n---\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/google_cloud_sql_pg.mdx",
    "filename": "google_cloud_sql_pg.mdx",
    "size_bytes": 6898,
    "line_count": 167,
    "preview": "---\ntitle: Google Cloud SQL for PostgreSQL\n---\n\n> [Cloud SQL for PostgreSQL](https://cloud.google.com/sql/docs/postgres) is a fully-managed database service that helps you set up, maintain, manage, and administer your PostgreSQL relational databases on Google Cloud Platform. Extend your database application to build AI-powered experiences leveraging Cloud SQL for PostgreSQL's LangChain integrations.\n\nThis notebook goes over how to use `Cloud SQL for PostgreSQL` to load Documents with the `PostgresLoader` class.\n\nLearn more about the package on [GitHub](https://github.com/googleapis/langchain-google-cloud-sql-pg-python/).\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/tencent_cos_directory.mdx",
    "filename": "tencent_cos_directory.mdx",
    "size_bytes": 1391,
    "line_count": 47,
    "preview": "---\ntitle: Tencent COS Directory\n---\n\n>[Tencent Cloud Object Storage (COS)](https://www.tencentcloud.com/products/cos) is a distributed\n> storage service that enables you to store any amount of data from anywhere via HTTP/HTTPS protocols.\n> `COS` has no restrictions on data structure or format. It also has no bucket size limit and\n> partition management, making it suitable for virtually any use case, such as data delivery,\n> data processing, and data lakes. `COS` provides a web-based console, multi-language SDKs and APIs,\n> command line tool, and graphical tools. It works well with Amazon S3 APIs, allowing you to quickly\n"
  }
,
  {
    "path": "python/integrations/document_loaders/mastodon.mdx",
    "filename": "mastodon.mdx",
    "size_bytes": 2264,
    "line_count": 54,
    "preview": "---\ntitle: Mastodon\n---\n\n>[Mastodon](https://joinmastodon.org/) is a federated social media and social networking service.\n\nThis loader fetches the text from the \"toots\" of a list of `Mastodon` accounts, using the `Mastodon.py` Python package.\n\nPublic accounts can the queried by default without any authentication. If non-public accounts or instances are queried, you have to register an application for your account which gets you an access token, and set that token and your account's API base URL.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/grobid.mdx",
    "filename": "grobid.mdx",
    "size_bytes": 3363,
    "line_count": 56,
    "preview": "---\ntitle: Grobid\n---\n\nGROBID is a machine learning library for extracting, parsing, and re-structuring raw documents.\n\nIt is designed and expected to be used to parse academic papers, where it works particularly well. Note: if the articles supplied to Grobid are large documents (e.g. dissertations) exceeding a certain number of elements, they might not be processed.\n\nThis loader uses Grobid to parse PDFs into `Documents` that retain metadata associated with the section of text.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/huawei_obs_directory.mdx",
    "filename": "huawei_obs_directory.mdx",
    "size_bytes": 1662,
    "line_count": 67,
    "preview": "---\ntitle: Huawei OBS Directory\n---\n\nThe following code demonstrates how to load objects from the Huawei OBS (Object Storage Service) as documents.\n\n```python\n# Install the required package\n# pip install esdk-obs-python\n```\n"
  }
,
  {
    "path": "python/integrations/document_loaders/glue_catalog.mdx",
    "filename": "glue_catalog.mdx",
    "size_bytes": 2220,
    "line_count": 54,
    "preview": "---\ntitle: Glue Catalog\n---\n\nThe [AWS Glue Data Catalog](https://docs.aws.amazon.com/en_en/glue/latest/dg/catalog-and-crawler.html) is a centralized metadata repository that allows you to manage, access, and share metadata about your data stored in AWS. It acts as a metadata store for your data assets, enabling various AWS services and your applications to query and connect to the data they need efficiently.\n\nWhen you define data sources, transformations, and targets in AWS Glue, the metadata about these elements is stored in the Data Catalog. This includes information about data locations, schema definitions, runtime metrics, and more. It supports various data store types, such as Amazon S3, Amazon RDS, Amazon Redshift, and external databases compatible with JDBC. It is also directly integrated with Amazon Athena, Amazon Redshift Spectrum, and Amazon EMR, allowing these services to directly access and query the data.\n\nThe LangChain GlueCatalogLoader will get the schema of all tables inside the given Glue database in the same format as Pandas dtype.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/json.mdx",
    "filename": "json.mdx",
    "size_bytes": 6593,
    "line_count": 197,
    "preview": "---\ntitle: JSONLoader\n---\n\nThis guide provides a quick overview for getting started with JSON [document loader](https://python.langchain.com/docs/concepts/document_loaders). For detailed documentation of all JSONLoader features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.json_loader.JSONLoader.html).\n\n- TODO: Add any other relevant links, like information about underlying API, etc.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 36310,
    "line_count": 334,
    "preview": "---\nsidebar_position: 0\nsidebarTitle: \"Document loaders\"\n---\n\nDocument loaders provide a **standard interface** for reading data from different sources (such as Slack, Notion, or Google Drive) into LangChain’s @[Document] format.\nThis ensures that data can be handled consistently regardless of the source.\n\nAll document loaders implement the @[`BaseLoader`] interface.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/obsidian.mdx",
    "filename": "obsidian.mdx",
    "size_bytes": 843,
    "line_count": 24,
    "preview": "---\ntitle: Obsidian\n---\n\n>[Obsidian](https://obsidian.md/) is a powerful and extensible knowledge base\nthat works on top of your local folder of plain text files.\n\nThis notebook covers how to load documents from an `Obsidian` database.\n\nSince `Obsidian` is just stored on disk as a folder of Markdown files, the loader just takes a path to this directory.\n"
  }
,
  {
    "path": "python/integrations/document_loaders/image_captions.mdx",
    "filename": "image_captions.mdx",
    "size_bytes": 3432548,
    "line_count": 115,
    "preview": "---\ntitle: Image captions\n---\n\nBy default, the loader utilizes the pre-trained [Salesforce BLIP image captioning model](https://huggingface.co/Salesforce/blip-image-captioning-base).\n\nThis notebook shows how to use the `ImageCaptionLoader` to generate a queryable index of image captions.\n\n```python\npip install -qU transformers langchain_openai langchain_chroma\n"
  }
,
  {
    "path": "python/integrations/document_loaders/larksuite.mdx",
    "filename": "larksuite.mdx",
    "size_bytes": 1805,
    "line_count": 62,
    "preview": "---\ntitle: LarkSuite (FeiShu)\n---\n\n>[LarkSuite](https://www.larksuite.com/) is an enterprise collaboration platform developed by ByteDance.\n\nThis notebook covers how to load data from the `LarkSuite` REST API into a format that can be ingested into LangChain, along with example usage for text summarization.\n\nThe LarkSuite API requires an access token (tenant_access_token or user_access_token), checkout [LarkSuite open platform document](https://open.larksuite.com/document) for API details.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/pypdfdirectory.mdx",
    "filename": "pypdfdirectory.mdx",
    "size_bytes": 5389,
    "line_count": 91,
    "preview": "---\ntitle: PyPDFDirectoryLoader\n---\n\nThis loader loads all PDF files from a specific directory.\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/browserbase.mdx",
    "filename": "browserbase.mdx",
    "size_bytes": 2104,
    "line_count": 60,
    "preview": "---\ntitle: Browserbase\n---\n\n[Browserbase](https://browserbase.com) is a developer platform to reliably run, manage, and monitor headless browsers.\n\nPower your AI data retrievals with:\n\n- [Serverless Infrastructure](https://docs.browserbase.com/under-the-hood) providing reliable browsers to extract data from complex UIs\n- [Stealth Mode](https://docs.browserbase.com/features/stealth-mode) with included fingerprinting tactics and automatic captcha solving\n"
  }
,
  {
    "path": "python/integrations/document_loaders/rst.mdx",
    "filename": "rst.mdx",
    "size_bytes": 857,
    "line_count": 26,
    "preview": "---\ntitle: RST\n---\n\n>A [reStructured Text (RST)](https://en.wikipedia.org/wiki/ReStructuredText) file is a file format for textual data used primarily in the Python programming language community for technical documentation.\n\n## `UnstructuredRSTLoader`\n\nYou can load data from RST files with `UnstructuredRSTLoader` using the following workflow.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/example_data/example.mdx",
    "filename": "example.mdx",
    "size_bytes": 857,
    "line_count": 55,
    "preview": "# Sample markdown document\n\n## Introduction\n\nWelcome to this sample Markdown document. Markdown is a lightweight markup language used for formatting text. It's widely used for documentation, readme files, and more.\n\n## Features\n\n### Headers\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/needle.mdx",
    "filename": "needle.mdx",
    "size_bytes": 11074,
    "line_count": 147,
    "preview": "---\ntitle: Needle Document Loader\n---\n\n[Needle](https://needle-ai.com) makes it easy to create your RAG pipelines with minimal effort.\n\nFor more details, refer to our [API documentation](https://docs.needle-ai.com/docs/api-reference/needle-api)\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/zeroxpdfloader.mdx",
    "filename": "zeroxpdfloader.mdx",
    "size_bytes": 9926,
    "line_count": 195,
    "preview": "---\ntitle: ZeroxPDFLoader\n---\n\n`ZeroxPDFLoader` is a document loader that leverages the [Zerox](https://github.com/getomni-ai/zerox) library. Zerox converts PDF documents into images, processes them using a vision-capable language model, and generates a structured Markdown representation. This loader allows for asynchronous operations and provides page-level document extraction.\n\n### Integration details\n\n| Class | Package | Local | Serializable | JS support|\n| :--- | :--- | :---: | :---: |  :---: |\n"
  }
,
  {
    "path": "python/integrations/document_loaders/browserless.mdx",
    "filename": "browserless.mdx",
    "size_bytes": 1934,
    "line_count": 54,
    "preview": "---\ntitle: Browserless\n---\n\nBrowserless is a service that allows you to run headless Chrome instances in the cloud. It's a great way to run browser-based automation at scale without having to worry about managing your own infrastructure.\n\nTo use Browserless as a document loader, initialize a `BrowserlessLoader` instance as shown in this notebook. Note that by default, `BrowserlessLoader` returns the `innerText` of the page's `body` element. To disable this and get the raw HTML, set `text_content` to `False`.\n\n```python\nfrom langchain_community.document_loaders import BrowserlessLoader\n"
  }
,
  {
    "path": "python/integrations/document_loaders/azlyrics.mdx",
    "filename": "azlyrics.mdx",
    "size_bytes": 2314,
    "line_count": 31,
    "preview": "---\ntitle: AZLyrics\n---\n\n>[AZLyrics](https://www.azlyrics.com/) is a large, legal, every day growing collection of lyrics.\n\nThis covers how to load AZLyrics webpages into a document format that we can use downstream.\n\n```python\nfrom langchain_community.document_loaders import AZLyricsLoader\n"
  }
,
  {
    "path": "python/integrations/document_loaders/quip.mdx",
    "filename": "quip.mdx",
    "size_bytes": 2231,
    "line_count": 43,
    "preview": "---\ntitle: Quip\n---\n\n>[Quip](https://quip.com) is a collaborative productivity software suite for mobile and Web. It allows groups of people to create and edit documents and spreadsheets as a group, typically for business purposes.\n\nA loader for `Quip` docs.\n\nPlease refer [here](https://quip.com/dev/automation/documentation/current#section/Authentication/Get-Access-to-Quip's-APIs) to know how to get personal access token.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/tomarkdown.mdx",
    "filename": "tomarkdown.mdx",
    "size_bytes": 5690,
    "line_count": 126,
    "preview": "---\ntitle: 2Markdown\n---\n\n# 2Markdown\n\n>[2markdown](https://2markdown.com/) service transforms website content into structured markdown files.\n\n```python\n# You will need to get your own API key. See https://2markdown.com/login\n"
  }
,
  {
    "path": "python/integrations/document_loaders/pebblo.mdx",
    "filename": "pebblo.mdx",
    "size_bytes": 4786,
    "line_count": 107,
    "preview": "---\ntitle: Pebblo Safe DocumentLoader\n---\n\n> [Pebblo](https://daxa-ai.github.io/pebblo/) enables developers to safely load data and promote their Gen AI app to deployment without worrying about the organization’s compliance and security requirements. The project identifies semantic topics and entities found in the loaded data and summarizes them on the UI or a PDF report.\n\nPebblo has two components.\n\n1. Pebblo Safe DocumentLoader for LangChain\n1. Pebblo Server\n"
  }
,
  {
    "path": "python/integrations/document_loaders/git.mdx",
    "filename": "git.mdx",
    "size_bytes": 1653,
    "line_count": 88,
    "preview": "---\ntitle: Git\n---\n\n>[Git](https://en.wikipedia.org/wiki/Git) is a distributed version control system that tracks changes in any set of computer files, usually used for coordinating work among programmers collaboratively developing source code during software development.\n\nThis notebook shows how to load text files from `Git` repository.\n\n## Load existing repository from disk\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/youtube_transcript.mdx",
    "filename": "youtube_transcript.mdx",
    "size_bytes": 3907,
    "line_count": 116,
    "preview": "---\ntitle: YouTube transcripts\n---\n\n>[YouTube](https://www.youtube.com/) is an online video sharing and social media platform created by Google.\n\nThis notebook covers how to load documents from `YouTube transcripts`.\n\n```python\nfrom langchain_community.document_loaders import YoutubeLoader\n"
  }
,
  {
    "path": "python/integrations/document_loaders/yt_dlp.mdx",
    "filename": "yt_dlp.mdx",
    "size_bytes": 4203,
    "line_count": 67,
    "preview": "---\ntitle: YoutubeLoaderDL\n---\n\nLoader for Youtube leveraging the `yt-dlp` library.\n\nThis package implements a [document loader](/oss/integrations/document_loaders/) for Youtube. In contrast to the [YoutubeLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.youtube.YoutubeLoader.html) of `langchain-community`, which relies on `pytube`, `YoutubeLoaderDL` is able to fetch YouTube metadata. `langchain-yt-dlp` leverages the robust `yt-dlp` library, providing a more reliable and feature-rich YouTube document loader.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/vsdx.mdx",
    "filename": "vsdx.mdx",
    "size_bytes": 12276,
    "line_count": 417,
    "preview": "---\ntitle: Vsdx\n---\n\n> A [visio file](https://fr.wikipedia.org/wiki/Microsoft_Visio) (with extension .vsdx) is associated with Microsoft Visio, a diagram creation software. It stores information about the structure, layout, and graphical elements of a diagram. This format facilitates the creation and sharing of visualizations in areas such as business, engineering, and computer science.\n\nA Visio file can contain multiple pages. Some of them may serve as the background for others, and this can occur across multiple layers. This **loader** extracts the textual content from each page and its associated pages, enabling the extraction of all visible text from each page, similar to what an OCR algorithm would do.\n\n**WARNING** : Only Visio files with the **.vsdx** extension are compatible with this loader. Files with extensions such as .vsd, ... are not compatible because they cannot be converted to compressed XML.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/powerscale.mdx",
    "filename": "powerscale.mdx",
    "size_bytes": 7141,
    "line_count": 134,
    "preview": "---\ntitle: Dell PowerScale Document Loader\n---\n\n[Dell PowerScale](https://www.dell.com/en-us/shop/powerscale-family/sf/powerscale) is an enterprise scale out storage system that hosts industry leading OneFS filesystem that can be hosted on-prem or deployed in the cloud.\n\nThis document loader utilizes unique capabilities from PowerScale that can determine what files that have been modified since an application's last run and only returns modified files for processing. This will eliminate the need to re-process (chunk and embed) files that have not been changed, improving the overall data ingestion workflow.\n\nThis loader requires PowerScale's MetadataIQ feature enabled. Additional information can be found on our GitHub Repo: [https://github.com/dell/powerscale-rag-connector](https://github.com/dell/powerscale-rag-connector)\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/upstage.mdx",
    "filename": "upstage.mdx",
    "size_bytes": 14028,
    "line_count": 48,
    "preview": "---\ntitle: Upstage\n---\n\nThis notebook covers how to get started with `UpstageDocumentParseLoader`.\n\n## Installation\n\nInstall `langchain-upstage` package.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/athena.mdx",
    "filename": "athena.mdx",
    "size_bytes": 1756,
    "line_count": 66,
    "preview": "---\ntitle: Athena\n---\n\n>[Amazon Athena](https://aws.amazon.com/athena/) is a serverless, interactive analytics service built\n>on open-source frameworks, supporting open-table and file formats. `Athena` provides a simplified,\n>flexible way to analyze petabytes of data where it lives. Analyze data or build applications\n>from an Amazon Simple Storage Service (S3) data lake and 30 data sources, including on-premises data\n>sources or other cloud systems using SQL or Python. `Athena` is built on open-source `Trino`\n>and `Presto` engines and `Apache Spark` frameworks, with no provisioning or configuration effort required.\n"
  }
,
  {
    "path": "python/integrations/document_loaders/snowflake.mdx",
    "filename": "snowflake.mdx",
    "size_bytes": 1275,
    "line_count": 50,
    "preview": "---\ntitle: Snowflake\n---\n\nThis notebooks goes over how to load documents from Snowflake\n\n```python\npip install -qU  snowflake-connector-python\n```\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/whatsapp_chat.mdx",
    "filename": "whatsapp_chat.mdx",
    "size_bytes": 650,
    "line_count": 19,
    "preview": "---\ntitle: WhatsApp Chat\n---\n\n>[WhatsApp](https://www.whatsapp.com/) (also called `WhatsApp Messenger`) is a freeware, cross-platform, centralized instant messaging (IM) and voice-over-IP (VoIP) service. It allows users to send text and voice messages, make voice and video calls, and share images, documents, user locations, and other content.\n\nThis notebook covers how to load data from the `WhatsApp Chats` into a format that can be ingested into LangChain.\n\n```python\nfrom langchain_community.document_loaders import WhatsAppChatLoader\n"
  }
,
  {
    "path": "python/integrations/document_loaders/microsoft_onenote.mdx",
    "filename": "microsoft_onenote.mdx",
    "size_bytes": 5341,
    "line_count": 83,
    "preview": "---\ntitle: Microsoft OneNote\n---\n\nThis notebook covers how to load documents from `OneNote`.\n\n## Prerequisites\n\n1. Register an application with the [Microsoft identity platform](https://learn.microsoft.com/en-us/azure/active-directory/develop/quickstart-register-app) instructions.\n2. When registration finishes, the Azure portal displays the app registration's Overview pane. You see the Application (client) ID. Also called the `client ID`, this value uniquely identifies your application in the Microsoft identity platform.\n"
  }
,
  {
    "path": "python/integrations/document_loaders/google_spanner.mdx",
    "filename": "google_spanner.mdx",
    "size_bytes": 10409,
    "line_count": 310,
    "preview": "---\ntitle: Google Spanner\n---\n\n> [Spanner](https://cloud.google.com/spanner) is a highly scalable database that combines unlimited scalability with relational semantics, such as secondary indexes, strong consistency, schemas, and SQL providing 99.999% availability in one easy solution.\n\nThis notebook goes over how to use [Spanner](https://cloud.google.com/spanner) to [save, load and delete langchain documents](/oss/integrations/document_loaders) with `SpannerLoader` and `SpannerDocumentSaver`.\n\nLearn more about the package on [GitHub](https://github.com/googleapis/langchain-google-spanner-python/).\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/airbyte.mdx",
    "filename": "airbyte.mdx",
    "size_bytes": 4438,
    "line_count": 161,
    "preview": "---\ntitle: AirbyteLoader\n---\n\n>[Airbyte](https://github.com/airbytehq/airbyte) is a data integration platform for ELT pipelines from APIs, databases & files to warehouses & lakes. It has the largest catalog of ELT connectors to data warehouses and databases.\n\nThis covers how to load any source from Airbyte into LangChain documents\n\n## Installation\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/tsv.mdx",
    "filename": "tsv.mdx",
    "size_bytes": 2712,
    "line_count": 124,
    "preview": "---\ntitle: TSV\n---\n\n>A [tab-separated values (TSV)](https://en.wikipedia.org/wiki/Tab-separated_values) file is a simple, text-based file format for storing tabular data.[3] Records are separated by newlines, and values within a record are separated by tab characters.\n\n## `UnstructuredTSVLoader`\n\nYou can also load the table using the `UnstructuredTSVLoader`. One advantage of using `UnstructuredTSVLoader` is that if you use it in `\"elements\"` mode, an HTML representation of the table will be available in the metadata.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/telegram.mdx",
    "filename": "telegram.mdx",
    "size_bytes": 1620,
    "line_count": 49,
    "preview": "---\ntitle: Telegram\n---\n\n>[Telegram Messenger](https://web.telegram.org/a/) is a globally accessible freemium, cross-platform, encrypted, cloud-based and centralized instant messaging service. The application also provides optional end-to-end encrypted chats and video calling, VoIP, file sharing and several other features.\n\nThis notebook covers how to load data from `Telegram` into a format that can be ingested into LangChain.\n\n```python\nfrom langchain_community.document_loaders import (\n"
  }
,
  {
    "path": "python/integrations/document_loaders/docling.mdx",
    "filename": "docling.mdx",
    "size_bytes": 13523,
    "line_count": 309,
    "preview": "---\ntitle: Docling\n---\n\n[Docling](https://github.com/DS4SD/docling) parses PDF, DOCX, PPTX, HTML, and other formats into a rich unified representation including document layout, tables etc., making them ready for generative AI workflows like RAG.\n\nThis integration provides Docling's capabilities via the `DoclingLoader` document loader.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/langsmith.mdx",
    "filename": "langsmith.mdx",
    "size_bytes": 4589,
    "line_count": 149,
    "preview": "---\ntitle: LangSmithLoader\n---\n\nThis guide provides a quick overview for getting started with the LangSmith [document loader](https://python.langchain.com/docs/concepts/document_loaders). For detailed documentation of all LangSmithLoader features and configurations head to the [API reference](https://python.langchain.com/api_reference/core/document_loaders/langchain_core.document_loaders.langsmith.LangSmithLoader.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/notion.mdx",
    "filename": "notion.mdx",
    "size_bytes": 4514,
    "line_count": 130,
    "preview": "---\ntitle: Notion DB 2/2\n---\n\n>[Notion](https://www.notion.so/) is a collaboration platform with modified Markdown support that integrates kanban boards, tasks, wikis and databases. It is an all-in-one workspace for notetaking, knowledge and data management, and project and task management.\n\n`NotionDBLoader` is a Python class for loading content from a `Notion` database. It retrieves pages from the database, reads their content, and returns a list of Document objects. `NotionDirectoryLoader` is used for loading data from a Notion database dump.\n\n## Requirements\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/mediawikidump.mdx",
    "filename": "mediawikidump.mdx",
    "size_bytes": 4938,
    "line_count": 55,
    "preview": "---\ntitle: MediaWiki Dump\n---\n\n>[MediaWiki XML Dumps](https://www.mediawiki.org/wiki/Manual:Importing_XML_dumps) contain the content of a wiki (wiki pages with all their revisions), without the site-related data. A XML dump does not create a full backup of the wiki database, the dump does not contain user accounts, images, edit logs, etc.\n\nThis covers how to load a MediaWiki XML dump file into a document format that we can use downstream.\n\nIt uses `mwxml` from `mediawiki-utilities` to dump and `mwparserfromhell` from `earwig` to parse MediaWiki wikicode.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/spider.mdx",
    "filename": "spider.mdx",
    "size_bytes": 2710,
    "line_count": 41,
    "preview": "---\ntitle: Spider\n---\n\n[Spider](https://spider.cloud/) is the [fastest](https://github.com/spider-rs/spider/blob/main/benches/BENCHMARKS.md) and most affordable crawler and scraper that returns LLM-ready data.\n\n## Setup\n\n```python\npip install spider-client\n"
  }
,
  {
    "path": "python/integrations/document_loaders/singlestore.mdx",
    "filename": "singlestore.mdx",
    "size_bytes": 3056,
    "line_count": 106,
    "preview": "---\ntitle: SingleStoreLoader\n---\n\nThe `SingleStoreLoader` allows you to load documents directly from a SingleStore database table. It is part of the `langchain-singlestore` integration package.\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/brave_search.mdx",
    "filename": "brave_search.mdx",
    "size_bytes": 3284,
    "line_count": 69,
    "preview": "---\ntitle: Brave Search\n---\n\n>[Brave Search](https://en.wikipedia.org/wiki/Brave_Search) is a search engine developed by Brave Software.\n>\n> - `Brave Search` uses its own web index. As of May 2022, it covered over 10 billion pages and was used to serve 92%\n> of search results without relying on any third-parties, with the remainder being retrieved\n> server-side from the Bing API or (on an opt-in basis) client-side from Google. According\n> to Brave, the index was kept \"intentionally smaller than that of Google or Bing\" in order to\n"
  }
,
  {
    "path": "python/integrations/document_loaders/datadog_logs.mdx",
    "filename": "datadog_logs.mdx",
    "size_bytes": 3330,
    "line_count": 43,
    "preview": "---\ntitle: Datadog Logs\n---\n\n>[Datadog](https://www.datadoghq.com/) is a monitoring and analytics platform for cloud-scale applications.\n\nThis loader fetches the logs from your applications in Datadog using the `datadog_api_client` Python package. You must initialize the loader with your `Datadog API key` and `APP key`, and you need to pass in the query to extract the desired logs.\n\n```python\nfrom langchain_community.document_loaders import DatadogLogsLoader\n"
  }
,
  {
    "path": "python/integrations/document_loaders/alibaba_cloud_maxcompute.mdx",
    "filename": "alibaba_cloud_maxcompute.mdx",
    "size_bytes": 3803,
    "line_count": 121,
    "preview": "---\ntitle: Alibaba Cloud MaxCompute\n---\n\n>[Alibaba Cloud MaxCompute](https://www.alibabacloud.com/product/maxcompute) (previously known as ODPS) is a general purpose, fully managed, multi-tenancy data processing platform for large-scale data warehousing. MaxCompute supports various data importing solutions and distributed computing models, enabling users to effectively query massive datasets, reduce production costs, and ensure data security.\n\nThe `MaxComputeLoader` lets you execute a MaxCompute SQL query and loads the results as one document per row.\n\n```python\npip install -qU  pyodps\n"
  }
,
  {
    "path": "python/integrations/document_loaders/news.mdx",
    "filename": "news.mdx",
    "size_bytes": 4705,
    "line_count": 76,
    "preview": "---\ntitle: News URL\n---\n\nThis covers how to load HTML news articles from a list of URLs into a document format that we can use downstream.\n\n```python\nfrom langchain_community.document_loaders import NewsURLLoader\n```\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/gitbook.mdx",
    "filename": "gitbook.mdx",
    "size_bytes": 5890,
    "line_count": 87,
    "preview": "---\ntitle: GitBook\n---\n\n>[GitBook](https://docs.gitbook.com/) is a modern documentation platform where teams can document everything from products to internal knowledge bases and APIs.\n\nThis notebook shows how to pull page data from any `GitBook`.\n\n```python\nfrom langchain_community.document_loaders import GitbookLoader\n"
  }
,
  {
    "path": "python/integrations/document_loaders/async_html.mdx",
    "filename": "async_html.mdx",
    "size_bytes": 2882,
    "line_count": 39,
    "preview": "---\ntitle: AsyncHtml\n---\n\n`AsyncHtmlLoader` loads raw HTML from a list of URLs concurrently.\n\n```python\nfrom langchain_community.document_loaders import AsyncHtmlLoader\n```\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/email.mdx",
    "filename": "email.mdx",
    "size_bytes": 3402,
    "line_count": 87,
    "preview": "---\ntitle: Email\n---\n\nThis notebook shows how to load email (`.eml`) or `Microsoft Outlook` (`.msg`) files.\n\nPlease see [this guide](/oss/integrations/providers/unstructured/) for more instructions on setting up Unstructured locally, including setting up required system dependencies.\n\n## Using unstructured\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/rss.mdx",
    "filename": "rss.mdx",
    "size_bytes": 6813,
    "line_count": 121,
    "preview": "---\ntitle: RSS Feeds\n---\n\nThis covers how to load HTML news articles from a list of RSS feed URLs into a document format that we can use downstream.\n\n```python\npip install -qU  feedparser newspaper3k listparser\n```\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/rockset.mdx",
    "filename": "rockset.mdx",
    "size_bytes": 6456,
    "line_count": 143,
    "preview": "---\ntitle: Rockset\n---\n\n⚠️ **Deprecation Notice: Rockset Integration Disabled**\n>\n> As of June 2024, Rockset has been [acquired by OpenAI](https://openai.com/index/openai-acquires-rockset/) and **shut down its public services**.\n>\n> Rockset was a real-time analytics database known for world-class indexing and retrieval. Now, its core team and technology are being integrated into OpenAI's infrastructure to power future AI products.\n>\n"
  }
,
  {
    "path": "python/integrations/document_loaders/pubmed.mdx",
    "filename": "pubmed.mdx",
    "size_bytes": 2994,
    "line_count": 48,
    "preview": "---\ntitle: PubMed\n---\n\n>[PubMed®](https://pubmed.ncbi.nlm.nih.gov/) by `The National Center for Biotechnology Information, National Library of Medicine` comprises more than 35 million citations for biomedical literature from `MEDLINE`, life science journals, and online books. Citations may include links to full text content from `PubMed Central` and publisher web sites.\n\n```python\nfrom langchain_community.document_loaders import PubMedLoader\n```\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/pypdfloader.mdx",
    "filename": "pypdfloader.mdx",
    "size_bytes": 28618,
    "line_count": 737,
    "preview": "---\ntitle: PyPDFLoader\n---\n\nThis notebook provides a quick overview for getting started with `PyPDF` [document loader](https://python.langchain.com/docs/concepts/document_loaders). For detailed documentation of all DocumentLoader features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/astradb.mdx",
    "filename": "astradb.mdx",
    "size_bytes": 2657,
    "line_count": 80,
    "preview": "---\ntitle: AstraDB\n---\n\n> [DataStax Astra DB](https://docs.datastax.com/en/astra-db-serverless/index.html) is a serverless\n> AI-ready database built on `Apache Cassandra®` and made conveniently available\n> through an easy-to-use JSON API.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/spreedly.mdx",
    "filename": "spreedly.mdx",
    "size_bytes": 7151,
    "line_count": 65,
    "preview": "---\ntitle: Spreedly\n---\n\n>[Spreedly](https://docs.spreedly.com/) is a service that allows you to securely store credit cards and use them to transact against any number of payment gateways and third party APIs. It does this by simultaneously providing a card tokenization/vault service as well as a gateway and receiver integration service. Payment methods tokenized by Spreedly are stored at `Spreedly`, allowing you to independently store a card and then pass that card to different end points based on your business requirements.\n\nThis notebook covers how to load data from the [Spreedly REST API](https://docs.spreedly.com/reference/api/v1/) into a format that can be ingested into LangChain, along with example usage for vectorization.\n\nNote: this notebook assumes the following packages are installed: `openai`, `chromadb`, and `tiktoken`.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/google_speech_to_text.mdx",
    "filename": "google_speech_to_text.mdx",
    "size_bytes": 3788,
    "line_count": 111,
    "preview": "---\ntitle: Google Speech-to-Text Audio Transcripts\n---\n\nThe `SpeechToTextLoader` allows to transcribe audio files with the [Google Cloud Speech-to-Text API](https://cloud.google.com/speech-to-text) and loads the transcribed text into documents.\n\nTo use it, you should have the `google-cloud-speech` python package installed, and a Google Cloud project with the [Speech-to-Text API enabled](https://cloud.google.com/speech-to-text/v2/docs/transcribe-client-libraries#before_you_begin).\n\n- [Bringing the power of large models to Google Cloud’s Speech API](https://cloud.google.com/blog/products/ai-machine-learning/bringing-power-large-models-google-clouds-speech-api)\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/image.mdx",
    "filename": "image.mdx",
    "size_bytes": 4023,
    "line_count": 45,
    "preview": "---\ntitle: Images\n---\n\nThis covers how to load images into a document format that we can use downstream with other LangChain modules.\n\nIt uses [Unstructured](https://unstructured.io/) to handle a wide variety of image formats, such as `.jpg` and `.png`. Please see [this guide](/oss/integrations/providers/unstructured/) for more instructions on setting up Unstructured locally, including setting up required system dependencies.\n\n## Using unstructured\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/pdfplumber.mdx",
    "filename": "pdfplumber.mdx",
    "size_bytes": 5298,
    "line_count": 88,
    "preview": "---\ntitle: PDFPlumber\n---\n\nLike PyMuPDF, the output Documents contain detailed metadata about the PDF and its pages, and returns one document per page.\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/mongodb.mdx",
    "filename": "mongodb.mdx",
    "size_bytes": 1446,
    "line_count": 63,
    "preview": "---\ntitle: MongoDB\n---\n\n[MongoDB](https://www.mongodb.com/) is a NoSQL , document-oriented database that supports JSON-like documents with a dynamic schema.\n\n## Overview\n\nThe MongoDB Document Loader returns a list of LangChain Documents from a MongoDB database.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/org_mode.mdx",
    "filename": "org_mode.mdx",
    "size_bytes": 889,
    "line_count": 28,
    "preview": "---\ntitle: Org-mode\n---\n\n>A [Org Mode document](https://en.wikipedia.org/wiki/Org-mode) is a document editing, formatting, and organizing mode, designed for notes, planning, and authoring within the free software text editor Emacs.\n\n## `UnstructuredOrgModeLoader`\n\nYou can load data from Org-mode files with `UnstructuredOrgModeLoader` using the following workflow.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/aws_s3_directory.mdx",
    "filename": "aws_s3_directory.mdx",
    "size_bytes": 1529,
    "line_count": 58,
    "preview": "---\ntitle: AWS S3 Directory\n---\n\n>[Amazon Simple Storage Service (Amazon S3)](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html) is an object storage service\n\n>[AWS S3 Directory](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html)\n\nThis covers how to load document objects from an `AWS S3 Directory` object.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/geopandas.mdx",
    "filename": "geopandas.mdx",
    "size_bytes": 3888,
    "line_count": 92,
    "preview": "---\ntitle: Geopandas\n---\n\n[Geopandas](https://geopandas.org/en/stable/index.html) is an open-source project to make working with geospatial data in python easier.\n\nGeoPandas extends the datatypes used by pandas to allow spatial operations on geometric types.\n\nGeometric operations are performed by shapely. Geopandas further depends on fiona for file access and matplotlib for plotting.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/box.mdx",
    "filename": "box.mdx",
    "size_bytes": 10917,
    "line_count": 271,
    "preview": "---\ntitle: BoxLoader and BoxBlobLoader\n---\n\nThe `langchain-box` package provides two methods to index your files from Box: `BoxLoader` and `BoxBlobLoader`. `BoxLoader` allows you to ingest text representations of files that have a text representation in Box. The `BoxBlobLoader` allows you download the blob for any document or image file for processing with the blob parser of your choice.\n\nThis notebook details getting started with both of these. For detailed documentation of all BoxLoader features and configurations head to the API Reference pages for [BoxLoader](https://python.langchain.com/api_reference/box/document_loaders/langchain_box.document_loaders.box.BoxLoader.html) and [BoxBlobLoader](https://python.langchain.com/api_reference/box/document_loaders/langchain_box.blob_loaders.box.BoxBlobLoader.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/yuque.mdx",
    "filename": "yuque.mdx",
    "size_bytes": 542,
    "line_count": 21,
    "preview": "---\ntitle: Yuque\n---\n\n>[Yuque](https://www.yuque.com/) is a professional cloud-based knowledge base for team collaboration in documentation.\n\nThis notebook covers how to load documents from `Yuque`.\n\nYou can obtain the personal access token by clicking on your personal avatar in the [Personal Settings](https://www.yuque.com/settings/tokens) page.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/rspace.mdx",
    "filename": "rspace.mdx",
    "size_bytes": 2082,
    "line_count": 59,
    "preview": "---\ntitle: RSpace\n---\n\nThis notebook shows how to use the RSpace document loader to import research notes and documents from RSpace Electronic\nLab Notebook into LangChain pipelines.\n\nTo start you'll need an RSpace account and an API key.\n\nYou can set up a free account at [https://community.researchspace.com](https://community.researchspace.com) or use your institutional RSpace.\n"
  }
,
  {
    "path": "python/integrations/document_loaders/ifixit.mdx",
    "filename": "ifixit.mdx",
    "size_bytes": 18457,
    "line_count": 69,
    "preview": "---\ntitle: iFixit\n---\n\n>[iFixit](https://www.ifixit.com) is the largest, open repair community on the web. The site contains nearly 100k repair manuals, 200k Questions & Answers on 42k devices, and all the data is licensed under CC-BY-NC-SA 3.0.\n\nThis loader will allow you to download the text of a repair guide, text of Q&A's and wikis from devices on `iFixit` using their open APIs.  It's incredibly useful for context related to technical documents and answers to questions about devices in the corpus of data on `iFixit`.\n\n```python\nfrom langchain_community.document_loaders import IFixitLoader\n"
  }
,
  {
    "path": "python/integrations/document_loaders/csv.mdx",
    "filename": "csv.mdx",
    "size_bytes": 17581,
    "line_count": 243,
    "preview": "---\ntitle: CSV\n---\n\n>A [comma-separated values (CSV)](https://en.wikipedia.org/wiki/Comma-separated_values) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas.\n\nLoad [csv](https://en.wikipedia.org/wiki/Comma-separated_values) data with a single row per document.\n\n```python\nfrom langchain_community.document_loaders.csv_loader import CSVLoader\n"
  }
,
  {
    "path": "python/integrations/document_loaders/arxiv.mdx",
    "filename": "arxiv.mdx",
    "size_bytes": 34848,
    "line_count": 92,
    "preview": "---\ntitle: ArxivLoader\n---\n\n[arXiv](https://arxiv.org/) is an open-access archive for 2 million scholarly articles in the fields of physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems science, and economics.\n\n## Setup\n\nTo access Arxiv document loader you'll need to install the `arxiv`, `PyMuPDF` and `langchain-community` integration packages. PyMuPDF transforms PDF files downloaded from the arxiv.org site into the text format.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/pdfminer.mdx",
    "filename": "pdfminer.mdx",
    "size_bytes": 114344,
    "line_count": 1790,
    "preview": "---\ntitle: PDFMinerLoader\n---\n\nThis guide provides a quick overview for getting started with `PDFMiner` [document loader](https://python.langchain.com/docs/concepts/document_loaders). For detailed documentation of all __ModuleName__Loader features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PDFMinerLoader.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/web_base.mdx",
    "filename": "web_base.mdx",
    "size_bytes": 9483,
    "line_count": 201,
    "preview": "---\ntitle: WebBaseLoader\n---\n\nThis covers how to use `WebBaseLoader` to load all text from `HTML` webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as `IMSDbLoader`, `AZLyricsLoader`, and `CollegeConfidentialLoader`.\n\nIf you don't want to worry about website crawling, bypassing JS-blocking sites, and data cleaning, consider using `FireCrawlLoader` or the faster option `SpiderLoader`.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/bshtml.mdx",
    "filename": "bshtml.mdx",
    "size_bytes": 3420,
    "line_count": 124,
    "preview": "---\ntitle: BSHTMLLoader\n---\n\nThis guide provides a quick overview for getting started with BeautifulSoup4 [document loader](https://python.langchain.com/docs/concepts/document_loaders). For detailed documentation of all __ModuleName__Loader features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.html_bs.BSHTMLLoader.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/airbyte_hubspot.mdx",
    "filename": "airbyte_hubspot.mdx",
    "size_bytes": 3394,
    "line_count": 92,
    "preview": "---\ntitle: Airbyte Hubspot (Deprecated)\n---\n\nNote: `AirbyteHubspotLoader` is deprecated. Please use [`AirbyteLoader`](/oss/integrations/document_loaders/airbyte) instead.\n\n>[Airbyte](https://github.com/airbytehq/airbyte) is a data integration platform for ELT pipelines from APIs, databases & files to warehouses & lakes. It has the largest catalog of ELT connectors to data warehouses and databases.\n\nThis loader exposes the Hubspot connector as a document loader, allowing you to load various Hubspot objects as documents.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/open_city_data.mdx",
    "filename": "open_city_data.mdx",
    "size_bytes": 1692,
    "line_count": 62,
    "preview": "---\ntitle: Open City Data\n---\n\n[Socrata](https://dev.socrata.com/foundry/data.sfgov.org/vw6y-z8j6) provides an API for city open data.\n\nFor a dataset such as [SF crime](https://data.sfgov.org/Public-Safety/Police-Department-Incident-Reports-Historical-2003/tmnf-yvry), see the `API` tab on top right.\n\nThat provides you with the `dataset identifier`.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/stripe.mdx",
    "filename": "stripe.mdx",
    "size_bytes": 1576,
    "line_count": 42,
    "preview": "---\ntitle: Stripe\n---\n\n>[Stripe](https://stripe.com/en-ca) is an Irish-American financial services and software as a service (SaaS) company. It offers payment-processing software and application programming interfaces for e-commerce websites and mobile applications.\n\nThis notebook covers how to load data from the `Stripe REST API` into a format that can be ingested into LangChain, along with example usage for vectorization.\n\n```python\nfrom langchain.indexes import VectorstoreIndexCreator\n"
  }
,
  {
    "path": "python/integrations/document_loaders/conll-u.mdx",
    "filename": "conll-u.mdx",
    "size_bytes": 1081,
    "line_count": 31,
    "preview": "---\ntitle: CoNLL-U\n---\n\n>[CoNLL-U](https://universaldependencies.org/format.html) is revised version of the CoNLL-X format. Annotations are encoded in plain text files (UTF-8, normalized to NFC, using only the LF character as line break, including an LF character at the end of file) with three types of lines:\n>\n>- Word lines containing the annotation of a word/token in 10 fields separated by single tab characters; see below.\n>- Blank lines marking sentence boundaries.\n>- Comment lines starting with hash (#).\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/blackboard.mdx",
    "filename": "blackboard.mdx",
    "size_bytes": 1646,
    "line_count": 24,
    "preview": "---\ntitle: Blackboard\n---\n\n>[Blackboard Learn](https://en.wikipedia.org/wiki/Blackboard_Learn) (previously the Blackboard Learning Management System) is a web-based virtual learning environment and learning management system developed by Blackboard Inc. The software features course management, customizable open architecture, and scalable design that allows integration with student information systems and authentication protocols. It may be installed on local servers, hosted by `Blackboard ASP Solutions`, or provided as Software as a Service hosted on Amazon Web Services. Its main purposes are stated to include the addition of online elements to courses traditionally delivered face-to-face and development of completely online courses with few or no face-to-face meetings\n\nThis covers how to load data from a [Blackboard Learn](https://www.anthology.com/products/teaching-and-learning/learning-effectiveness/blackboard-learn) instance.\n\nThis loader is not compatible with all `Blackboard` courses. It is only\n    compatible with courses that use the new `Blackboard` interface.\n"
  }
,
  {
    "path": "python/integrations/document_loaders/google_cloud_storage_directory.mdx",
    "filename": "google_cloud_storage_directory.mdx",
    "size_bytes": 3977,
    "line_count": 71,
    "preview": "---\ntitle: Google Cloud Storage Directory\n---\n\n>[Google Cloud Storage](https://en.wikipedia.org/wiki/Google_Cloud_Storage) is a managed service for storing unstructured data.\n\nThis covers how to load document objects from an `Google Cloud Storage (GCS) directory (bucket)`.\n\n```python\npip install -qU  langchain-google-community[gcs]\n"
  }
,
  {
    "path": "python/integrations/document_loaders/mathpix.mdx",
    "filename": "mathpix.mdx",
    "size_bytes": 2384,
    "line_count": 89,
    "preview": "---\ntitle: MathPixPDFLoader\n---\n\nInspired by Daniel Gross's snippet here: [https://gist.github.com/danielgross/3ab4104e14faccc12b49200843adab21](https://gist.github.com/danielgross/3ab4104e14faccc12b49200843adab21)\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/mintbase.mdx",
    "filename": "mintbase.mdx",
    "size_bytes": 1374,
    "line_count": 54,
    "preview": "---\ntitle: Near Blockchain\n---\n\nThe intention of this notebook is to provide a means of testing functionality in the LangChain Document Loader for Near Blockchain.\n\nInitially this Loader supports:\n\n* Loading NFTs as Documents from NFT Smart Contracts (NEP-171 and NEP-177)\n* Near Mainnnet, Near Testnet (default is mainnet)\n"
  }
,
  {
    "path": "python/integrations/document_loaders/nuclia.mdx",
    "filename": "nuclia.mdx",
    "size_bytes": 2987,
    "line_count": 74,
    "preview": "---\ntitle: Nuclia\n---\n\n>[Nuclia](https://nuclia.com) automatically indexes your unstructured data from any internal and external source, providing optimized search results and generative answers. It can handle video and audio transcription, image content extraction, and document parsing.\n\n>The `Nuclia Understanding API` supports the processing of unstructured data, including text, web pages, documents, and audio/video contents. It extracts all texts wherever they are (using speech-to-text or OCR when needed), it also extracts metadata, embedded files (like images in a PDF), and web links. If machine learning is enabled, it identifies entities, provides a summary of the content and generates embeddings for all the sentences.\n\n## Setup\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/blockchain.mdx",
    "filename": "blockchain.mdx",
    "size_bytes": 1853,
    "line_count": 73,
    "preview": "---\ntitle: Blockchain\n---\n\nThe intention of this notebook is to provide a means of testing functionality in the LangChain Document Loader for Blockchain.\n\nInitially this Loader supports:\n\n* Loading NFTs as Documents from NFT Smart Contracts (ERC721 and ERC1155)\n* Ethereum Mainnnet, Ethereum Testnet, Polygon Mainnet, Polygon Testnet (default is eth-mainnet)\n"
  }
,
  {
    "path": "python/integrations/document_loaders/microsoft_onedrive.mdx",
    "filename": "microsoft_onedrive.mdx",
    "size_bytes": 7250,
    "line_count": 136,
    "preview": "---\ntitle: Microsoft OneDrive\n---\n\n>[Microsoft OneDrive](https://en.wikipedia.org/wiki/OneDrive) (formerly `SkyDrive`) is a file hosting service operated by Microsoft.\n\nThis notebook covers how to load documents from `OneDrive`. By default the document loader loads `pdf`, `doc`, `docx` and `txt` files. You can load other file types by providing appropriate parsers (see more below).\n\n## Prerequisites\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/google_cloud_sql_mssql.mdx",
    "filename": "google_cloud_sql_mssql.mdx",
    "size_bytes": 14880,
    "line_count": 383,
    "preview": "---\ntitle: Google Cloud SQL for SQL server\n---\n\n> [Cloud SQL](https://cloud.google.com/sql) is a fully managed relational database service that offers high performance, seamless integration, and impressive scalability. It offers [MySQL](https://cloud.google.com/sql/mysql), [PostgreSQL](https://cloud.google.com/sql/postgres), and [SQL Server](https://cloud.google.com/sql/sqlserver) database engines. Extend your database application to build AI-powered experiences leveraging Cloud SQL's LangChain integrations.\n\nThis notebook goes over how to use [Cloud SQL for SQL server](https://cloud.google.com/sql/sqlserver) to [save, load and delete langchain documents](/oss/integrations/document_loaders) with `MSSQLLoader` and `MSSQLDocumentSaver`.\n\nLearn more about the package on [GitHub](https://github.com/googleapis/langchain-google-cloud-sql-mssql-python/).\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/epub.mdx",
    "filename": "epub.mdx",
    "size_bytes": 302136,
    "line_count": 49,
    "preview": "---\ntitle: EPub\n---\n\n>[EPUB](https://en.wikipedia.org/wiki/EPUB) is an e-book file format that uses the \".epub\" file extension. The term is short for electronic publication and is sometimes styled ePub. `EPUB` is supported by many e-readers, and compatible software is available for most smartphones, tablets, and computers.\n\nThis covers how to load `.epub` documents into the Document format that we can use downstream. You'll need to install the [`pandoc`](https://pandoc.org/installing.html) package for this loader to work with e.g. `brew install pandoc` for OSX.\n\nPlease see [this guide](/oss/integrations/providers/unstructured/) for more instructions on setting up Unstructured locally, including setting up required system dependencies.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/etherscan.mdx",
    "filename": "etherscan.mdx",
    "size_bytes": 20764,
    "line_count": 136,
    "preview": "---\ntitle: Etherscan\n---\n\n>[Etherscan](https://docs.etherscan.io/)  is the leading blockchain explorer, search, API and analytics platform for Ethereum,\na decentralized smart contracts platform.\n\n## Overview\n\nThe `Etherscan` loader use `Etherscan API` to load transactions histories under specific account on `Ethereum Mainnet`.\n"
  }
,
  {
    "path": "python/integrations/document_loaders/pyspark_dataframe.mdx",
    "filename": "pyspark_dataframe.mdx",
    "size_bytes": 4124,
    "line_count": 76,
    "preview": "---\ntitle: PySpark\n---\n\nThis notebook goes over how to load data from a [PySpark](https://spark.apache.org/docs/latest/api/python/) DataFrame.\n\n```python\npip install -qU  pyspark\n```\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/odt.mdx",
    "filename": "odt.mdx",
    "size_bytes": 1430,
    "line_count": 25,
    "preview": "---\ntitle: Open Document Format (ODT)\n---\n\n>The [Open Document Format for Office Applications (ODF)](https://en.wikipedia.org/wiki/OpenDocument), also known as `OpenDocument`, is an open file format for word processing documents, spreadsheets, presentations and graphics and using ZIP-compressed XML files. It was developed with the aim of providing an open, XML-based file format specification for office applications.\n\n>The standard is developed and maintained by a technical committee in the Organization for the Advancement of Structured Information Standards (`OASIS`) consortium. It was based on the Sun Microsystems specification for OpenOffice.org XML, the default format for `OpenOffice.org` and `LibreOffice`. It was originally developed for `StarOffice` \"to provide an open standard for office documents.\"\n\nThe `UnstructuredODTLoader` is used to load `Open Office ODT` files.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/jupyter_notebook.mdx",
    "filename": "jupyter_notebook.mdx",
    "size_bytes": 2230,
    "line_count": 37,
    "preview": "---\ntitle: Jupyter Notebook\n---\n\n>[Jupyter Notebook](https://en.wikipedia.org/wiki/Project_Jupyter#Applications) (formerly `IPython Notebook`) is a web-based interactive computational environment for creating notebook documents.\n\nThis notebook covers how to load data from a `Jupyter notebook (.ipynb)` into a format suitable by LangChain.\n\n```python\nfrom langchain_community.document_loaders import NotebookLoader\n"
  }
,
  {
    "path": "python/integrations/document_loaders/psychic.mdx",
    "filename": "psychic.mdx",
    "size_bytes": 2236,
    "line_count": 58,
    "preview": "---\ntitle: Psychic\n---\n\nThis notebook covers how to load documents from `Psychic`. See [here](/oss/integrations/providers/psychic) for more details.\n\n## Prerequisites\n\n1. Follow the Quick Start section in [this document](/oss/integrations/providers/psychic)\n2. Log into the [Psychic dashboard](https://dashboard.psychic.dev/) and get your secret key\n"
  }
,
  {
    "path": "python/integrations/document_loaders/iugu.mdx",
    "filename": "iugu.mdx",
    "size_bytes": 1169,
    "line_count": 32,
    "preview": "---\ntitle: Iugu\n---\n\n>[Iugu](https://www.iugu.com/) is a Brazilian services and software as a service (SaaS) company. It offers payment-processing software and application programming interfaces for e-commerce websites and mobile applications.\n\nThis notebook covers how to load data from the `Iugu REST API` into a format that can be ingested into LangChain, along with example usage for vectorization.\n\n```python\nfrom langchain.indexes import VectorstoreIndexCreator\n"
  }
,
  {
    "path": "python/integrations/document_loaders/couchbase.mdx",
    "filename": "couchbase.mdx",
    "size_bytes": 23470,
    "line_count": 98,
    "preview": "---\ntitle: Couchbase\n---\n\n>[Couchbase](http://couchbase.com/) is an award-winning distributed NoSQL cloud database that delivers unmatched versatility, performance, scalability, and financial value for all of your cloud, mobile, AI, and edge computing applications.\n\n## Installation\n\n```python\npip install -qU  couchbase\n"
  }
,
  {
    "path": "python/integrations/document_loaders/toml.mdx",
    "filename": "toml.mdx",
    "size_bytes": 1270,
    "line_count": 31,
    "preview": "---\ntitle: TOML\n---\n\n>[TOML](https://en.wikipedia.org/wiki/TOML) is a file format for configuration files. It is intended to be easy to read and write, and is designed to map unambiguously to a dictionary. Its specification is open-source. `TOML` is implemented in many programming languages. The name `TOML` is an acronym for \"Tom's Obvious, Minimal Language\" referring to its creator, Tom Preston-Werner.\n\nIf you need to load `Toml` files, use the `TomlLoader`.\n\n```python\nfrom langchain_community.document_loaders import TomlLoader\n"
  }
,
  {
    "path": "python/integrations/document_loaders/pypdfium2.mdx",
    "filename": "pypdfium2.mdx",
    "size_bytes": 27501,
    "line_count": 723,
    "preview": "---\ntitle: PyPDFium2Loader\n---\n\nThis guide provides a quick overview for getting started with `PyPDF` [document loader](https://python.langchain.com/docs/concepts/document_loaders). For detailed documentation of all DocumentLoader features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyPDFium2Loader.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/tencent_cos_file.mdx",
    "filename": "tencent_cos_file.mdx",
    "size_bytes": 1147,
    "line_count": 35,
    "preview": "---\ntitle: Tencent COS File\n---\n\n>[Tencent Cloud Object Storage (COS)](https://www.tencentcloud.com/products/cos) is a distributed\n> storage service that enables you to store any amount of data from anywhere via HTTP/HTTPS protocols.\n> `COS` has no restrictions on data structure or format. It also has no bucket size limit and\n> partition management, making it suitable for virtually any use case, such as data delivery,\n> data processing, and data lakes. `COS` provides a web-based console, multi-language SDKs and APIs,\n> command line tool, and graphical tools. It works well with Amazon S3 APIs, allowing you to quickly\n"
  }
,
  {
    "path": "python/integrations/document_loaders/airbyte_zendesk_support.mdx",
    "filename": "airbyte_zendesk_support.mdx",
    "size_bytes": 3508,
    "line_count": 94,
    "preview": "---\ntitle: Airbyte Zendesk Support (Deprecated)\n---\n\nNote: This connector-specific loader is deprecated. Please use [`AirbyteLoader`](/oss/integrations/document_loaders/airbyte) instead.\n\n>[Airbyte](https://github.com/airbytehq/airbyte) is a data integration platform for ELT pipelines from APIs, databases & files to warehouses & lakes. It has the largest catalog of ELT connectors to data warehouses and databases.\n\nThis loader exposes the Zendesk Support connector as a document loader, allowing you to load various objects as documents.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/outline.mdx",
    "filename": "outline.mdx",
    "size_bytes": 2533,
    "line_count": 74,
    "preview": "---\ntitle: Outline Document Loader\n---\n\n>[Outline](https://www.getoutline.com/) is an open-source collaborative knowledge base platform designed for team information sharing.\n\nThis notebook shows how to obtain langchain Documents from your Outline collections.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/imsdb.mdx",
    "filename": "imsdb.mdx",
    "size_bytes": 1100,
    "line_count": 35,
    "preview": "---\ntitle: IMSDb\n---\n\n>[IMSDb](https://imsdb.com/) is the `Internet Movie Script Database`.\n\nThis covers how to load `IMSDb` webpages into a document format that we can use downstream.\n\n```python\nfrom langchain_community.document_loaders import IMSDbLoader\n"
  }
,
  {
    "path": "python/integrations/document_loaders/tidb.mdx",
    "filename": "tidb.mdx",
    "size_bytes": 4341,
    "line_count": 116,
    "preview": "---\ntitle: TiDB\n---\n\n> [TiDB Cloud](https://tidbcloud.com/), is a comprehensive Database-as-a-Service (DBaaS) solution, that provides dedicated and serverless options. TiDB Serverless is now integrating a built-in vector search into the MySQL landscape. With this enhancement, you can seamlessly develop AI applications using TiDB Serverless without the need for a new database or additional technical stacks. Be among the first to experience it by joining the waitlist for the private beta at [tidb.cloud/ai](https://tidb.cloud/ai).\n\nThis notebook introduces how to use `TiDBLoader` to load data from TiDB in langchain.\n\n## Prerequisites\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/unstructured_pdfloader.mdx",
    "filename": "unstructured_pdfloader.mdx",
    "size_bytes": 62894,
    "line_count": 133,
    "preview": "---\ntitle: UnstructuredPDFLoader\n---\n\n[Unstructured](https://unstructured-io.github.io/unstructured/) supports a common interface for working with unstructured or semi-structured file formats, such as Markdown or PDF. LangChain's [UnstructuredPDFLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.UnstructuredPDFLoader.html) integrates with Unstructured to parse PDF documents into LangChain [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects.\n\nPlease see [this page](/oss/integrations/providers/unstructured/) for more information on installing system requirements.\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/reddit.mdx",
    "filename": "reddit.mdx",
    "size_bytes": 7006,
    "line_count": 59,
    "preview": "---\ntitle: Reddit\n---\n\n>[Reddit](https://www.reddit.com) is an American social news aggregation, content rating, and discussion website.\n\nThis loader fetches the text from the Posts of Subreddits or Reddit users, using the `praw` Python package.\n\nMake a [Reddit Application](https://www.reddit.com/prefs/apps/) and initialize the loader with your Reddit API credentials.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/sitemap.mdx",
    "filename": "sitemap.mdx",
    "size_bytes": 6560,
    "line_count": 188,
    "preview": "---\ntitle: SitemapLoader\n---\n\nExtends from the `WebBaseLoader`, `SitemapLoader` loads a sitemap from a given URL, and then scrapes and loads all pages in the sitemap, returning each page as a Document.\n\nThe scraping is done concurrently. There are reasonable limits to concurrent requests, defaulting to 2 per second.  If you aren't concerned about being a good citizen, or you control the scrapped server, or don't care about load you can increase this limit. Note, while this will speed up the scraping process, it may cause the server to block you. Be careful!\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/airbyte_salesforce.mdx",
    "filename": "airbyte_salesforce.mdx",
    "size_bytes": 3840,
    "line_count": 97,
    "preview": "---\ntitle: Airbyte Salesforce (Deprecated)\n---\n\nNote: This connector-specific loader is deprecated. Please use [`AirbyteLoader`](/oss/integrations/document_loaders/airbyte) instead.\n\n>[Airbyte](https://github.com/airbytehq/airbyte) is a data integration platform for ELT pipelines from APIs, databases & files to warehouses & lakes. It has the largest catalog of ELT connectors to data warehouses and databases.\n\nThis loader exposes the Salesforce connector as a document loader, allowing you to load various Salesforce objects as documents.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/trello.mdx",
    "filename": "trello.mdx",
    "size_bytes": 3711,
    "line_count": 111,
    "preview": "---\ntitle: Trello\n---\n\n>[Trello](https://www.atlassian.com/software/trello) is a web-based project management and collaboration tool that allows individuals and teams to organize and track their tasks and projects. It provides a visual interface known as a \"board\" where users can create lists and cards to represent their tasks and activities.\n\nThe TrelloLoader allows you to load cards from a Trello board and is implemented on top of [py-trello](https://pypi.org/project/py-trello/)\n\nThis currently supports `api_key/token` only.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/source_code.mdx",
    "filename": "source_code.mdx",
    "size_bytes": 7013,
    "line_count": 302,
    "preview": "---\ntitle: Source Code\n---\n\nThis notebook covers how to load source code files using a special approach with language parsing: each top-level function and class in the code is loaded into separate documents. Any remaining code top-level code outside the already loaded functions and classes will be loaded into a separate document.\n\nThis approach can potentially improve the accuracy of QA models over source code.\n\nThe supported languages for code parsing are:\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/evernote.mdx",
    "filename": "evernote.mdx",
    "size_bytes": 1986,
    "line_count": 38,
    "preview": "---\ntitle: EverNote\n---\n\n>[EverNote](https://evernote.com/) is intended for archiving and creating notes in which photos, audio and saved web content can be embedded. Notes are stored in virtual \"notebooks\" and can be tagged, annotated, edited, searched, and exported.\n\nThis notebook shows how to load an `Evernote` [export](https://help.evernote.com/hc/en-us/articles/209005557-Export-notes-and-notebooks-as-ENEX-or-HTML) file (.enex) from disk.\n\nA document will be created for each note in the export.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/surrealdb.mdx",
    "filename": "surrealdb.mdx",
    "size_bytes": 4356,
    "line_count": 117,
    "preview": "---\ntitle: SurrealDB\n---\n\n>[SurrealDB](https://surrealdb.com/) is an end-to-end cloud-native database designed for modern applications, including web, mobile, serverless, Jamstack, backend, and traditional applications. With SurrealDB, you can simplify your database and API infrastructure, reduce development time, and build secure, performant apps quickly and cost-effectively.\n>\n>**Key features of SurrealDB include:**\n>\n>* **Reduces development time:** SurrealDB simplifies your database and API stack by removing the need for most server-side components, allowing you to build secure, performant apps faster and cheaper.\n>* **Real-time collaborative API backend service:** SurrealDB functions as both a database and an API backend service, enabling real-time collaboration.\n"
  }
,
  {
    "path": "python/integrations/document_loaders/twitter.mdx",
    "filename": "twitter.mdx",
    "size_bytes": 14695,
    "line_count": 47,
    "preview": "---\ntitle: Twitter\n---\n\n>[Twitter](https://twitter.com/) is an online social media and social networking service.\n\nThis loader fetches the text from the Tweets of a list of `Twitter` users, using the `tweepy` Python package.\nYou must initialize the loader with your `Twitter API` token, and you need to pass in the Twitter username you want to extract.\n\n```python\n"
  }
,
  {
    "path": "python/integrations/document_loaders/mhtml.mdx",
    "filename": "mhtml.mdx",
    "size_bytes": 1447,
    "line_count": 27,
    "preview": "---\ntitle: mhtml\n---\n\nMHTML is a is used both for emails but also for archived webpages. MHTML, sometimes referred as MHT, stands for MIME HTML is a single file in which entire webpage is archived. When one saves a webpage as MHTML format, this file extension will contain HTML code, images, audio files, flash animation etc.\n\n```python\nfrom langchain_community.document_loaders import MHTMLLoader\n```\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/azure_document_intelligence.mdx",
    "filename": "azure_document_intelligence.mdx",
    "size_bytes": 4030,
    "line_count": 118,
    "preview": "---\ntitle: Azure AI Document Intelligence\n---\n\n>[Azure AI Document Intelligence](https://aka.ms/doc-intelligence) (formerly known as `Azure Form Recognizer`) is machine-learning\n>based service that extracts texts (including handwriting), tables, document structures (e.g., titles, section headings, etc.) and key-value-pairs from\n>digital or scanned PDFs, images, Office and HTML files.\n>\n>Document Intelligence supports `PDF`, `JPEG/JPG`, `PNG`, `BMP`, `TIFF`, `HEIF`, `DOCX`, `XLSX`, `PPTX` and `HTML`.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/discord.mdx",
    "filename": "discord.mdx",
    "size_bytes": 1462,
    "line_count": 41,
    "preview": "---\ntitle: Discord\n---\n\n>[Discord](https://discord.com/) is a VoIP and instant messaging social platform. Users have the ability to communicate with voice calls, video calls, text messaging, media and files in private chats or as part of communities called \"servers\". A server is a collection of persistent chat rooms and voice channels which can be accessed via invite links.\n\nFollow these steps to download your `Discord` data:\n\n1. Go to your **User Settings**\n2. Then go to **Privacy and Safety**\n"
  }
,
  {
    "path": "python/integrations/document_loaders/assemblyai.mdx",
    "filename": "assemblyai.mdx",
    "size_bytes": 3424,
    "line_count": 115,
    "preview": "---\ntitle: AssemblyAI Audio Transcripts\n---\n\nThe `AssemblyAIAudioTranscriptLoader` allows to transcribe audio files with the [AssemblyAI API](https://www.assemblyai.com) and loads the transcribed text into documents.\n\nTo use it, you should have the `assemblyai` python package installed, and the\nenvironment variable `ASSEMBLYAI_API_KEY` set with your API key. Alternatively, the API key can also be passed as an argument.\n\nMore info about AssemblyAI:\n"
  }
,
  {
    "path": "python/integrations/document_loaders/airbyte_typeform.mdx",
    "filename": "airbyte_typeform.mdx",
    "size_bytes": 3485,
    "line_count": 93,
    "preview": "---\ntitle: Airbyte Typeform (Deprecated)\n---\n\nNote: This connector-specific loader is deprecated. Please use [`AirbyteLoader`](/oss/integrations/document_loaders/airbyte) instead.\n\n>[Airbyte](https://github.com/airbytehq/airbyte) is a data integration platform for ELT pipelines from APIs, databases & files to warehouses & lakes. It has the largest catalog of ELT connectors to data warehouses and databases.\n\nThis loader exposes the Typeform connector as a document loader, allowing you to load various Typeform objects as documents.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/google_bigtable.mdx",
    "filename": "google_bigtable.mdx",
    "size_bytes": 9824,
    "line_count": 294,
    "preview": "---\ntitle: Google Bigtable\n---\n\n> [Bigtable](https://cloud.google.com/bigtable) is a key-value and wide-column store, ideal for fast access to structured, semi-structured, or unstructured data. Extend your database application to build AI-powered experiences leveraging Bigtable's LangChain integrations.\n\nThis notebook goes over how to use [Bigtable](https://cloud.google.com/bigtable) to [save, load and delete langchain documents](/oss/integrations/document_loaders) with `BigtableLoader` and `BigtableSaver`.\n\nLearn more about the package on [GitHub](https://github.com/googleapis/langchain-google-bigtable-python/).\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/airbyte_stripe.mdx",
    "filename": "airbyte_stripe.mdx",
    "size_bytes": 3343,
    "line_count": 93,
    "preview": "---\ntitle: Airbyte Stripe (Deprecated)\n---\n\nNote: This connector-specific loader is deprecated. Please use [`AirbyteLoader`](/oss/integrations/document_loaders/airbyte) instead.\n\n>[Airbyte](https://github.com/airbytehq/airbyte) is a data integration platform for ELT pipelines from APIs, databases & files to warehouses & lakes. It has the largest catalog of ELT connectors to data warehouses and databases.\n\nThis loader exposes the Stripe connector as a document loader, allowing you to load various Stripe objects as documents.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/kinetica.mdx",
    "filename": "kinetica.mdx",
    "size_bytes": 1818,
    "line_count": 74,
    "preview": "---\ntitle: Kinetica\n---\n\nThis notebooks goes over how to load documents from Kinetica\n\n```python\npip install gpudb==7.2.0.9\n```\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/xorbits.mdx",
    "filename": "xorbits.mdx",
    "size_bytes": 7036,
    "line_count": 177,
    "preview": "---\ntitle: Xorbits Pandas DataFrame\n---\n\nThis notebook goes over how to load data from a [xorbits.pandas](https://doc.xorbits.io/en/latest/reference/pandas/frame.html) DataFrame.\n\n```python\npip install -qU  xorbits\n```\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/unstructured_file.mdx",
    "filename": "unstructured_file.mdx",
    "size_bytes": 20793,
    "line_count": 357,
    "preview": "---\ntitle: Unstructured\n---\n\nThis notebook covers how to use `Unstructured` [document loader](https://python.langchain.com/docs/concepts/document_loaders) to load files of many types. `Unstructured` currently supports loading of text files, powerpoints, html, pdfs, images, and more.\n\nPlease see [this guide](/oss/integrations/providers/unstructured) for more instructions on setting up Unstructured locally, including setting up required system dependencies.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/slack.mdx",
    "filename": "slack.mdx",
    "size_bytes": 1222,
    "line_count": 34,
    "preview": "---\ntitle: Slack\n---\n\n>[Slack](https://slack.com/) is an instant messaging program.\n\nThis notebook covers how to load documents from a Zipfile generated from a `Slack` export.\n\nIn order to get this `Slack` export, follow these instructions:\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/chatgpt_loader.mdx",
    "filename": "chatgpt_loader.mdx",
    "size_bytes": 824,
    "line_count": 23,
    "preview": "---\ntitle: ChatGPT Data\n---\n\nThis notebook covers how to load `conversations.json` from your `ChatGPT` data export folder.\n\nYou can get your data export by email by going to: [chat.openai.com/](https://chat.openai.com/) -> (Profile) - Settings -> Export data -> Confirm export.\n\n```python\nfrom langchain_community.document_loaders.chatgpt import ChatGPTLoader\n"
  }
,
  {
    "path": "python/integrations/document_loaders/hacker_news.mdx",
    "filename": "hacker_news.mdx",
    "size_bytes": 1203,
    "line_count": 36,
    "preview": "---\ntitle: Hacker News\n---\n\n>[Hacker News](https://en.wikipedia.org/wiki/Hacker_News) (sometimes abbreviated as `HN`) is a social news website focusing on computer science and entrepreneurship. It is run by the investment fund and startup incubator `Y Combinator`. In general, content that can be submitted is defined as \"anything that gratifies one's intellectual curiosity.\"\n\nThis notebook covers how to pull page data and comments from [Hacker News](https://news.ycombinator.com/)\n\n```python\nfrom langchain_community.document_loaders import HNLoader\n"
  }
,
  {
    "path": "python/integrations/document_loaders/amazon_textract.mdx",
    "filename": "amazon_textract.mdx",
    "size_bytes": 9619,
    "line_count": 148,
    "preview": "---\ntitle: Amazon Textract\n---\n\n>[Amazon Textract](https://docs.aws.amazon.com/managedservices/latest/userguide/textract.html) is a machine learning (ML) service that automatically extracts text, handwriting, and data from scanned documents.\n>\n>It goes beyond simple optical character recognition (OCR) to identify, understand, and extract data from forms and tables. Today, many companies manually extract data from scanned documents such as PDFs, images, tables, and forms, or through simple OCR software that requires manual configuration (which often must be updated when the form changes). To overcome these manual and expensive processes, `Textract` uses ML to read and process any type of document, accurately extracting text, handwriting, tables, and other data with no manual effort.\n\n`Textract` supports `JPEG`, `PNG`, `PDF`, and `TIFF` file formats; more information is available in [the documentation](https://docs.aws.amazon.com/textract/latest/dg/limits-document.html).\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/airbyte_cdk.mdx",
    "filename": "airbyte_cdk.mdx",
    "size_bytes": 4198,
    "line_count": 101,
    "preview": "---\ntitle: Airbyte CDK (Deprecated)\n---\n\nNote: `AirbyteCDKLoader` is deprecated. Please use [`AirbyteLoader`](/oss/integrations/document_loaders/airbyte) instead.\n\n>[Airbyte](https://github.com/airbytehq/airbyte) is a data integration platform for ELT pipelines from APIs, databases & files to warehouses & lakes. It has the largest catalog of ELT connectors to data warehouses and databases.\n\nA lot of source connectors are implemented using the [Airbyte CDK](https://docs.airbyte.com/connector-development/cdk-python/). This loader allows to run any of these connectors and return the data as documents.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/docusaurus.mdx",
    "filename": "docusaurus.mdx",
    "size_bytes": 35905,
    "line_count": 117,
    "preview": "---\ntitle: Docusaurus\n---\n\n> [Docusaurus](https://docusaurus.io/) is a static-site generator which provides out-of-the-box documentation features.\n\nBy utilizing the existing `SitemapLoader`, this loader scans and loads all pages from a given Docusaurus application and returns the main documentation content of each page as a Document.\n\n```python\nfrom langchain_community.document_loaders import DocusaurusLoader\n"
  }
,
  {
    "path": "python/integrations/document_loaders/bibtex.mdx",
    "filename": "bibtex.mdx",
    "size_bytes": 4011,
    "line_count": 93,
    "preview": "---\ntitle: BibTeX\n---\n\n>[BibTeX](https://www.ctan.org/pkg/bibtex) is a file format and reference management system commonly used in conjunction with `LaTeX` typesetting. It serves as a way to organize and store bibliographic information for academic and research documents.\n\n`BibTeX` files have a `.bib` extension and consist of plain text entries representing references to various publications, such as books, articles, conference papers, theses, and more. Each `BibTeX` entry follows a specific structure and contains fields for different bibliographic details like author names, publication title, journal or book title, year of publication, page numbers, and more.\n\nBibTeX files can also store the path to documents, such as `.pdf` files that can be retrieved.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/airbyte_shopify.mdx",
    "filename": "airbyte_shopify.mdx",
    "size_bytes": 3426,
    "line_count": 93,
    "preview": "---\ntitle: Airbyte Shopify (Deprecated)\n---\n\nNote: This connector-specific loader is deprecated. Please use [`AirbyteLoader`](/oss/integrations/document_loaders/airbyte) instead.\n\n>[Airbyte](https://github.com/airbytehq/airbyte) is a data integration platform for ELT pipelines from APIs, databases & files to warehouses & lakes. It has the largest catalog of ELT connectors to data warehouses and databases.\n\nThis loader exposes the Shopify connector as a document loader, allowing you to load various Shopify objects as documents.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/youtube_audio.mdx",
    "filename": "youtube_audio.mdx",
    "size_bytes": 7145,
    "line_count": 158,
    "preview": "---\ntitle: YouTube audio\n---\n\nBuilding chat or QA applications on YouTube videos is a topic of high interest.\n\nBelow we show how to easily go from a `YouTube url` to `audio of the video` to `text` to `chat`!\n\nWe wil use the `OpenAIWhisperParser`, which will use the OpenAI Whisper API to transcribe audio to text,\nand the  `OpenAIWhisperParserLocal` for local support and running on private clouds or on premise.\n"
  }
,
  {
    "path": "python/integrations/document_loaders/google_datastore.mdx",
    "filename": "google_datastore.mdx",
    "size_bytes": 5969,
    "line_count": 177,
    "preview": "---\ntitle: Google Firestore in Datastore Mode\n---\n\n> [Firestore in Datastore Mode](https://cloud.google.com/datastore) is a NoSQL document database built for automatic scaling, high performance and ease of application development. Extend your database application to build AI-powered experiences leveraging Datastore's LangChain integrations.\n\nThis notebook goes over how to use [Firestore in Datastore Mode](https://cloud.google.com/datastore) to [save, load and delete langchain documents](/oss/integrations/document_loaders) with `DatastoreLoader` and `DatastoreSaver`.\n\nLearn more about the package on [GitHub](https://github.com/googleapis/langchain-google-datastore-python/).\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/scrapfly.mdx",
    "filename": "scrapfly.mdx",
    "size_bytes": 2154,
    "line_count": 56,
    "preview": "---\ntitle: ScrapFly\n---\n\n[ScrapFly](https://scrapfly.io/) is a web scraping API with headless browser capabilities, proxies, and anti-bot bypass. It allows for extracting web page data into accessible LLM markdown or text.\n\n#### Installation\n\nInstall ScrapFly Python SDK and he required LangChain packages using pip:\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/cassandra.mdx",
    "filename": "cassandra.mdx",
    "size_bytes": 3379,
    "line_count": 90,
    "preview": "---\ntitle: Cassandra\n---\n\n[Cassandra](https://cassandra.apache.org/) is a NoSQL, row-oriented, highly scalable and highly available database.Starting with version 5.0, the database ships with [vector search capabilities](https://cassandra.apache.org/doc/trunk/cassandra/vector-search/overview.html).\n\n## Overview\n\nThe Cassandra Document Loader returns a list of LangChain Documents from a Cassandra database.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/hugging_face_dataset.mdx",
    "filename": "hugging_face_dataset.mdx",
    "size_bytes": 21352,
    "line_count": 99,
    "preview": "---\ntitle: HuggingFace dataset\n---\n\n>The [Hugging Face Hub](https://huggingface.co/docs/hub/index) is home to over 5,000 [datasets](https://huggingface.co/docs/hub/index#datasets) in more than 100 languages that can be used for a broad range of tasks across NLP, Computer Vision, and Audio. They used for a diverse range of tasks such as translation,\nautomatic speech recognition, and image classification.\n\nThis notebook shows how to load `Hugging Face Hub` datasets to LangChain.\n\n```python\n"
  }
,
  {
    "path": "python/integrations/document_loaders/joplin.mdx",
    "filename": "joplin.mdx",
    "size_bytes": 1290,
    "line_count": 34,
    "preview": "---\ntitle: Joplin\n---\n\n>[Joplin](https://joplinapp.org/) is an open-source note-taking app. Capture your thoughts and securely access them from any device.\n\nThis notebook covers how to load documents from a `Joplin` database.\n\n`Joplin` has a [REST API](https://joplinapp.org/api/references/rest_api/) for accessing its local database. This loader uses the API to retrieve all notes in the database and their metadata. This requires an access token that can be obtained from the app by following these steps:\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/google_cloud_storage_file.mdx",
    "filename": "google_cloud_storage_file.mdx",
    "size_bytes": 1657,
    "line_count": 47,
    "preview": "---\ntitle: Google Cloud Storage File\n---\n\n>[Google Cloud Storage](https://en.wikipedia.org/wiki/Google_Cloud_Storage) is a managed service for storing unstructured data.\n\nThis covers how to load document objects from an `Google Cloud Storage (GCS) file object (blob)`.\n\n```python\npip install -qU  langchain-google-community[gcs]\n"
  }
,
  {
    "path": "python/integrations/document_loaders/acreom.mdx",
    "filename": "acreom.mdx",
    "size_bytes": 679,
    "line_count": 21,
    "preview": "---\ntitle: acreom\n---\n\n[acreom](https://acreom.com) is a dev-first knowledge base with tasks running on local markdown files.\n\nBelow is an example on how to load a local acreom vault into LangChain. As the local vault in acreom is a folder of plain text .md files, the loader requires the path to the directory.\n\nVault files may contain some metadata which is stored as a YAML header. These values will be added to the document’s metadata if `collect_metadata` is set to true.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/pymupdf.mdx",
    "filename": "pymupdf.mdx",
    "size_bytes": 19527,
    "line_count": 529,
    "preview": "---\ntitle: PyMuPDFLoader\n---\n\nThis notebook provides a quick overview for getting started with `PyMuPDF` [document loader](https://python.langchain.com/docs/concepts/document_loaders). For detailed documentation of all __ModuleName__Loader features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyMuPDFLoader.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/google_cloud_sql_mysql.mdx",
    "filename": "google_cloud_sql_mysql.mdx",
    "size_bytes": 14981,
    "line_count": 372,
    "preview": "---\ntitle: Google Cloud SQL for MySQL\n---\n\n> [Cloud SQL](https://cloud.google.com/sql) is a fully managed relational database service that offers high performance, seamless integration, and impressive scalability. It offers [MySQL](https://cloud.google.com/sql/mysql), [PostgreSQL](https://cloud.google.com/sql/postgresql), and [SQL Server](https://cloud.google.com/sql/sqlserver) database engines. Extend your database application to build AI-powered experiences leveraging Cloud SQL's LangChain integrations.\n\nThis notebook goes over how to use [Cloud SQL for MySQL](https://cloud.google.com/sql/mysql) to [save, load and delete langchain documents](/oss/integrations/document_loaders) with `MySQLLoader` and `MySQLDocumentSaver`.\n\nLearn more about the package on [GitHub](https://github.com/googleapis/langchain-google-cloud-sql-mysql-python/).\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/aws_s3_file.mdx",
    "filename": "aws_s3_file.mdx",
    "size_bytes": 1316,
    "line_count": 46,
    "preview": "---\ntitle: AWS S3 File\n---\n\n>[Amazon Simple Storage Service (Amazon S3)](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html) is an object storage service.\n\n>[AWS S3 Buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingBucket.html)\n\nThis covers how to load document objects from an `AWS S3 File` object.\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/polaris_ai_datainsight.mdx",
    "filename": "polaris_ai_datainsight.mdx",
    "size_bytes": 9878,
    "line_count": 166,
    "preview": "---\ntitle: PolarisAIDataInsightLoader\n---\n\n> [Polaris AI DataInsight](https://datainsight.polarisoffice.com/playground) is a document parser\n> that extracts document elements (text, images, complex tables, charts, etc.) from various file formats\n> into structured JSON, making them easy to integrate into RAG systems.\n\n## Installation\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/dedoc.mdx",
    "filename": "dedoc.mdx",
    "size_bytes": 11301,
    "line_count": 257,
    "preview": "---\ntitle: Dedoc\n---\n\nThis sample demonstrates the use of `Dedoc` in combination with `LangChain` as a `DocumentLoader`.\n\n## Overview\n\n[Dedoc](https://dedoc.readthedocs.io) is an [open-source](https://github.com/ispras/dedoc)\nlibrary/service that extracts texts, tables, attached files and document structure\n"
  }
,
  {
    "path": "python/integrations/document_loaders/recursive_url.mdx",
    "filename": "recursive_url.mdx",
    "size_bytes": 7874,
    "line_count": 187,
    "preview": "---\ntitle: Recursive URL\n---\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/google_firestore.mdx",
    "filename": "google_firestore.mdx",
    "size_bytes": 6607,
    "line_count": 208,
    "preview": "---\ntitle: Google Firestore (Native Mode)\n---\n\n> [Firestore](https://cloud.google.com/firestore) is a serverless document-oriented database that scales to meet any demand. Extend your database application to build AI-powered experiences leveraging Firestore's LangChain integrations.\n\nThis notebook goes over how to use [Firestore](https://cloud.google.com/firestore) to [save, load and delete langchain documents](/oss/integrations/document_loaders) with `FirestoreLoader` and `FirestoreSaver`.\n\nLearn more about the package on [GitHub](https://github.com/googleapis/langchain-google-firestore-python/).\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/microsoft_excel.mdx",
    "filename": "microsoft_excel.mdx",
    "size_bytes": 5264,
    "line_count": 64,
    "preview": "---\ntitle: Microsoft Excel\n---\n\nThe `UnstructuredExcelLoader` is used to load `Microsoft Excel` files. The loader works with both `.xlsx` and `.xls` files. The page content will be the raw text of the Excel file. If you use the loader in `\"elements\"` mode, an HTML representation of the Excel file will be available in the document metadata under the `text_as_html` key.\n\nPlease see [this guide](/oss/integrations/providers/unstructured/) for more instructions on setting up Unstructured locally, including setting up required system dependencies.\n\n```python\npip install -qU langchain-community unstructured openpyxl\n"
  }
,
  {
    "path": "python/integrations/document_loaders/tensorflow_datasets.mdx",
    "filename": "tensorflow_datasets.mdx",
    "size_bytes": 8053,
    "line_count": 158,
    "preview": "---\ntitle: TensorFlow Datasets\n---\n\n>[TensorFlow Datasets](https://www.tensorflow.org/datasets) is a collection of datasets ready to use, with TensorFlow or other Python ML frameworks, such as Jax. All datasets are exposed as [tf.data.Datasets](https://www.tensorflow.org/api_docs/python/tf/data/Dataset), enabling easy-to-use and high-performance input pipelines. To get started see the [guide](https://www.tensorflow.org/datasets/overview) and the [list of datasets](https://www.tensorflow.org/datasets/catalog/overview#all_datasets).\n\nThis notebook shows how to load `TensorFlow Datasets` into a Document format that we can use downstream.\n\n## Installation\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/google_bigquery.mdx",
    "filename": "google_bigquery.mdx",
    "size_bytes": 3515,
    "line_count": 112,
    "preview": "---\ntitle: Google BigQuery\n---\n\n>[Google BigQuery](https://cloud.google.com/bigquery) is a serverless and cost-effective enterprise data warehouse that works across clouds and scales with your data.\n`BigQuery` is a part of the `Google Cloud Platform`.\n\nLoad a `BigQuery` query with one document per row.\n\n```python\n"
  }
,
  {
    "path": "python/integrations/document_loaders/url.mdx",
    "filename": "url.mdx",
    "size_bytes": 46606,
    "line_count": 108,
    "preview": "---\ntitle: URL\n---\n\nThis example covers how to load `HTML` documents from a list of `URLs` into the @[`Document`] format that we can use downstream.\n\n## Unstructured URL loader\n\nFor the examples below, please install the `unstructured` library and see [this guide](/oss/integrations/providers/unstructured/) for more instructions on setting up Unstructured locally, including setting up required system dependencies:\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/roam.mdx",
    "filename": "roam.mdx",
    "size_bytes": 1013,
    "line_count": 33,
    "preview": "---\ntitle: Roam\n---\n\n>[ROAM](https://roamresearch.com/) is a note-taking tool for networked thought, designed to create a personal knowledge base.\n\nThis notebook covers how to load documents from a Roam database. This takes a lot of inspiration from the example repo [here](https://github.com/JimmyLv/roam-qa).\n\n## 🧑 Instructions for ingesting your own dataset\n\n"
  }
,
  {
    "path": "python/integrations/document_loaders/scrapingant.mdx",
    "filename": "scrapingant.mdx",
    "size_bytes": 33046,
    "line_count": 93,
    "preview": "---\ntitle: ScrapingAnt\n---\n\n[ScrapingAnt](https://scrapingant.com/) is a web scraping API with headless browser capabilities, proxies, and anti-bot bypass. It allows for extracting web page data into accessible LLM markdown.\n\nThis particular integration uses only Markdown extraction feature, but don't hesitate to [reach out to us](mailto:support@scrapingant.com) if you need more features provided by ScrapingAnt, but not yet implemented in this integration.\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/graphs/azure_cosmosdb_gremlin.mdx",
    "filename": "azure_cosmosdb_gremlin.mdx",
    "size_bytes": 4268,
    "line_count": 146,
    "preview": "---\ntitle: Azure Cosmos DB for Apache Gremlin\n---\n\n>[Azure Cosmos DB for Apache Gremlin](https://learn.microsoft.com/en-us/azure/cosmos-db/gremlin/introduction) is a graph database service that can be used to store massive graphs with billions of vertices and edges. You can query the graphs with millisecond latency and evolve the graph structure easily.\n>\n>[Gremlin](https://en.wikipedia.org/wiki/Gremlin_(query_language)) is a graph traversal language and virtual machine developed by `Apache TinkerPop` of the `Apache Software Foundation`.\n\nThis notebook shows how to use LLMs to provide a natural language interface to a graph database you can query with the `Gremlin` query language.\n\n"
  }
,
  {
    "path": "python/integrations/graphs/networkx.mdx",
    "filename": "networkx.mdx",
    "size_bytes": 3717,
    "line_count": 166,
    "preview": "---\ntitle: NetworkX\n---\n\n>[NetworkX](https://networkx.org/) is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.\n\nThis notebook goes over how to do question answering over a graph data structure.\n\n## Setting up\n\n"
  }
,
  {
    "path": "python/integrations/graphs/diffbot.mdx",
    "filename": "diffbot.mdx",
    "size_bytes": 6054,
    "line_count": 160,
    "preview": "---\ntitle: Diffbot\n---\n\n>[Diffbot](https://docs.diffbot.com/docs/getting-started-with-diffbot) is a suite of ML-based products that make it easy to structure web data.\n>\n>Diffbot's [Natural Language Processing API](https://www.diffbot.com/products/natural-language/) allows for the extraction of entities, relationships, and semantic meaning from unstructured text data.\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/v0.3/docs/docs/integrations/graphs/diffbot.ipynb)\n\n## Use case\n"
  }
,
  {
    "path": "python/integrations/graphs/hugegraph.mdx",
    "filename": "hugegraph.mdx",
    "size_bytes": 4459,
    "line_count": 150,
    "preview": "---\ntitle: HugeGraph\n---\n\n>[HugeGraph](https://hugegraph.apache.org/) is a convenient, efficient, and adaptable graph database compatible with\n>the `Apache TinkerPop3` framework and the `Gremlin` query language.\n>\n>[Gremlin](https://en.wikipedia.org/wiki/Gremlin_(query_language)) is a graph traversal language and virtual machine developed by `Apache TinkerPop` of the `Apache Software Foundation`.\n\nThis notebook shows how to use LLMs to provide a natural language interface to [HugeGraph](https://hugegraph.apache.org/cn/) database.\n"
  }
,
  {
    "path": "python/integrations/graphs/apache_age.mdx",
    "filename": "apache_age.mdx",
    "size_bytes": 10806,
    "line_count": 383,
    "preview": "---\ntitle: Apache AGE\n---\n\n>[Apache AGE](https://age.apache.org/) is a PostgreSQL extension that provides graph database functionality. AGE is an acronym for A Graph Extension, and is inspired by Bitnine’s fork of PostgreSQL 10, AgensGraph, which is a multi-model database. The goal of the project is to create single storage that can handle both relational and graph model data so that users can use standard ANSI SQL along with openCypher, the Graph query language. The data elements `Apache AGE` stores are nodes, edges connecting them, and attributes of nodes and edges.\n\n>This notebook shows how to use LLMs to provide a natural language interface to a graph database you can query with the `Cypher` query language.\n\n>[Cypher](https://en.wikipedia.org/wiki/Cypher_(query_language)) is a declarative graph query language that allows for expressive and efficient data querying in a property graph.\n\n"
  }
,
  {
    "path": "python/integrations/graphs/arangodb.mdx",
    "filename": "arangodb.mdx",
    "size_bytes": 11567,
    "line_count": 455,
    "preview": "---\ntitle: ArangoDB\n---\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/arangodb/interactive_tutorials/blob/master/notebooks/LangChain.ipynb)\n\n>[ArangoDB](https://github.com/arangodb/arangodb) is a scalable graph database system to drive value from\n>connected data, faster. Native graphs, an integrated search engine, and JSON support, via\n>a single query language. `ArangoDB` runs on-prem or in the cloud.\n\n"
  }
,
  {
    "path": "python/integrations/graphs/amazon_neptune_open_cypher.mdx",
    "filename": "amazon_neptune_open_cypher.mdx",
    "size_bytes": 5925,
    "line_count": 148,
    "preview": "---\ntitle: Amazon Neptune with Cypher\n---\n\n>[Amazon Neptune](https://aws.amazon.com/neptune/) is a high-performance graph analytics and serverless database for superior scalability and availability.\n>\n>This example shows the QA chain that queries the `Neptune` graph database using `openCypher` and returns a human-readable response.\n>\n>[Cypher](https://en.wikipedia.org/wiki/Cypher_(query_language)) is a declarative graph query language that allows for expressive and efficient data querying in a property graph.\n>\n"
  }
,
  {
    "path": "python/integrations/graphs/memgraph.mdx",
    "filename": "memgraph.mdx",
    "size_bytes": 23219,
    "line_count": 482,
    "preview": "---\ntitle: Memgraph\n---\n\nMemgraph is an open-source graph database, tuned for dynamic analytics environments and compatible with Neo4j. To query the database, Memgraph uses Cypher - the most widely adopted, fully-specified, and open query language for property graph databases.\n\nThis notebook will show you how to [query Memgraph with natural language](#natural-language-querying) and how to [construct a knowledge graph](#constructing-knowledge-graph) from your unstructured data.\n\nBut first, make sure to [set everything up](#setting-up).\n\n"
  }
,
  {
    "path": "python/integrations/graphs/timbr.mdx",
    "filename": "timbr.mdx",
    "size_bytes": 8726,
    "line_count": 213,
    "preview": "---\ntitle: Timbr\n---\n\n>[Timbr](https://docs.timbr.ai/doc/docs/integration/langchain-sdk/) integrates natural language inputs with Timbr's ontology-driven semantic layer. Leveraging Timbr's robust ontology capabilities, the SDK integrates with Timbr data models and leverages semantic relationships and annotations, enabling users to query data using business-friendly language.\n\n>Timbr provides a pre-built SQL agent, `TimbrSqlAgent`, which can be used for end-to-end purposes from user prompt, through semantic SQL query generation and validation, to query execution and result analysis.\n\n>For customizations and partial usage, you can use LangChain chains and LangGraph nodes with our 5 main tools:\n\n"
  }
,
  {
    "path": "python/integrations/graphs/tigergraph.mdx",
    "filename": "tigergraph.mdx",
    "size_bytes": 1329,
    "line_count": 44,
    "preview": "---\ntitle: TigerGraph\n---\n\n>[TigerGraph](https://www.tigergraph.com/tigergraph-db/) is a natively distributed and high-performance graph database.\n> The storage of data in a graph format of vertices and edges leads to rich relationships,\n> ideal for grouding LLM responses.\n\nA big example of the `TigerGraph` and `LangChain` integration [presented here](https://github.com/tigergraph/graph-ml-notebooks/blob/main/applications/large_language_models/TigerGraph_LangChain_Demo.ipynb).\n\n"
  }
,
  {
    "path": "python/integrations/graphs/kuzu_db.mdx",
    "filename": "kuzu_db.mdx",
    "size_bytes": 8225,
    "line_count": 213,
    "preview": "---\ntitle: Kuzu\n---\n\n> [Kùzu](https://kuzudb.com/) is an embeddable, scalable, extremely fast graph database.\n> It is permissively licensed with an MIT license, and you can see its source code [here](https://github.com/kuzudb/kuzu).\n\n> Key characteristics of Kùzu:\n>\n>- Performance and scalability: Implements modern, state-of-the-art join algorithms for graphs.\n"
  }
,
  {
    "path": "python/integrations/graphs/rdflib_sparql.mdx",
    "filename": "rdflib_sparql.mdx",
    "size_bytes": 11744,
    "line_count": 200,
    "preview": "---\ntitle: RDFLib\n---\n\n>[RDFLib](https://rdflib.readthedocs.io/) is a pure Python package for working with [RDF](https://en.wikipedia.org/wiki/Resource_Description_Framework). `RDFLib` contains most things you need to work with `RDF`, including:\n>\n>- parsers and serializers for RDF/XML, N3, NTriples, N-Quads, Turtle, TriX, Trig and JSON-LD\n>- a Graph interface which can be backed by any one of a number of Store implementations\n>- store implementations for in-memory, persistent on disk (Berkeley DB) and remote SPARQL endpoints\n>- a SPARQL 1.1 implementation - supporting SPARQL 1.1 Queries and Update statements\n"
  }
,
  {
    "path": "python/integrations/graphs/ontotext.mdx",
    "filename": "ontotext.mdx",
    "size_bytes": 14246,
    "line_count": 375,
    "preview": "---\ntitle: Ontotext GraphDB\n---\n\n>[Ontotext GraphDB](https://graphdb.ontotext.com/) is a graph database and knowledge discovery tool compliant with [RDF](https://www.w3.org/RDF/) and [SPARQL](https://www.w3.org/TR/sparql11-query/).\n\n>This notebook shows how to use LLMs to provide natural language querying (NLQ to SPARQL, also called `text2sparql`) for `Ontotext GraphDB`.\n\n## GraphDB LLM functionalities\n\n"
  }
,
  {
    "path": "python/integrations/graphs/neo4j_cypher.mdx",
    "filename": "neo4j_cypher.mdx",
    "size_bytes": 13456,
    "line_count": 474,
    "preview": "---\ntitle: Neo4j\n---\n\n>[Neo4j](https://neo4j.com/docs/getting-started/) is a graph database management system developed by `Neo4j, Inc`.\n\n>The data elements `Neo4j` stores are nodes, edges connecting them, and attributes of nodes and edges. Described by its developers as an ACID-compliant transactional database with native graph storage and processing, `Neo4j` is available in a non-open-source \"community edition\" licensed with a modification of the GNU General Public License, with online backup and high availability extensions licensed under a closed-source commercial license. Neo also licenses `Neo4j` with these extensions under closed-source commercial terms.\n\n>This notebook shows how to use LLMs to provide a natural language interface to a graph database you can query with the `Cypher` query language.\n\n"
  }
,
  {
    "path": "python/integrations/graphs/falkordb.mdx",
    "filename": "falkordb.mdx",
    "size_bytes": 3478,
    "line_count": 145,
    "preview": "---\ntitle: FalkorDB\n---\n\n>[FalkorDB](https://www.falkordb.com/) is a low-latency Graph Database that delivers knowledge to GenAI.\n\nThis notebook shows how to use LLMs to provide a natural language interface to `FalkorDB` database.\n\n## Setting up\n\n"
  }
,
  {
    "path": "python/integrations/llms/cloudflare_workersai.mdx",
    "filename": "cloudflare_workersai.mdx",
    "size_bytes": 3560,
    "line_count": 56,
    "preview": "---\ntitle: Cloudflare Workers AI\n---\n\n[Cloudflare AI documentation](https://developers.cloudflare.com/workers-ai/models/) listed all generative text models available.\n\nBoth Cloudflare account ID and API token are required. Find how to obtain them from [this document](https://developers.cloudflare.com/workers-ai/get-started/rest-api/).\n\n```python\nfrom langchain_classic.chains import LLMChain\n"
  }
,
  {
    "path": "python/integrations/llms/azure_openai.mdx",
    "filename": "azure_openai.mdx",
    "size_bytes": 5758,
    "line_count": 144,
    "preview": "---\ntitle: Azure OpenAI\n---\n\n<Warning>\n**You are currently on a page documenting the use of Azure OpenAI text completion models. The latest and most popular Azure OpenAI models are [chat completion models](/oss/langchain/models).**\n\nUnless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/oss/integrations/chat/azure_chat_openai/).\n</Warning>\n\n"
  }
,
  {
    "path": "python/integrations/llms/promptlayer_openai.mdx",
    "filename": "promptlayer_openai.mdx",
    "size_bytes": 2656,
    "line_count": 91,
    "preview": "---\ntitle: PromptLayer OpenAI\n---\n\n`PromptLayer` is the first platform that allows you to track, manage, and share your GPT prompt engineering. `PromptLayer` acts a middleware between your code and `OpenAI’s` python library.\n\n`PromptLayer` records all your `OpenAI API` requests, allowing you to search and explore request history in the `PromptLayer` dashboard.\n\nThis example showcases how to connect to [PromptLayer](https://www.promptlayer.com) to start recording your OpenAI requests.\n\n"
  }
,
  {
    "path": "python/integrations/llms/manifest.mdx",
    "filename": "manifest.mdx",
    "size_bytes": 3786,
    "line_count": 109,
    "preview": "---\ntitle: Manifest\n---\n\nThis notebook goes over how to use Manifest and LangChain.\n\nFor more detailed information on `manifest`, and how to use it with local huggingface models like in this example, see [github.com/HazyResearch/manifest](https://github.com/HazyResearch/manifest)\n\nAnother example of [using Manifest with LangChain](https://github.com/HazyResearch/manifest/blob/main/examples/langchain_chatgpt.html).\n\n"
  }
,
  {
    "path": "python/integrations/llms/baseten.mdx",
    "filename": "baseten.mdx",
    "size_bytes": 4476,
    "line_count": 106,
    "preview": "---\ntitle: Baseten\n---\n\n[Baseten](https://baseten.co) is a [Provider](/oss/integrations/providers/baseten) in the LangChain ecosystem that implements the LLMs component.\n\nThis example demonstrates using an LLM — Mistral 7B hosted on Baseten — with LangChain.\n\n# Setup\n\n"
  }
,
  {
    "path": "python/integrations/llms/amazon_api_gateway.mdx",
    "filename": "amazon_api_gateway.mdx",
    "size_bytes": 3416,
    "line_count": 131,
    "preview": "---\ntitle: Amazon API Gateway\n---\n\n>[Amazon API Gateway](https://aws.amazon.com/api-gateway/) is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any >scale. APIs act as the \"front door\" for applications to access data, business logic, or functionality from your backend services. Using `API Gateway`, you can create RESTful APIs and >WebSocket APIs that enable real-time two-way communication applications. API Gateway supports containerized and serverless workloads, as well as web applications.\n\n>`API Gateway` handles all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including traffic management, CORS support, authorization >and access control, throttling, monitoring, and API version management. `API Gateway` has no minimum fees or startup costs. You pay for the API calls you receive and the amount of data >transferred out and, with the `API Gateway` tiered pricing model, you can reduce your cost as your API usage scales.\n\n```python\n##Installing the langchain packages needed to use the integration\n"
  }
,
  {
    "path": "python/integrations/llms/forefrontai.mdx",
    "filename": "forefrontai.mdx",
    "size_bytes": 1568,
    "line_count": 69,
    "preview": "---\ntitle: ForefrontAI\n---\n\nThe `Forefront` platform gives you the ability to fine-tune and use [open-source Large Language Models (LLMs)](https://docs.forefront.ai/get-started/models).\n\nThis notebook goes over how to use LangChain with [ForefrontAI](https://www.forefront.ai/).\n\n## Imports\n\n"
  }
,
  {
    "path": "python/integrations/llms/volcengine_maas.mdx",
    "filename": "volcengine_maas.mdx",
    "size_bytes": 1105,
    "line_count": 36,
    "preview": "---\ntitle: Volc Engine Maas\n---\n\nThis guide provides you with a guide on how to get started with Volc Engine's MaaS llm models.\n\n```python\n# Install the package\npip install -qU  volcengine\n```\n"
  }
,
  {
    "path": "python/integrations/llms/google_generative_ai.mdx",
    "filename": "google_generative_ai.mdx",
    "size_bytes": 8435,
    "line_count": 170,
    "preview": "---\ntitle: GoogleGenerativeAI\ndescription: Get started using legacy Gemini LLMs in LangChain.\n---\n\n<Warning>\n**You are currently on a page documenting the use of Google models as text completion models. Many popular Google models are [chat completion models](/oss/langchain/models).**\n\nYou may be looking for [this page instead](/oss/integrations/chat/google_generative_ai/).\n</Warning>\n"
  }
,
  {
    "path": "python/integrations/llms/ctransformers.mdx",
    "filename": "ctransformers.mdx",
    "size_bytes": 1101,
    "line_count": 56,
    "preview": "---\ntitle: C Transformers\n---\n\nThe [C Transformers](https://github.com/marella/ctransformers) library provides Python bindings for GGML models.\n\nThis example goes over how to use LangChain to interact with `C Transformers` [models](https://github.com/marella/ctransformers#supported-models).\n\n**Install**\n\n"
  }
,
  {
    "path": "python/integrations/llms/oci_generative_ai.mdx",
    "filename": "oci_generative_ai.mdx",
    "size_bytes": 4111,
    "line_count": 100,
    "preview": "---\ntitle: Oracle Cloud Infrastructure Generative AI\n---\n\nOracle Cloud Infrastructure (OCI) Generative AI is a fully managed service that provides a set of state-of-the-art, customizable Large Language Models (LLMs) that cover a wide range of use cases, and which is available through a single API.\nUsing the OCI Generative AI service you can access ready-to-use pretrained models, or create and host your own fine-tuned custom models based on your own data on dedicated AI clusters. Detailed documentation of the service and API is available __[here](https://docs.oracle.com/en-us/iaas/Content/generative-ai/home.htm)__ and __[here](https://docs.oracle.com/en-us/iaas/api/#/en/generative-ai/20231130/)__.\n\nThis notebook explains how to use OCI's Generative AI complete models with LangChain.\n\n## Setup\n"
  }
,
  {
    "path": "python/integrations/llms/deepsparse.mdx",
    "filename": "deepsparse.mdx",
    "size_bytes": 1212,
    "line_count": 34,
    "preview": "---\ntitle: DeepSparse\n---\n\nThis page covers how to use the [DeepSparse](https://github.com/neuralmagic/deepsparse) inference runtime within LangChain.\nIt is broken into two parts: installation and setup, and then examples of DeepSparse usage.\n\n## Installation and setup\n\n- Install the Python package with `pip install deepsparse`\n"
  }
,
  {
    "path": "python/integrations/llms/ipex_llm.mdx",
    "filename": "ipex_llm.mdx",
    "size_bytes": 9269,
    "line_count": 248,
    "preview": "---\ntitle: IPEX-LLM\n---\n\n> [IPEX-LLM](https://github.com/intel-analytics/ipex-llm) is a PyTorch library for running LLM on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max) with very low latency.\n\n- [IPEX-LLM on Intel GPU](#ipex-llm-on-intel-gpu)\n- [IPEX-LLM on Intel CPU](#ipex-llm-on-intel-cpu)\n\n## IPEX-LLM on intel GPU\n"
  }
,
  {
    "path": "python/integrations/llms/cerebriumai.mdx",
    "filename": "cerebriumai.mdx",
    "size_bytes": 1697,
    "line_count": 70,
    "preview": "---\ntitle: CerebriumAI\n---\n\n`Cerebrium` is an AWS Sagemaker alternative. It also provides API access to [several LLM models](https://docs.cerebrium.ai/cerebrium/prebuilt-models/deployment).\n\nThis notebook goes over how to use LangChain with [CerebriumAI](https://docs.cerebrium.ai/introduction).\n\n## Install cerebrium\n\n"
  }
,
  {
    "path": "python/integrations/llms/sambanovacloud.mdx",
    "filename": "sambanovacloud.mdx",
    "size_bytes": 9105,
    "line_count": 152,
    "preview": "---\ntitle: SambaNovaCloud\n---\n\n<Warning>\n**You are currently on a page documenting the use of SambaNovaCloud models as text completion models. We recommend you to use the [chat completion models](/oss/langchain/models).**\n\nYou may be looking for [SambaNova Chat Models](/oss/integrations/chat/sambanova/) .\n</Warning>\n\n"
  }
,
  {
    "path": "python/integrations/llms/oci_model_deployment_endpoint.mdx",
    "filename": "oci_model_deployment_endpoint.mdx",
    "size_bytes": 5410,
    "line_count": 134,
    "preview": "---\ntitle: OCI Data Science Model Deployment Endpoint\n---\n\n[OCI Data Science](https://docs.oracle.com/en-us/iaas/data-science/using/home.htm) is a fully managed and serverless platform for data science teams to build, train, and manage machine learning models in the Oracle Cloud Infrastructure.\n\n> For the latest updates, examples and experimental features, please see [ADS LangChain Integration](https://accelerated-data-science.readthedocs.io/en/latest/user_guide/large_language_model/langchain_models.html).\n\nThis notebooks goes over how to use an LLM hosted on a [OCI Data Science Model Deployment](https://docs.oracle.com/en-us/iaas/data-science/using/model-dep-about.htm).\n\n"
  }
,
  {
    "path": "python/integrations/llms/aimlapi.mdx",
    "filename": "aimlapi.mdx",
    "size_bytes": 3828,
    "line_count": 100,
    "preview": "---\ntitle: AimlapiLLM\n---\n\n<Warning>\n**You are currently on a page documenting the use of AI/ML API models as text completion models. Many of the latest and most popular AI/ML API models are [chat completion models](/oss/langchain/models).**\n\nYou may be looking for [this page instead](/oss/integrations/chat/aimlapi).\n</Warning>\n\n"
  }
,
  {
    "path": "python/integrations/llms/javelin.mdx",
    "filename": "javelin.mdx",
    "size_bytes": 5830,
    "line_count": 156,
    "preview": "---\ntitle: Javelin AI Gateway Tutorial\n---\n\nThis Jupyter Notebook will explore how to interact with the Javelin AI Gateway using the Python SDK.\nThe Javelin AI Gateway facilitates the utilization of Large Language Models (LLMs) like OpenAI, Cohere, Anthropic, and others by\nproviding a secure and unified endpoint. The gateway itself provides a centralized mechanism to roll out models systematically,\nprovide access security, policy & cost guardrails for enterprises, etc.,\n\nFor a complete listing of all the features & benefits of Javelin, please visit [www.getjavelin.io](https://www.getjavelin.io)\n"
  }
,
  {
    "path": "python/integrations/llms/beam.mdx",
    "filename": "beam.mdx",
    "size_bytes": 1918,
    "line_count": 69,
    "preview": "---\ntitle: Beam\n---\n\nCalls the Beam API wrapper to deploy and make subsequent calls to an instance of the gpt2 LLM in a cloud deployment. Requires installation of the Beam library and registration of Beam Client ID and Client Secret. By calling the wrapper an instance of the model is created and run, with returned text relating to the prompt. Additional calls can then be made by directly calling the Beam API.\n\n[Create an account](https://www.beam.cloud/), if you don't have one already. Grab your API keys from the [dashboard](https://www.beam.cloud/dashboard/settings/api-keys).\n\nInstall the Beam CLI\n\n"
  }
,
  {
    "path": "python/integrations/llms/openlm.mdx",
    "filename": "openlm.mdx",
    "size_bytes": 2144,
    "line_count": 72,
    "preview": "---\ntitle: OpenLM\n---\n\n[OpenLM](https://github.com/r2d4/openlm) is a zero-dependency OpenAI-compatible LLM provider that can call different inference endpoints directly via HTTP.\n\nIt implements the OpenAI Completion class so that it can be used as a drop-in replacement for the OpenAI API. This changeset utilizes BaseOpenAI for minimal added code.\n\nThis example goes over how to use LangChain to interact with both OpenAI and HuggingFace. You'll need API keys from both.\n\n"
  }
,
  {
    "path": "python/integrations/llms/sparkllm.mdx",
    "filename": "sparkllm.mdx",
    "size_bytes": 1735,
    "line_count": 61,
    "preview": "---\ntitle: SparkLLM\n---\n\n[SparkLLM](https://xinghuo.xfyun.cn/spark) is a large-scale cognitive model independently developed by iFLYTEK.\nIt has cross-domain knowledge and language understanding ability by learning a large amount of texts, codes and images.\nIt can understand and perform tasks based on natural dialogue.\n\n## Prerequisite\n\n"
  }
,
  {
    "path": "python/integrations/llms/baidu_qianfan_endpoint.mdx",
    "filename": "baidu_qianfan_endpoint.mdx",
    "size_bytes": 6044,
    "line_count": 165,
    "preview": "---\ntitle: Baidu Qianfan\n---\n\nBaidu AI Cloud Qianfan Platform is a one-stop large model development and service operation platform for enterprise developers. Qianfan not only provides including the model of Wenxin Yiyan (ERNIE-Bot) and the third-party open-source models, but also provides various AI development tools and the whole set of development environment, which facilitates customers to use and develop large model applications easily.\n\nBasically, those model are split into the following type:\n\n- Embedding\n- Chat\n"
  }
,
  {
    "path": "python/integrations/llms/textgen.mdx",
    "filename": "textgen.mdx",
    "size_bytes": 2437,
    "line_count": 77,
    "preview": "---\ntitle: TextGen\n---\n\n[GitHub:oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) A gradio web UI for running Large Language Models like LLaMA, llama.cpp, GPT-J, Pythia, OPT, and GALACTICA.\n\nThis example goes over how to use LangChain to interact with LLM models via the `text-generation-webui` API integration.\n\nPlease ensure that you have `text-generation-webui` configured and an LLM installed.  Recommended installation via the [one-click installer appropriate](https://github.com/oobabooga/text-generation-webui#one-click-installers) for your OS.\n\n"
  }
,
  {
    "path": "python/integrations/llms/ai21.mdx",
    "filename": "ai21.mdx",
    "size_bytes": 2108,
    "line_count": 81,
    "preview": "---\ntitle: AI21LLM\n---\n\n:::caution This service is deprecated.\nSee [this page](https://python.langchain.com/docs/integrations/chat/ai21/) for the updated ChatAI21 object. :::\n\nThis example goes over how to use LangChain to interact with `AI21` Jurassic models. To use the Jamba model, use the [ChatAI21 object](https://python.langchain.com/docs/integrations/chat/ai21/) instead.\n\n[See a full list of AI21 models and tools on LangChain.](https://pypi.org/project/langchain-ai21/)\n"
  }
,
  {
    "path": "python/integrations/llms/openai.mdx",
    "filename": "openai.mdx",
    "size_bytes": 3963,
    "line_count": 123,
    "preview": "---\ntitle: OpenAI\n---\n\n<Warning>\n**You are currently on a page documenting the use of OpenAI text completion models. The latest and most popular OpenAI models are [chat completion models](/oss/langchain/models).**\n\nUnless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/oss/integrations/chat/openai/).\n</Warning>\n\n"
  }
,
  {
    "path": "python/integrations/llms/octoai.mdx",
    "filename": "octoai.mdx",
    "size_bytes": 2560,
    "line_count": 56,
    "preview": "---\ntitle: OctoAI\n---\n\n[OctoAI](https://docs.octoai.cloud/docs) offers easy access to efficient compute and enables users to integrate their choice of AI models into applications. The `OctoAI` compute service helps you run, tune, and scale AI applications easily.\n\nThis example goes over how to use LangChain to interact with `OctoAI` [LLM endpoints](https://octoai.cloud/templates)\n\n## Setup\n\n"
  }
,
  {
    "path": "python/integrations/llms/symblai_nebula.mdx",
    "filename": "symblai_nebula.mdx",
    "size_bytes": 3030,
    "line_count": 42,
    "preview": "---\ntitle: Nebula (Symbl.ai)\n---\n\n[Nebula](https://symbl.ai/nebula/) is a large language model (LLM) built by [Symbl.ai](https://symbl.ai). It is trained to perform generative tasks on human conversations. Nebula excels at modeling the nuanced details of a conversation and performing tasks on the conversation.\n\nNebula documentation: [docs.symbl.ai/docs/nebula-llm](https://docs.symbl.ai/docs/nebula-llm)\n\nThis example goes over how to use LangChain to interact with the [Nebula platform](https://docs.symbl.ai/docs/nebula-llm).\n\n"
  }
,
  {
    "path": "python/integrations/llms/writer.mdx",
    "filename": "writer.mdx",
    "size_bytes": 1709,
    "line_count": 74,
    "preview": "---\ntitle: WRITER LLM\n---\n\n[WRITER](https://writer.com/) is a platform to generate different language content.\n\nThis example goes over how to use LangChain to interact with `WRITER` [models](https://dev.writer.com/docs/models).\n\n## Setup\n\n"
  }
,
  {
    "path": "python/integrations/llms/baichuan.mdx",
    "filename": "baichuan.mdx",
    "size_bytes": 1068,
    "line_count": 54,
    "preview": "---\ntitle: Baichuan LLM\n---\n\nBaichuan Inc. ([www.baichuan-ai.com/](https://www.baichuan-ai.com/)) is a Chinese startup in the era of AGI, dedicated to addressing fundamental human needs: Efficiency, Health, and Happiness.\n\n```python\n##Installing the langchain packages needed to use the integration\npip install -qU langchain-community\n```\n"
  }
,
  {
    "path": "python/integrations/llms/databricks.mdx",
    "filename": "databricks.mdx",
    "size_bytes": 4942,
    "line_count": 136,
    "preview": "---\ntitle: Databricks\n---\n\n> [Databricks](https://www.databricks.com/) Lakehouse Platform unifies data, analytics, and AI on one platform.\n\nThis guide provides a quick overview for getting started with Databricks [LLM models](https://python.langchain.com/docs/concepts/text_llms). For detailed documentation of all features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/llms/langchain_community.llms.databricks.Databricks.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/llms/bedrock.mdx",
    "filename": "bedrock.mdx",
    "size_bytes": 3125,
    "line_count": 75,
    "preview": "---\ntitle: Bedrock\n---\n\n<Warning>\n**You are currently on a page documenting the use of Amazon Bedrock models as text completion models. Many popular models available on Bedrock are [chat completion models](/oss/langchain/models).**\n\nYou may be looking for [this page instead](/oss/integrations/chat/bedrock/).\n</Warning>\n\n"
  }
,
  {
    "path": "python/integrations/llms/sagemaker.mdx",
    "filename": "sagemaker.mdx",
    "size_bytes": 4964,
    "line_count": 165,
    "preview": "---\ntitle: SageMakerEndpoint\n---\n\n[Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a system that can build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n\nThis notebooks goes over how to use an LLM hosted on a `SageMaker endpoint`.\n\n```python\n!pip3 install langchain boto3\n"
  }
,
  {
    "path": "python/integrations/llms/mosaicml.mdx",
    "filename": "mosaicml.mdx",
    "size_bytes": 1098,
    "line_count": 47,
    "preview": "---\ntitle: MosaicML\n---\n\n[MosaicML](https://docs.mosaicml.com/en/latest/inference.html) offers a managed inference service. You can either use a variety of open-source models, or deploy your own.\n\nThis example goes over how to use LangChain to interact with MosaicML Inference for text completion.\n\n```python\n# sign up for an account: https://forms.mosaicml.com/demo?utm_source=langchain\n"
  }
,
  {
    "path": "python/integrations/llms/ibm_watsonx.mdx",
    "filename": "ibm_watsonx.mdx",
    "size_bytes": 7494,
    "line_count": 216,
    "preview": "---\ntitle: IBM watsonx.ai\n---\n\n>[`WatsonxLLM`](https://ibm.github.io/watsonx-ai-python-sdk/fm_extensions.html#langchain) is a wrapper for IBM [watsonx.ai](https://www.ibm.com/products/watsonx-ai) foundation models.\n\nThis example shows how to communicate with `watsonx.ai` models using `LangChain`.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/llms/alibabacloud_pai_eas_endpoint.mdx",
    "filename": "alibabacloud_pai_eas_endpoint.mdx",
    "size_bytes": 2581,
    "line_count": 46,
    "preview": "---\ntitle: Alibaba Cloud PAI EAS\n---\n\n>[Machine Learning Platform for AI of Alibaba Cloud](https://www.alibabacloud.com/help/en/pai) is a machine learning or deep learning engineering platform intended for enterprises and developers. It provides easy-to-use, cost-effective, high-performance, and easy-to-scale plug-ins that can be applied to various industry scenarios. With over 140 built-in optimization algorithms, `Machine Learning Platform for AI` provides whole-process AI engineering capabilities including data labeling (`PAI-iTAG`), model building (`PAI-Designer` and `PAI-DSW`), model training (`PAI-DLC`), compilation optimization, and inference deployment (`PAI-EAS`). `PAI-EAS` supports different types of hardware resources, including CPUs and GPUs, and features high throughput and low latency. It allows you to deploy large-scale complex models with a few clicks and perform elastic scale-ins and scale-outs in real time. It also provides a comprehensive O&M and monitoring system.\n\n```python\n##Installing the langchain packages needed to use the integration\npip install -qU langchain-community\n```\n"
  }
,
  {
    "path": "python/integrations/llms/gpt4all.mdx",
    "filename": "gpt4all.mdx",
    "size_bytes": 2373,
    "line_count": 95,
    "preview": "---\ntitle: GPT4All\n---\n\n[GitHub:nomic-ai/gpt4all](https://github.com/nomic-ai/gpt4all) an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue.\n\nThis example goes over how to use LangChain to interact with `GPT4All` models.\n\n```python\npip install -qU langchain-community gpt4all\n"
  }
,
  {
    "path": "python/integrations/llms/fireworks.mdx",
    "filename": "fireworks.mdx",
    "size_bytes": 4365,
    "line_count": 146,
    "preview": "---\ntitle: Fireworks\n---\n\n<Warning>\n**You are currently on a page documenting the use of Fireworks models as text completion models. Many popular Fireworks models are [chat completion models](/oss/langchain/models).**\n\nYou may be looking for [this page instead](/oss/integrations/chat/fireworks/).\n</Warning>\n\n"
  }
,
  {
    "path": "python/integrations/llms/koboldai.mdx",
    "filename": "koboldai.mdx",
    "size_bytes": 910,
    "line_count": 27,
    "preview": "---\ntitle: KoboldAI API\n---\n\n[KoboldAI](https://github.com/KoboldAI/KoboldAI-Client) is a \"a browser-based front-end for AI-assisted writing with multiple local & remote AI models...\". It has a public and local API that is able to be used in langchain.\n\nThis example goes over how to use LangChain with that API.\n\nDocumentation can be found in the browser adding /api to the end of your endpoint (i.e [127.0.0.1/:5000/api](http://127.0.0.1/:5000/api)).\n\n"
  }
,
  {
    "path": "python/integrations/llms/predictionguard.mdx",
    "filename": "predictionguard.mdx",
    "size_bytes": 4516,
    "line_count": 158,
    "preview": "---\ntitle: PredictionGuard\n---\n\n>[Prediction Guard](https://predictionguard.com) is a secure, scalable GenAI platform that safeguards sensitive data, prevents common AI malfunctions, and runs on affordable hardware.\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/llms/gradient.mdx",
    "filename": "gradient.mdx",
    "size_bytes": 4818,
    "line_count": 160,
    "preview": "---\ntitle: Gradient\n---\n\n`Gradient` allows to fine tune and get completions on LLMs with a simple web API.\n\nThis notebook goes over how to use LangChain with [Gradient](https://gradient.ai/).\n\n## Imports\n\n"
  }
,
  {
    "path": "python/integrations/llms/gooseai.mdx",
    "filename": "gooseai.mdx",
    "size_bytes": 1527,
    "line_count": 75,
    "preview": "---\ntitle: GooseAI\n---\n\n`GooseAI` is a fully managed NLP-as-a-Service, delivered via API. GooseAI provides access to [these models](https://goose.ai/docs/models).\n\nThis notebook goes over how to use LangChain with [GooseAI](https://goose.ai/).\n\n## Install openai\n\n"
  }
,
  {
    "path": "python/integrations/llms/edenai.mdx",
    "filename": "edenai.mdx",
    "size_bytes": 4386,
    "line_count": 166,
    "preview": "---\ntitle: Eden AI\n---\n\nEden AI is revolutionizing the AI landscape by uniting the best AI providers, empowering users to unlock limitless possibilities and tap into the true potential of artificial intelligence. With an all-in-one comprehensive and hassle-free platform, it allows users to deploy AI features to production lightning fast, enabling effortless access to the full breadth of AI capabilities via a single API. (website: [edenai.co/](https://edenai.co/))\n\nThis example goes over how to use LangChain to interact with Eden AI models\n\n-----------------------------------------------------------------------------------\n\n"
  }
,
  {
    "path": "python/integrations/llms/nlpcloud.mdx",
    "filename": "nlpcloud.mdx",
    "size_bytes": 1680,
    "line_count": 61,
    "preview": "---\ntitle: NLP Cloud\n---\n\nThe [NLP Cloud](https://nlpcloud.io) serves high performance pre-trained or custom models for NER, sentiment-analysis, classification, summarization, paraphrasing, grammar and spelling correction, keywords and keyphrases extraction, chatbot, product description and ad generation, intent classification, text generation, image generation, blog post generation, code generation, question answering, automatic speech recognition, machine translation, language detection, semantic search, semantic similarity, tokenization, POS tagging, embeddings, and dependency parsing. It is ready for production, served through a REST API.\n\nThis example goes over how to use LangChain to interact with `NLP Cloud` [models](https://docs.nlpcloud.com/#models).\n\n```python\npip install -qU  nlpcloud\n"
  }
,
  {
    "path": "python/integrations/llms/llamafile.mdx",
    "filename": "llamafile.mdx",
    "size_bytes": 3593,
    "line_count": 65,
    "preview": "---\ntitle: Llamafile\n---\n\n[Llamafile](https://github.com/Mozilla-Ocho/llamafile) lets you distribute and run LLMs with a single file.\n\nLlamafile does this by combining [llama.cpp](https://github.com/ggerganov/llama.cpp) with [Cosmopolitan Libc](https://github.com/jart/cosmopolitan) into one framework that collapses all the complexity of LLMs down to a single-file executable (called a \"llamafile\") that runs locally on most computers, with no installation.\n\n## Setup\n\n"
  }
,
  {
    "path": "python/integrations/llms/azure_ml.mdx",
    "filename": "azure_ml.mdx",
    "size_bytes": 8916,
    "line_count": 204,
    "preview": "---\ntitle: Azure ML\n---\n\n[Azure ML](https://azure.microsoft.com/en-us/products/machine-learning/) is a platform used to build, train, and deploy machine learning models. Users can explore the types of models to deploy in the Model Catalog, which provides foundational and general purpose models from different providers.\n\nThis notebook goes over how to use an LLM hosted on an `Azure ML Online Endpoint`.\n\n```python\n##Installing the langchain packages needed to use the integration\n"
  }
,
  {
    "path": "python/integrations/llms/minimax.mdx",
    "filename": "minimax.mdx",
    "size_bytes": 1550,
    "line_count": 67,
    "preview": "---\ntitle: Minimax\n---\n\n[Minimax](https://api.minimax.chat) is a Chinese startup that provides natural language processing models for companies and individuals.\n\nThis example demonstrates using LangChain to interact with Minimax.\n\n# Setup\n\n"
  }
,
  {
    "path": "python/integrations/llms/runhouse.mdx",
    "filename": "runhouse.mdx",
    "size_bytes": 4012,
    "line_count": 156,
    "preview": "---\ntitle: Runhouse\n---\n\n[Runhouse](https://github.com/run-house/runhouse) allows remote compute and data across environments and users. See the [Runhouse docs](https://www.run.house/docs).\n\nThis example goes over how to use LangChain and [Runhouse](https://github.com/run-house/runhouse) to interact with models hosted on your own GPU, or on-demand GPUs on AWS, GCP, AWS, or Lambda.\n\n**Note**: Code uses `SelfHosted` name instead of the `Runhouse`.\n\n"
  }
,
  {
    "path": "python/integrations/llms/openllm.mdx",
    "filename": "openllm.mdx",
    "size_bytes": 1133,
    "line_count": 41,
    "preview": "---\ntitle: OpenLLM\n---\n\n[🦾 OpenLLM](https://github.com/bentoml/OpenLLM) lets developers run any **open-source LLMs** as **OpenAI-compatible API** endpoints with **a single command**.\n\n- 🔬 Build for fast and production usages\n- 🚂 Support llama3, qwen2, gemma, etc, and many **quantized** versions [full list](https://github.com/bentoml/openllm-models)\n- ⛓️ OpenAI-compatible API\n- 💬 Built-in ChatGPT like UI\n"
  }
,
  {
    "path": "python/integrations/llms/openvino.mdx",
    "filename": "openvino.mdx",
    "size_bytes": 5017,
    "line_count": 147,
    "preview": "---\ntitle: OpenVINO\n---\n\n[OpenVINO™](https://github.com/openvinotoolkit/openvino) is an open-source toolkit for optimizing and deploying AI inference. OpenVINO™ Runtime can enable running the same model optimized across various hardware [devices](https://github.com/openvinotoolkit/openvino?tab=readme-ov-file#supported-hardware-matrix). Accelerate your deep learning performance across use cases like: language + LLMs, computer vision, automatic speech recognition, and more.\n\nOpenVINO models can be run locally through the `HuggingFacePipeline` [class](https://python.langchain.com/docs/integrations/llms/huggingface_pipeline). To deploy a model with OpenVINO, you can specify the `backend=\"openvino\"` parameter to trigger OpenVINO as backend inference framework.\n\nTo use, you should have the `optimum-intel` with OpenVINO Accelerator python [package installed](https://github.com/huggingface/optimum-intel?tab=readme-ov-file#installation).\n\n"
  }
,
  {
    "path": "python/integrations/llms/tongyi.mdx",
    "filename": "tongyi.mdx",
    "size_bytes": 1961,
    "line_count": 73,
    "preview": "---\ntitle: Tongyi Qwen\n---\n\nTongyi Qwen is a large-scale language model developed by Alibaba's Damo Academy. It is capable of understanding user intent through natural language understanding and semantic analysis, based on user input in natural language. It provides services and assistance to users in different domains and tasks. By providing clear and detailed instructions, you can obtain results that better align with your expectations.\n\n## Setting up\n\n```python\n# Install the package\n"
  }
,
  {
    "path": "python/integrations/llms/google_vertex_ai.mdx",
    "filename": "google_vertex_ai.mdx",
    "size_bytes": 647286,
    "line_count": 601,
    "preview": "---\ntitle: VertexAI\n---\n\n<Danger>\n    **Deprecated**\n\n    This integration is deprecated and will be removed in a future release. Please use [`GoogleGenerativeAI`](/oss/integrations/llms/google_generative_ai) instead. See the full [release notes and migration guide](https://github.com/langchain-ai/langchain-google/discussions/1422).\n</Danger>\n\n"
  }
,
  {
    "path": "python/integrations/llms/modal.mdx",
    "filename": "modal.mdx",
    "size_bytes": 3031,
    "line_count": 98,
    "preview": "---\ntitle: Modal\n---\n\nThe [Modal cloud platform](https://modal.com/docs/guide) provides convenient, on-demand access to serverless cloud compute from Python scripts on your local computer.\nUse `modal` to run your own custom LLM models instead of depending on LLM APIs.\n\nThis example goes over how to use LangChain to interact with a `modal` HTTPS [web endpoint](https://modal.com/docs/guide/webhooks).\n\n[_Question-answering with LangChain_](https://modal.com/docs/guide/ex/potus_speech_qanda) is another example of how to use LangChain alonside `Modal`. In that example, Modal runs the LangChain application end-to-end and uses OpenAI as its LLM API.\n"
  }
,
  {
    "path": "python/integrations/llms/mlx_pipelines.mdx",
    "filename": "mlx_pipelines.mdx",
    "size_bytes": 1842,
    "line_count": 57,
    "preview": "---\ntitle: MLX Local Pipelines\n---\n\nMLX models can be run locally through the `MLXPipeline` class.\n\nThe [MLX Community](https://huggingface.co/mlx-community) hosts over 150 models, all open source and publicly available on Hugging Face Model Hub a online platform where people can easily collaborate and build ML together.\n\nThese can be called from LangChain either through this local pipeline wrapper or by calling their hosted inference endpoints through the MlXPipeline class. For more information on mlx, see the [examples repo](https://github.com/ml-explore/mlx-examples/tree/main/llms) notebook.\n\n"
  }
,
  {
    "path": "python/integrations/llms/xinference.mdx",
    "filename": "xinference.mdx",
    "size_bytes": 2553,
    "line_count": 81,
    "preview": "---\ntitle: Xorbits Inference (Xinference)\n---\n\n[Xinference](https://github.com/xorbitsai/inference) is a powerful and versatile library designed to serve LLMs,\nspeech recognition models, and multimodal models, even on your laptop. It supports a variety of models compatible with GGML, such as chatglm, baichuan, whisper, vicuna, orca, and many others. This notebook demonstrates how to use Xinference with LangChain.\n\n## Installation\n\nInstall `Xinference` through PyPI:\n"
  }
,
  {
    "path": "python/integrations/llms/aleph_alpha.mdx",
    "filename": "aleph_alpha.mdx",
    "size_bytes": 1228,
    "line_count": 69,
    "preview": "---\ntitle: Aleph Alpha\n---\n\n[The Luminous series](https://docs.aleph-alpha.com/docs/category/luminous/) is a family of Large Language Models (LLMs).\n\nThis example goes over how to use LangChain to interact with Aleph Alpha models\n\n```python\n# Installing the langchain package needed to use the integration\n"
  }
,
  {
    "path": "python/integrations/llms/pipelineai.mdx",
    "filename": "pipelineai.mdx",
    "size_bytes": 2067,
    "line_count": 76,
    "preview": "---\ntitle: PipelineAI\n---\n\n>[PipelineAI](https://pipeline.ai) allows you to run your ML models at scale in the cloud. It also provides API access to [several LLM models](https://pipeline.ai).\n\nThis notebook goes over how to use LangChain with [PipelineAI](https://docs.pipeline.ai/docs).\n\n## PipelineAI example\n\n"
  }
,
  {
    "path": "python/integrations/llms/modelscope_endpoint.mdx",
    "filename": "modelscope_endpoint.mdx",
    "size_bytes": 6639,
    "line_count": 143,
    "preview": "---\ntitle: ModelScopeEndpoint\n---\n\n\nModelScope ([Home](https://www.modelscope.cn/) | [GitHub](https://github.com/modelscope/modelscope)) is built upon the notion of “Model-as-a-Service” (MaaS). It seeks to bring together most advanced machine learning models from the AI community, and streamlines the process of leveraging AI models in real-world applications. The core ModelScope library open-sourced in this repository provides the interfaces and implementations that allow developers to perform model inference, training and evaluation. This will help you get started with ModelScope completion models (LLMs) using LangChain.\n\n## Overview\n\n### Integration details\n"
  }
,
  {
    "path": "python/integrations/llms/anyscale.mdx",
    "filename": "anyscale.mdx",
    "size_bytes": 2069,
    "line_count": 83,
    "preview": "---\ntitle: Anyscale\n---\n\n[Anyscale](https://www.anyscale.com/) is a fully-managed [Ray](https://www.ray.io/) platform, on which you can build, deploy, and manage scalable AI and Python applications\n\nThis example goes over how to use LangChain to interact with [Anyscale Endpoint](https://app.endpoints.anyscale.com/).\n\n```python\n##Installing the langchain packages needed to use the integration\n"
  }
,
  {
    "path": "python/integrations/llms/stochasticai.mdx",
    "filename": "stochasticai.mdx",
    "size_bytes": 1535,
    "line_count": 69,
    "preview": "---\ntitle: StochasticAI\n---\n\n>[Stochastic Acceleration Platform](https://docs.stochastic.ai/docs/introduction/) aims to simplify the life cycle of a Deep Learning model. From uploading and versioning the model, through training, compression and acceleration to putting it into production.\n\nThis example goes over how to use LangChain to interact with `StochasticAI` models.\n\nYou have to get the API_KEY and the API_URL [here](https://app.stochastic.ai/workspace/profile/settings?tab=profile).\n\n"
  }
,
  {
    "path": "python/integrations/llms/sambastudio.mdx",
    "filename": "sambastudio.mdx",
    "size_bytes": 10974,
    "line_count": 136,
    "preview": "---\ntitle: SambaStudio\n---\n\n<Warning>\n**You are currently on a page documenting the use of SambaStudio models as text completion models. We recommend you to use the [chat completion models](/oss/langchain/models).**\n\nYou may be looking for [SambaNova Chat Models](/oss/integrations/chat/sambanova) .\n</Warning>\n\n"
  }
,
  {
    "path": "python/integrations/llms/yandex.mdx",
    "filename": "yandex.mdx",
    "size_bytes": 1632,
    "line_count": 54,
    "preview": "---\ntitle: YandexGPT\n---\n\nThis notebook goes over how to use LangChain with [YandexGPT](https://cloud.yandex.com/en/services/yandexgpt).\n\nTo use, you should have the `yandexcloud` python package installed.\n\n```python\npip install -qU  yandexcloud\n"
  }
,
  {
    "path": "python/integrations/llms/konko.mdx",
    "filename": "konko.mdx",
    "size_bytes": 2453,
    "line_count": 52,
    "preview": "---\ntitle: Konko\n---\n\n\n>[Konko](https://www.konko.ai/) API is a fully managed Web API designed to help application developers:\n\n1. **Select** the right open source or proprietary LLMs for their application\n2. **Build** applications faster with integrations to leading application frameworks and fully managed APIs\n3. **Fine tune** smaller open-source LLMs to achieve industry-leading performance at a fraction of the cost\n"
  }
,
  {
    "path": "python/integrations/llms/jsonformer_experimental.mdx",
    "filename": "jsonformer_experimental.mdx",
    "size_bytes": 4157,
    "line_count": 146,
    "preview": "---\ntitle: JSONFormer\n---\n\n[JSONFormer](https://github.com/1rgs/jsonformer) is a library that wraps local Hugging Face pipeline models for structured decoding of a subset of the JSON Schema.\n\nIt works by filling in the structure tokens and then sampling the content tokens from the model.\n\n**Warning - this module is still experimental**\n\n"
  }
,
  {
    "path": "python/integrations/llms/runpod.mdx",
    "filename": "runpod.mdx",
    "size_bytes": 4789,
    "line_count": 152,
    "preview": "---\ntitle: RunPod\n---\n\nGet started with RunPod LLMs.\n\n## Overview\n\nThis guide covers how to use the LangChain `RunPod` LLM class to interact with text generation models hosted on [RunPod Serverless](https://www.runpod.io/serverless-gpu).\n\n"
  }
,
  {
    "path": "python/integrations/llms/banana.mdx",
    "filename": "banana.mdx",
    "size_bytes": 1446,
    "line_count": 63,
    "preview": "---\ntitle: Banana\n---\n\n[Banana](https://www.banana.dev/about-us) is focused on building the machine learning infrastructure.\n\nThis example goes over how to use LangChain to interact with Banana models\n\n```python\n##Installing the langchain packages needed to use the integration\n"
  }
,
  {
    "path": "python/integrations/llms/arcee.mdx",
    "filename": "arcee.mdx",
    "size_bytes": 2265,
    "line_count": 73,
    "preview": "---\ntitle: Arcee\n---\n\nThis notebook demonstrates how to use the `Arcee` class for generating text using Arcee's Domain Adapted Language Models (DALMs).\n\n```python\n##Installing the langchain packages needed to use the integration\npip install -qU langchain-community\n```\n"
  }
,
  {
    "path": "python/integrations/llms/weight_only_quantization.mdx",
    "filename": "weight_only_quantization.mdx",
    "size_bytes": 6040,
    "line_count": 152,
    "preview": "---\ntitle: Intel Weight-Only Quantization\n---\n\n## Weight-Only quantization for huggingface models with intel extension for transformers pipelines\n\nHugging Face models can be run locally with Weight-Only quantization through the `WeightOnlyQuantPipeline` class.\n\nThe [Hugging Face Model Hub](https://huggingface.co/models) hosts over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.\n\n"
  }
,
  {
    "path": "python/integrations/llms/yi.mdx",
    "filename": "yi.mdx",
    "size_bytes": 1996,
    "line_count": 80,
    "preview": "---\ntitle: Yi\n---\n\n[01.AI](https://www.lingyiwanwu.com/en), founded by Dr. Kai-Fu Lee, is a global company at the forefront of AI 2.0. They offer cutting-edge Large Language Models, including the Yi series, which range from 6B to hundreds of billions of parameters. 01.AI also provides multimodal models, an open API platform, and open-source options like Yi-34B/9B/6B and Yi-VL.\n\n```python\n## Installing the langchain packages needed to use the integration\npip install -qU langchain-community\n```\n"
  }
,
  {
    "path": "python/integrations/llms/aphrodite.mdx",
    "filename": "aphrodite.mdx",
    "size_bytes": 7804,
    "line_count": 141,
    "preview": "---\ntitle: Aphrodite Engine\n---\n\n[Aphrodite](https://github.com/PygmalionAI/aphrodite-engine) is the open-source large-scale inference engine designed to serve thousands of users on the [PygmalionAI](https://pygmalion.chat) website.\n\n* Attention mechanism by vLLM for fast throughput and low latencies\n* Support for for many SOTA sampling methods\n* Exllamav2 GPTQ kernels for better throughput at lower batch sizes\n\n"
  }
,
  {
    "path": "python/integrations/llms/anthropic.mdx",
    "filename": "anthropic.mdx",
    "size_bytes": 1364,
    "line_count": 52,
    "preview": "---\ntitle: AnthropicLLM\n---\n\n<Warning>\n**You are currently on a page documenting the use of Anthropic legacy Claude 2 models as text completion models. The latest and most popular Anthropic models are [chat completion models](/oss/langchain/models), and the text completion models have been deprecated.**\n\nYou are probably looking for [this page instead](/oss/integrations/chat/anthropic/).\n</Warning>\n\n"
  }
,
  {
    "path": "python/integrations/llms/yuan2.mdx",
    "filename": "yuan2.mdx",
    "size_bytes": 1713,
    "line_count": 43,
    "preview": "---\ntitle: Yuan2.0\n---\n\n[Yuan2.0](https://github.com/IEIT-Yuan/Yuan-2.0) is a new generation Fundamental Large Language Model developed by IEIT System. We have published all three models, Yuan 2.0-102B, Yuan 2.0-51B, and Yuan 2.0-2B. And we provide relevant scripts for pretraining, fine-tuning, and inference services for other developers. Yuan2.0 is based on Yuan1.0, utilizing a wider range of high-quality pre training data and instruction fine-tuning datasets to enhance the model's understanding of semantics, mathematics, reasoning, code, knowledge, and other aspects.\n\nThis example goes over how to use LangChain to interact with `Yuan2.0`(2B/51B/102B) Inference for text generation.\n\nYuan2.0 set up an inference service so user just need request the inference api to get result, which is introduced in [Yuan2.0 Inference-Server](https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/docs/inference_server.md).\n\n"
  }
,
  {
    "path": "python/integrations/llms/deepinfra.mdx",
    "filename": "deepinfra.mdx",
    "size_bytes": 2750,
    "line_count": 107,
    "preview": "---\ntitle: DeepInfra\n---\n\n[DeepInfra](https://deepinfra.com/?utm_source=langchain) is a serverless inference as a service that provides access to a [variety of LLMs](https://deepinfra.com/models?utm_source=langchain) and [embeddings models](https://deepinfra.com/models?type=embeddings&utm_source=langchain). This notebook goes over how to use LangChain with DeepInfra for language models.\n\n## Set the environment API Key\n\nMake sure to get your API key from DeepInfra. You have to [Login](https://deepinfra.com/login?from=%2Fdash) and get a new token.\n\n"
  }
,
  {
    "path": "python/integrations/llms/opaqueprompts.mdx",
    "filename": "opaqueprompts.mdx",
    "size_bytes": 8099,
    "line_count": 151,
    "preview": "---\ntitle: OpaquePrompts\n---\n\n[OpaquePrompts](https://opaqueprompts.readthedocs.io/en/latest/) is a service that enables applications to leverage the power of language models without compromising user privacy. Designed for composability and ease of integration into existing applications and services, OpaquePrompts is consumable via a simple Python library as well as through LangChain. Perhaps more importantly, OpaquePrompts leverages the power of [confidential computing](https://en.wikipedia.org/wiki/Confidential_computing) to ensure that even the OpaquePrompts service itself cannot access the data it is protecting.\n\nThis notebook goes over how to use LangChain to interact with `OpaquePrompts`.\n\n```python\n# install the opaqueprompts and langchain packages\n"
  }
,
  {
    "path": "python/integrations/llms/predibase.mdx",
    "filename": "predibase.mdx",
    "size_bytes": 5905,
    "line_count": 185,
    "preview": "---\ntitle: Predibase\n---\n\n[Predibase](https://predibase.com/) allows you to train, fine-tune, and deploy any ML model—from linear regression to large language model.\n\nThis example demonstrates using LangChain with models deployed on Predibase\n\n# Setup\n\n"
  }
,
  {
    "path": "python/integrations/llms/friendli.mdx",
    "filename": "friendli.mdx",
    "size_bytes": 6454,
    "line_count": 103,
    "preview": "---\ntitle: Friendli\n---\n\n\n> [Friendli](https://friendli.ai/) enhances AI application performance and optimizes cost savings with scalable, efficient deployment options, tailored for high-demand AI workloads.\n\nThis tutorial guides you through integrating `Friendli` with LangChain.\n\n## Setup\n"
  }
,
  {
    "path": "python/integrations/llms/together.mdx",
    "filename": "together.mdx",
    "size_bytes": 1713,
    "line_count": 59,
    "preview": "---\ntitle: Together AI\n---\n\n<Warning>\n**You are currently on a page documenting the use of Together AI models as text completion models. Many popular Together AI models are [chat completion models](/oss/langchain/models).**\n\nYou may be looking for [this page instead](/oss/integrations/chat/together/).\n</Warning>\n\n"
  }
,
  {
    "path": "python/integrations/llms/moonshot.mdx",
    "filename": "moonshot.mdx",
    "size_bytes": 695,
    "line_count": 30,
    "preview": "---\ntitle: MoonshotChat\n---\n\n[Moonshot](https://platform.moonshot.cn/) is a Chinese startup that provides LLM service for companies and individuals.\n\nThis example goes over how to use LangChain to interact with Moonshot.\n\n```python\nfrom langchain_community.llms.moonshot import Moonshot\n"
  }
,
  {
    "path": "python/integrations/llms/lmformatenforcer_experimental.mdx",
    "filename": "lmformatenforcer_experimental.mdx",
    "size_bytes": 7168,
    "line_count": 214,
    "preview": "---\ntitle: LM Format Enforcer\n---\n\n[LM Format Enforcer](https://github.com/noamgat/lm-format-enforcer) is a library that enforces the output format of language models by filtering tokens.\n\nIt works by combining a character level parser with a tokenizer prefix tree to allow only the tokens which contains sequences of characters that lead to a potentially valid format.\n\nIt supports batched generation.\n\n"
  }
,
  {
    "path": "python/integrations/llms/huggingface_endpoint.mdx",
    "filename": "huggingface_endpoint.mdx",
    "size_bytes": 3971,
    "line_count": 124,
    "preview": "---\ntitle: Huggingface Endpoints\n---\n\n>The [Hugging Face Hub](https://huggingface.co/docs/hub/index) is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.\n\nThe `Hugging Face Hub` also offers various endpoints to build ML applications.\nThis example showcases how to connect to the different Endpoints types.\n\nIn particular, text generation inference is powered by [Text Generation Inference](https://github.com/huggingface/text-generation-inference): a custom-built Rust, Python and gRPC server for blazing-faset text generation inference.\n"
  }
,
  {
    "path": "python/integrations/llms/cohere.mdx",
    "filename": "cohere.mdx",
    "size_bytes": 2945,
    "line_count": 103,
    "preview": "---\ntitle: Cohere\n---\n\n<Warning>\n**You are currently on a page documenting the use of Cohere models as text completion models. Many popular Cohere models are [chat completion models](/oss/langchain/models).**\n\nYou may be looking for [this page instead](/oss/integrations/chat/cohere/).\n</Warning>\n\n"
  }
,
  {
    "path": "python/integrations/llms/ollama.mdx",
    "filename": "ollama.mdx",
    "size_bytes": 58080,
    "line_count": 136,
    "preview": "---\ntitle: Ollama\n---\n\n<Warning>\n**You are currently on a page documenting the use of Ollama models as text completion models. Many popular Ollama models are [chat completion models](/oss/langchain/models).**\n\nYou may be looking for [this page instead](/oss/integrations/chat/ollama/).\n</Warning>\n\n"
  }
,
  {
    "path": "python/integrations/llms/replicate.mdx",
    "filename": "replicate.mdx",
    "size_bytes": 13608,
    "line_count": 304,
    "preview": "---\ntitle: Replicate\n---\n\n>[Replicate](https://replicate.com/blog/machine-learning-needs-better-tools) runs machine learning models in the cloud. We have a library of open-source models that you can run with a few lines of code. If you're building your own machine learning models, Replicate makes it easy to deploy them at scale.\n\nThis example goes over how to use LangChain to interact with `Replicate` [models](https://replicate.com/explore)\n\n## Setup\n\n"
  }
,
  {
    "path": "python/integrations/llms/ctranslate2.mdx",
    "filename": "ctranslate2.mdx",
    "size_bytes": 6301,
    "line_count": 134,
    "preview": "---\ntitle: CTranslate2\n---\n\n**CTranslate2** is a C++ and Python library for efficient inference with Transformer models.\n\nThe project implements a custom runtime that applies many performance optimization techniques such as weights quantization, layers fusion, batch reordering, etc., to accelerate and reduce the memory usage of Transformer models on CPU and GPU.\n\nFull list of features and supported models is included in the [project's repository](https://opennmt.net/CTranslate2/guides/transformers.html). To start, please check out the official [quickstart guide](https://opennmt.net/CTranslate2/quickstart.html).\n\n"
  }
,
  {
    "path": "python/integrations/llms/outlines.mdx",
    "filename": "outlines.mdx",
    "size_bytes": 4366,
    "line_count": 148,
    "preview": "---\ntitle: Outlines\n---\n\nThis will help you get started with Outlines LLM. For detailed documentation of all Outlines features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/llms/langchain_community.llms.outlines.Outlines.html).\n\n[Outlines](https://github.com/outlines-dev/outlines) is a library for constrained language generation. It allows you to use Large Language Models (LLMs) with various backends while applying constraints to the generated output.\n\n## Overview\n\n"
  }
,
  {
    "path": "python/integrations/llms/exllamav2.mdx",
    "filename": "exllamav2.mdx",
    "size_bytes": 7287,
    "line_count": 169,
    "preview": "---\ntitle: ExLlamaV2\n---\n\n[ExLlamav2](https://github.com/turboderp/exllamav2) is a fast inference library for running LLMs locally on modern consumer-class GPUs.\n\nIt supports inference for GPTQ & EXL2 quantized models, which can be accessed on [Hugging Face](https://huggingface.co/TheBloke).\n\nThis notebook goes over how to run `exllamav2` within LangChain.\n\n"
  }
,
  {
    "path": "python/integrations/llms/layerup_security.mdx",
    "filename": "layerup_security.mdx",
    "size_bytes": 3105,
    "line_count": 99,
    "preview": "---\ntitle: Layerup Security\n---\n\nThe [Layerup Security](https://uselayerup.com) integration allows you to secure your calls to any LangChain LLM, LLM chain or LLM agent. The LLM object wraps around any existing LLM object, allowing for a secure layer between your users and your LLMs.\n\nWhile the Layerup Security object is designed as an LLM, it is not actually an LLM itself, it simply wraps around an LLM, allowing it to adapt the same functionality as the underlying LLM.\n\n## Setup\nFirst, you'll need a Layerup Security account from the Layerup [website](https://uselayerup.com).\n"
  }
,
  {
    "path": "python/integrations/llms/pipeshift.mdx",
    "filename": "pipeshift.mdx",
    "size_bytes": 5236,
    "line_count": 77,
    "preview": "---\ntitle: Pipeshift\n---\n\nThis will help you get started with Pipeshift completion models (LLMs) using LangChain. For detailed documentation on `Pipeshift` features and configuration options, please refer to the [API reference](https://dashboard.pipeshift.com/docs).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "python/integrations/llms/titan_takeoff.mdx",
    "filename": "titan_takeoff.mdx",
    "size_bytes": 3922,
    "line_count": 115,
    "preview": "---\ntitle: Titan Takeoff\n---\n\n`TitanML` helps businesses build and deploy better, smaller, cheaper, and faster NLP models through our training, compression, and inference optimization platform.\n\nOur inference server, [Titan Takeoff](https://docs.titanml.co/docs/intro) enables deployment of LLMs locally on your hardware in a single command. Most generative model architectures are supported, such as Falcon, Llama 2, GPT2, T5 and many more. If you experience trouble with a specific model, please let us know at [hello@titanml.co](mailto:hello@titanml.co).\n\n## Example usage\n\n"
  }
,
  {
    "path": "python/integrations/llms/huggingface_pipelines.mdx",
    "filename": "huggingface_pipelines.mdx",
    "size_bytes": 6295,
    "line_count": 201,
    "preview": "---\ntitle: Hugging Face Local Pipelines\n---\n\nHugging Face models can be run locally through the `HuggingFacePipeline` class.\n\nThe [Hugging Face Model Hub](https://huggingface.co/models) hosts over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.\n\nThese can be called from LangChain either through this local pipeline wrapper or by calling their hosted inference endpoints through the HuggingFaceHub class.\n\n"
  }
,
  {
    "path": "python/integrations/llms/petals.mdx",
    "filename": "petals.mdx",
    "size_bytes": 1944,
    "line_count": 87,
    "preview": "---\ntitle: Petals\n---\n\n`Petals` runs 100B+ language models at home, BitTorrent-style.\n\nThis notebook goes over how to use LangChain with [Petals](https://github.com/bigscience-workshop/petals).\n\n## Install petals\n\n"
  }
,
  {
    "path": "python/integrations/llms/bittensor.mdx",
    "filename": "bittensor.mdx",
    "size_bytes": 4204,
    "line_count": 122,
    "preview": "---\ntitle: Bittensor\n---\n\n>[Bittensor](https://bittensor.com/) is a mining network, similar to Bitcoin, that includes built-in incentives designed to encourage miners to contribute compute + knowledge.\n>\n>`NIBittensorLLM` is developed by [Neural Internet](https://neuralinternet.ai/), powered by `Bittensor`.\n\n>This LLM showcases true potential of decentralized AI by giving you the best response(s) from the `Bittensor protocol`, which consist of various AI models such as `OpenAI`, `LLaMA2` etc.\n\n"
  }
,
  {
    "path": "python/integrations/llms/chatglm.mdx",
    "filename": "chatglm.mdx",
    "size_bytes": 5579,
    "line_count": 116,
    "preview": "---\ntitle: ChatGLM\n---\n\n[ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level).\n\n[ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) is the second-generation version of the open-source bilingual (Chinese-English) chat model ChatGLM-6B. It retains the smooth conversation flow and low deployment threshold of the first-generation model, while introducing the new features like better performance, longer context and more efficient inference.\n\n[ChatGLM3](https://github.com/THUDM/ChatGLM3) is a new generation of pre-trained dialogue models jointly released by Zhipu AI and Tsinghua KEG. ChatGLM3-6B is the open-source model in the ChatGLM3 series\n\n"
  }
,
  {
    "path": "python/integrations/llms/vllm.mdx",
    "filename": "vllm.mdx",
    "size_bytes": 4175,
    "line_count": 155,
    "preview": "---\ntitle: vLLM\n---\n\n[vLLM](https://vllm.readthedocs.io/en/latest/index.html) is a fast and easy-to-use library for LLM inference and serving, offering:\n\n* State-of-the-art serving throughput\n* Efficient management of attention key and value memory with PagedAttention\n* Continuous batching of incoming requests\n* Optimized CUDA kernels\n"
  }
,
  {
    "path": "python/integrations/llms/rellm_experimental.mdx",
    "filename": "rellm_experimental.mdx",
    "size_bytes": 2624,
    "line_count": 101,
    "preview": "---\ntitle: RELLM\n---\n\n[RELLM](https://github.com/r2d4/rellm) is a library that wraps local Hugging Face pipeline models for structured decoding.\n\nIt works by generating tokens one at a time. At each step, it masks tokens that don't conform to the provided partial regular expression.\n\n**Warning - this module is still experimental**\n\n"
  }
,
  {
    "path": "python/integrations/llms/clarifai.mdx",
    "filename": "clarifai.mdx",
    "size_bytes": 8062,
    "line_count": 147,
    "preview": "---\ntitle: Clarifai\n---\n\n>[Clarifai](https://www.clarifai.com/) is an AI Platform that provides the full AI lifecycle ranging from data exploration, data labeling, model training, evaluation, and inference.\n\nThis example goes over how to use LangChain to interact with `Clarifai` [models](https://clarifai.com/explore/models).\n\nTo use Clarifai, you must have an account and a Personal Access Token (PAT) key.\n[Check here](https://clarifai.com/settings/security) to get or create a PAT.\n"
  }
,
  {
    "path": "python/integrations/llms/solar.mdx",
    "filename": "solar.mdx",
    "size_bytes": 805,
    "line_count": 34,
    "preview": "---\ntitle: Solar\n---\n\n*This community integration is deprecated. You should use [`ChatUpstage`](/oss/integrations/chat/upstage) instead to access Solar LLM via the chat model connector.*\n\n```python\nimport os\n\nfrom langchain_community.llms.solar import Solar\n"
  }
,
  {
    "path": "python/integrations/llms/nvidia_ai_endpoints.mdx",
    "filename": "nvidia_ai_endpoints.mdx",
    "size_bytes": 5126,
    "line_count": 142,
    "preview": "---\ntitle: NVIDIA\n---\n\nThis will help you get started with NVIDIA models. For detailed documentation of all `NVIDIA` features and configurations head to the [API reference](https://python.langchain.com/api_reference/nvidia_ai_endpoints/llms/langchain_nvidia_ai_endpoints.chat_models.NVIDIA.html).\n\n## Overview\n\nThe `langchain-nvidia-ai-endpoints` package contains LangChain integrations building applications with models on\nNVIDIA NIM inference microservice. These models are optimized by NVIDIA to deliver the best performance on NVIDIA\n"
  }
,
  {
    "path": "python/integrations/caches/redis_llm_caching.mdx",
    "filename": "redis_llm_caching.mdx",
    "size_bytes": 7332,
    "line_count": 268,
    "preview": "---\ntitle: Redis Cache for LangChain\n---\n\nThis notebook demonstrates how to use the `RedisCache` and `RedisSemanticCache` classes from the langchain-redis package to implement caching for LLM responses.\n\n## Setup\n\nFirst, let's install the required dependencies and ensure we have a Redis instance running.\n\n"
  }
,
  {
    "path": "python/migrate/langgraph-v1.mdx",
    "filename": "langgraph-v1.mdx",
    "size_bytes": 2591,
    "line_count": 74,
    "preview": "---\ntitle: LangGraph v1 migration guide\nsidebarTitle: LangGraph v1\n---\n\nThis guide outlines changes in LangGraph v1 and how to migrate from previous versions. For a high-level overview of changes, see the [what's new](/oss/releases/langgraph-v1) page.\n\nTo upgrade:\n\n<CodeGroup>\n"
  }
,
  {
    "path": "python/migrate/langchain-v1.mdx",
    "filename": "langchain-v1.mdx",
    "size_bytes": 32556,
    "line_count": 1015,
    "preview": "---\ntitle: LangChain v1 migration guide\nsidebarTitle: LangChain v1\n---\n\nThis guide outlines the major changes between [LangChain v1](/oss/releases/langchain-v1) and previous versions.\n\n## Simplified package\n\nThe `langchain` package namespace has been significantly reduced in v1 to focus on essential building blocks for agents. The streamlined package makes it easier to discover and use the core functionality.\n"
  }
,
  {
    "path": "langchain/agents.mdx",
    "filename": "agents.mdx",
    "size_bytes": 28641,
    "line_count": 907,
    "preview": "---\ntitle: Agents\n---\n\nAgents combine language models with [tools](/oss/langchain/tools) to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\n\n:::python\n@[`create_agent`] provides a production-ready agent implementation.\n:::\n:::js\n"
  }
,
  {
    "path": "langchain/studio.mdx",
    "filename": "studio.mdx",
    "size_bytes": 307,
    "line_count": 12,
    "preview": "---\ntitle: LangSmith Studio\nsidebarTitle: LangSmith Studio\n---\n\nimport Studio from '/snippets/oss/studio.mdx';\n\n<Studio/>\n\n<Tip>\n"
  }
,
  {
    "path": "langchain/philosophy.mdx",
    "filename": "philosophy.mdx",
    "size_bytes": 7742,
    "line_count": 131,
    "preview": "---\ntitle: Philosophy\ndescription: LangChain exists to be the easiest place to start building with LLMs, while also being flexible and production-ready.\nmode: wide\n---\n\nLangChain is driven by a few core beliefs:\n\n- Large Language Models (LLMs) are great, powerful new technology.\n- LLMs are even better when you combine them with external sources of data.\n"
  }
,
  {
    "path": "langchain/middleware/overview.mdx",
    "filename": "overview.mdx",
    "size_bytes": 2717,
    "line_count": 84,
    "preview": "---\ntitle: Overview\ndescription: Control and customize agent execution at every step\n---\n\nMiddleware provides a way to more tightly control what happens inside the agent. Middleware is useful for the following:\n\n- Tracking agent behavior with logging, analytics, and debugging.\n- Transforming prompts, [tool selection](/oss/langchain/middleware/built-in#llm-tool-selector), and output formatting.\n- Adding [retries](/oss/langchain/middleware/built-in#tool-retry), [fallbacks](/oss/langchain/middleware/built-in#model-fallback), and early termination logic.\n"
  }
,
  {
    "path": "langchain/middleware/built-in.mdx",
    "filename": "built-in.mdx",
    "size_bytes": 74715,
    "line_count": 2405,
    "preview": "---\ntitle: Built-in middleware\ndescription: Prebuilt middleware for common agent use cases\n---\n\nLangChain provides prebuilt middleware for common use cases. Each middleware is production-ready and configurable for your specific needs.\n\n## Provider-agnostic middleware\n\nThe following middleware work with any LLM provider:\n"
  }
,
  {
    "path": "langchain/middleware/custom.mdx",
    "filename": "custom.mdx",
    "size_bytes": 34316,
    "line_count": 1259,
    "preview": "---\ntitle: Custom middleware\n---\n\nBuild custom middleware by implementing hooks that run at specific points in the agent execution flow.\n\n## Hooks\n\nMiddleware provides two styles of hooks to intercept agent execution:\n\n"
  }
,
  {
    "path": "langchain/tools.mdx",
    "filename": "tools.mdx",
    "size_bytes": 17280,
    "line_count": 578,
    "preview": "---\ntitle: Tools\n---\n\nTools extend what [agents](/oss/langchain/agents) can do—letting them fetch real-time data, execute code, query external databases, and take actions in the world.\n\nUnder the hood, tools are callable functions with well-defined inputs and outputs that get passed to a [chat model](/oss/langchain/models). The model decides when to invoke a tool based on the conversation context, and what input arguments to provide.\n\n<Tip>\n    For details on how models handle tool calls, see [Tool calling](/oss/langchain/models#tool-calling).\n"
  }
,
  {
    "path": "langchain/rag.mdx",
    "filename": "rag.mdx",
    "size_bytes": 32928,
    "line_count": 911,
    "preview": "---\ntitle: Build a RAG agent with LangChain\nsidebarTitle: RAG agent\n---\n\nimport ChatModelTabsPy from '/snippets/chat-model-tabs.mdx';\nimport ChatModelTabsJS from '/snippets/chat-model-tabs-js.mdx';\nimport EmbeddingsTabsPy from '/snippets/embeddings-tabs-py.mdx';\nimport EmbeddingsTabsJS from '/snippets/embeddings-tabs-js.mdx';\nimport VectorstoreTabsPy from '/snippets/vectorstore-tabs-py.mdx';\n"
  }
,
  {
    "path": "langchain/guardrails.mdx",
    "filename": "guardrails.mdx",
    "size_bytes": 21705,
    "line_count": 693,
    "preview": "---\ntitle: Guardrails\ndescription: Implement safety checks and content filtering for your agents\n---\n\nGuardrails help you build safe, compliant AI applications by validating and filtering content at key points in your agent's execution. They can detect sensitive information, enforce content policies, validate outputs, and prevent unsafe behaviors before they cause problems.\n\nCommon use cases include:\n- Preventing PII leakage\n- Detecting and blocking prompt injection attacks\n"
  }
,
  {
    "path": "langchain/sql-agent.mdx",
    "filename": "sql-agent.mdx",
    "size_bytes": 28876,
    "line_count": 815,
    "preview": "---\ntitle: Build a SQL agent\nsidebarTitle: SQL agent\n---\n\nimport ChatModelTabsPy from '/snippets/chat-model-tabs.mdx';\nimport ChatModelTabsJS from '/snippets/chat-model-tabs-js.mdx';\n\n## Overview\n\n"
  }
,
  {
    "path": "langchain/retrieval.mdx",
    "filename": "retrieval.mdx",
    "size_bytes": 16988,
    "line_count": 455,
    "preview": "---\ntitle: Retrieval\n---\n\nLarge Language Models (LLMs) are powerful, but they have two key limitations:\n\n* **Finite context** — they can’t ingest entire corpora at once.\n* **Static knowledge** — their training data is frozen at a point in time.\n\nRetrieval addresses these problems by fetching relevant external knowledge at query time. This is the foundation of **Retrieval-Augmented Generation (RAG)**: enhancing an LLM’s answers with context-specific information.\n"
  }
,
  {
    "path": "langchain/overview.mdx",
    "filename": "overview.mdx",
    "size_bytes": 3956,
    "line_count": 87,
    "preview": "---\ntitle: LangChain overview\nsidebarTitle: Overview\ndescription: LangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool — so you can build agents that adapt as fast as the ecosystem evolves\n---\n\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and [more](/oss/integrations/providers/overview). LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\n\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use [LangGraph](/oss/langgraph/overview), our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\n\n"
  }
,
  {
    "path": "langchain/deploy.mdx",
    "filename": "deploy.mdx",
    "size_bytes": 1182,
    "line_count": 23,
    "preview": "---\ntitle: LangSmith Deployment\nsidebarTitle: Deployment\n---\n\nimport deploy from '/snippets/oss/deploy.mdx';\n\nWhen you're ready to deploy your LangChain agent to production, LangSmith provides a managed hosting platform designed for agent workloads. Traditional hosting platforms are built for stateless, short-lived web applications, while LangGraph is **purpose-built for stateful, long-running agents** that require persistent state and background execution. LangSmith handles the infrastructure, scaling, and operational concerns so you can deploy directly from your repository.\n\n## Prerequisites\n"
  }
,
  {
    "path": "langchain/observability.mdx",
    "filename": "observability.mdx",
    "size_bytes": 3685,
    "line_count": 100,
    "preview": "---\ntitle: LangSmith Observability\nsidebarTitle: Observability\n---\n\nimport observability from '/snippets/oss/observability.mdx';\n\n:::python\n\nAs you build and run agents with LangChain, you need visibility into how they behave: which [tools](/oss/langchain/tools) they call, what prompts they generate, and how they make decisions. LangChain agents built with @[`create_agent`] automatically support tracing through [LangSmith](/langsmith/home), a platform for capturing, debugging, evaluating, and monitoring LLM application behavior.\n"
  }
,
  {
    "path": "langchain/short-term-memory.mdx",
    "filename": "short-term-memory.mdx",
    "size_bytes": 33844,
    "line_count": 1153,
    "preview": "---\ntitle: Short-term memory\n---\n\n## Overview\n\nMemory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.\n\nShort term memory lets your application remember previous interactions within a single thread or conversation.\n\n"
  }
,
  {
    "path": "langchain/get-help.mdx",
    "filename": "get-help.mdx",
    "size_bytes": 1716,
    "line_count": 39,
    "preview": "---\ntitle: Get help\n---\n\nConnect with the LangChain community, access learning resources, and get the support you need to build with confidence.\n\n## Learning resources\n\nStart your journey or deepen your knowledge with our comprehensive learning materials.\n\n"
  }
,
  {
    "path": "langchain/knowledge-base.mdx",
    "filename": "knowledge-base.mdx",
    "size_bytes": 25998,
    "line_count": 662,
    "preview": "---\ntitle: Build a semantic search engine with LangChain\nsidebarTitle: Semantic search\n---\n\nimport EmbeddingsTabsPy from '/snippets/embeddings-tabs-py.mdx';\nimport EmbeddingsTabsJS from '/snippets/embeddings-tabs-js.mdx';\nimport VectorstoreTabsPy from '/snippets/vectorstore-tabs-py.mdx';\nimport VectorstoreTabsJS from '/snippets/vectorstore-tabs-js.mdx';\n\n"
  }
,
  {
    "path": "langchain/multi-agent/router-knowledge-base.mdx",
    "filename": "router-knowledge-base.mdx",
    "size_bytes": 47368,
    "line_count": 1539,
    "preview": "---\ntitle: Build a multi-source knowledge base with routing\nsidebarTitle: \"Router: Knowledge base\"\n---\n\nimport ChatModelTabsPy from '/snippets/chat-model-tabs.mdx';\nimport ChatModelTabsJs from '/snippets/chat-model-tabs-js.mdx';\n\n## Overview\n\n"
  }
,
  {
    "path": "langchain/multi-agent/custom-workflow.mdx",
    "filename": "custom-workflow.mdx",
    "size_bytes": 11347,
    "line_count": 311,
    "preview": "---\ntitle: Custom workflow\n---\n\nIn the **custom workflow** architecture, you define your own bespoke execution flow using [LangGraph](/oss/langgraph/overview). You have complete control over the graph structure—including sequential steps, conditional branches, loops, and parallel execution.\n\n```mermaid\ngraph LR\n    A([Input]) --> B{{Conditional}}\n    B -->|path_a| C[Deterministic step]\n"
  }
,
  {
    "path": "langchain/multi-agent/handoffs-customer-support.mdx",
    "filename": "handoffs-customer-support.mdx",
    "size_bytes": 45650,
    "line_count": 1523,
    "preview": "---\ntitle: Build customer support with handoffs\nsidebarTitle: \"Handoffs: Customer support\"\n---\n\nimport ChatModelTabsPy from '/snippets/chat-model-tabs.mdx';\nimport ChatModelTabsJs from '/snippets/chat-model-tabs-js.mdx';\n\nThe [state machine pattern](/oss/langchain/multi-agent/handoffs) describes workflows where an agent's behavior changes as it moves through different states of a task. This tutorial shows how to implement a state machine by using tool calls to dynamically change a single agent's configuration—updating its available tools and instructions based on the current state. The state can be determined from multiple sources: the agent's past actions (tool calls), external state (such as API call results), or even initial user input (for example, by running a classifier to determine user intent).\n\n"
  }
,
  {
    "path": "langchain/multi-agent/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 13233,
    "line_count": 291,
    "preview": "---\ntitle: Multi-agent\nsidebarTitle: Overview\n---\n\nMulti-agent systems coordinate specialized components to tackle complex workflows. However, not every complex task requires this approach — a single agent with the right (sometimes dynamic) tools and prompt can often achieve similar results.\n\n## Why multi-agent?\n\nWhen developers say they need \"multi-agent,\" they're usually looking for one or more of these capabilities:\n"
  }
,
  {
    "path": "langchain/multi-agent/subagents-personal-assistant.mdx",
    "filename": "subagents-personal-assistant.mdx",
    "size_bytes": 50974,
    "line_count": 1506,
    "preview": "---\ntitle: Build a personal assistant with subagents\nsidebarTitle: \"Subagents: Personal assistant\"\n---\n\nimport ChatModelTabsPy from '/snippets/chat-model-tabs.mdx';\nimport ChatModelTabsJs from '/snippets/chat-model-tabs-js.mdx';\n\n## Overview\n\n"
  }
,
  {
    "path": "langchain/multi-agent/handoffs.mdx",
    "filename": "handoffs.mdx",
    "size_bytes": 28363,
    "line_count": 822,
    "preview": "---\ntitle: Handoffs\n---\n\nIn the **handoffs** architecture, behavior changes dynamically based on state. The core mechanism: [tools](/oss/langchain/tools) update a state variable (e.g., `current_step` or `active_agent`) that persists across turns, and the system reads this variable to adjust behavior—either applying different configuration (system prompt, tools) or routing to a different [agent](/oss/langchain/agents). This pattern supports both handoffs between distinct agents and dynamic configuration changes within a single agent.\n\n<Tip>\nThe term **handoffs** was coined by [OpenAI](https://openai.github.io/openai-agents-python/handoffs/) for using tool calls (e.g., `transfer_to_sales_agent`) to transfer control between agents or states.\n</Tip>\n\n"
  }
,
  {
    "path": "langchain/multi-agent/skills.mdx",
    "filename": "skills.mdx",
    "size_bytes": 4490,
    "line_count": 120,
    "preview": "---\ntitle: Skills\n---\n\nIn the **skills** architecture, specialized capabilities are packaged as invokable \"skills\" that augment an [agent's](/oss/langchain/agents) behavior. Skills are primarily prompt-driven specializations that an agent can invoke on-demand.\n\n<Tip>\nThis pattern is conceptually identical to [llms.txt](https://llmstxt.org/) (introduced by Jeremy Howard), which uses tool calling for progressive disclosure of documentation. The skills pattern applies the same approach to specialized prompts and domain knowledge rather than just documentation pages.\n</Tip>\n\n"
  }
,
  {
    "path": "langchain/multi-agent/router.mdx",
    "filename": "router.mdx",
    "size_bytes": 7672,
    "line_count": 221,
    "preview": "---\ntitle: Router\n---\n\nIn the **router** architecture, a routing step classifies input and directs it to specialized [agents](/oss/langchain/agents). This is useful when you have distinct **verticals**—separate knowledge domains that each require their own agent.\n\n```mermaid\ngraph LR\n    A([Query]) --> B[Router]\n    B --> C[Agent A]\n"
  }
,
  {
    "path": "langchain/multi-agent/subagents.mdx",
    "filename": "subagents.mdx",
    "size_bytes": 21618,
    "line_count": 579,
    "preview": "---\ntitle: Subagents\n---\n\nIn the **subagents** architecture, a central main [agent](/oss/langchain/agents) (often referred to as a **supervisor**) coordinates subagents by calling them as [tools](/oss/langchain/tools). The main agent decides which subagent to invoke, what input to provide, and how to combine results. Subagents are stateless—they don't remember past interactions, with all conversation memory maintained by the main agent. This provides [context](/oss/langchain/context-engineering) isolation: each subagent invocation works in a clean context window, preventing context bloat in the main conversation.\n\n```mermaid\ngraph LR\n    A[User] --> B[Main Agent]\n    B --> C[Subagent A]\n"
  }
,
  {
    "path": "langchain/multi-agent/skills-sql-assistant.mdx",
    "filename": "skills-sql-assistant.mdx",
    "size_bytes": 54787,
    "line_count": 1673,
    "preview": "---\ntitle: Build a SQL assistant with on-demand skills\nsidebarTitle: \"Skills: SQL assistant\"\n---\n\nimport ChatModelTabsPy from '/snippets/chat-model-tabs.mdx';\nimport ChatModelTabsJs from '/snippets/chat-model-tabs-js.mdx';\n\nThis tutorial shows how to use **progressive disclosure** - a context management technique where the agent loads information on-demand rather than upfront - to implement **skills** (specialized prompt-based instructions). The agent loads skills via tool calls, rather than dynamically changing the system prompt, discovering and loading only the skills it needs for each task.\n\n"
  }
,
  {
    "path": "langchain/install.mdx",
    "filename": "install.mdx",
    "size_bytes": 2207,
    "line_count": 104,
    "preview": "---\ntitle: Install LangChain\nsidebarTitle: Install\n---\n\nTo install the LangChain package:\n\n:::python\n<CodeGroup>\n    ```bash pip\n"
  }
,
  {
    "path": "langchain/models.mdx",
    "filename": "models.mdx",
    "size_bytes": 74812,
    "line_count": 1941,
    "preview": "---\ntitle: Models\n---\n\nimport ChatModelTabsPy from '/snippets/chat-model-tabs.mdx';\nimport ChatModelTabsJS from '/snippets/chat-model-tabs-js.mdx';\n\n[LLMs](https://en.wikipedia.org/wiki/Large_language_model) are powerful AI tools that can interpret and generate text like humans. They're versatile enough to write content, translate languages, summarize, and answer questions without needing specialized training for each task.\n\nIn addition to text generation, many models support:\n"
  }
,
  {
    "path": "langchain/component-architecture.mdx",
    "filename": "component-architecture.mdx",
    "size_bytes": 3953,
    "line_count": 121,
    "preview": "---\ntitle: Component architecture\n---\n\nLangChain's power comes from how its components work together to create sophisticated AI applications. This page provides diagrams showcasing the relationships between different components.\n\n## Core component ecosystem\n\nThe diagram below shows how LangChain's major components connect to form complete AI applications:\n\n"
  }
,
  {
    "path": "langchain/runtime.mdx",
    "filename": "runtime.mdx",
    "size_bytes": 8079,
    "line_count": 269,
    "preview": "---\ntitle: Runtime\n---\n\n## Overview\n\n:::python\nLangChain's @[`create_agent`] runs on LangGraph's runtime under the hood.\n:::\n:::js\n"
  }
,
  {
    "path": "langchain/streaming/overview.mdx",
    "filename": "overview.mdx",
    "size_bytes": 41201,
    "line_count": 1107,
    "preview": "---\ntitle: Overview\ndescription: Stream real-time updates from agent runs\n---\n\nLangChain implements a streaming system to surface real-time updates.\n\nStreaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\n\n## Overview\n"
  }
,
  {
    "path": "langchain/streaming/frontend.mdx",
    "filename": "frontend.mdx",
    "size_bytes": 73340,
    "line_count": 2474,
    "preview": "---\ntitle: Frontend\ndescription: Build generative UIs with real-time streaming from LangChain agents, LangGraph graphs, and custom APIs\n---\n\nThe `useStream` React hook provides seamless integration with LangGraph streaming capabilities. It handles all the complexities of streaming, state management, and branching logic, letting you focus on building great generative UI experiences.\n\nKey features:\n\n* <Icon icon=\"messages\" size={16} /> **Messages streaming** — Handle a stream of message chunks to form a complete message\n"
  }
,
  {
    "path": "langchain/long-term-memory.mdx",
    "filename": "long-term-memory.mdx",
    "size_bytes": 10187,
    "line_count": 333,
    "preview": "---\ntitle: Long-term memory\n---\n\n## Overview\n\nLangChain agents use [LangGraph persistence](/oss/langgraph/persistence#memory-store) to enable long-term memory. This is a more advanced topic and requires knowledge of LangGraph to use.\n\n\n## Memory storage\n"
  }
,
  {
    "path": "langchain/messages.mdx",
    "filename": "messages.mdx",
    "size_bytes": 59553,
    "line_count": 1814,
    "preview": "---\ntitle: Messages\n---\n\n{/* TODO: section on metadata types (response and usage) */}\n\nMessages are the fundamental unit of context for models in LangChain. They represent the input and output of models, carrying both the content and metadata needed to represent the state of a conversation when interacting with an LLM.\n\nMessages are objects that contain:\n\n"
  }
,
  {
    "path": "langchain/ui.mdx",
    "filename": "ui.mdx",
    "size_bytes": 1093,
    "line_count": 23,
    "preview": "---\ntitle: Agent Chat UI\n---\n\nimport agent_chat_ui from '/snippets/oss/agent-chat-ui.mdx';\n\n<agent_chat_ui />\n\n### Connect to your agent\n\n"
  }
,
  {
    "path": "langchain/human-in-the-loop.mdx",
    "filename": "human-in-the-loop.mdx",
    "size_bytes": 20290,
    "line_count": 573,
    "preview": "---\ntitle: Human-in-the-loop\n---\n\nThe Human-in-the-Loop (HITL) [middleware](/oss/langchain/middleware/built-in#human-in-the-loop) lets you add human oversight to agent tool calls.\nWhen a model proposes an action that might require review — for example, writing to a file or executing SQL — the middleware can pause execution and wait for a decision.\n\nIt does this by checking each tool call against a configurable policy. If intervention is needed, the middleware issues an @[interrupt] that halts execution. The graph state is saved using LangGraph's [persistence layer](/oss/langgraph/persistence), so execution can pause safely and resume later.\n\nA human decision then determines what happens next: the action can be approved as-is (`approve`), modified before running (`edit`), or rejected with feedback (`reject`).\n"
  }
,
  {
    "path": "langchain/evals.mdx",
    "filename": "evals.mdx",
    "size_bytes": 6807,
    "line_count": 242,
    "preview": "---\ntitle: Evaluate agent performance\n---\n\nTo evaluate your agent's performance you can use `LangSmith` [evaluations](https://docs.langchain.com/langsmith/evaluation). You would need to first define an evaluator function to judge the results from an agent, such as final outputs or trajectory. Depending on your evaluation technique, this may or may not involve a reference output:\n\n:::python\n```python\ndef evaluator(*, outputs: dict, reference_outputs: dict):\n    # compare agent outputs against reference outputs\n"
  }
,
  {
    "path": "langchain/quickstart.mdx",
    "filename": "quickstart.mdx",
    "size_bytes": 20398,
    "line_count": 604,
    "preview": "---\ntitle: Quickstart\n---\n\nThis quickstart takes you from a simple setup to a fully functional AI agent in just a few minutes.\n\n<Tip>\n    **LangChain Docs MCP server**\n\n    If you're using an AI coding assistant or IDE (e.g. Claude Code or Cursor), you should install the [LangChain Docs MCP server](/use-these-docs) to get the most out of it. This ensures your agent has access to up-to-date LangChain documentation and examples.\n"
  }
,
  {
    "path": "langchain/structured-output.mdx",
    "filename": "structured-output.mdx",
    "size_bytes": 41098,
    "line_count": 1153,
    "preview": "---\ntitle: Structured output\n---\n\n:::python\n\nStructured output allows agents to return data in a specific, predictable format. Instead of parsing natural language responses, you get structured data in the form of JSON objects, [Pydantic models](https://docs.pydantic.dev/latest/concepts/models/#basic-model-usage), or dataclasses that your application can use directly.\n\nLangChain's @[`create_agent`] handles structured output automatically. The user sets their desired structured output schema, and when the model generates the structured data, it's captured, validated, and returned in the `'structured_response'` key of the agent's state.\n\n"
  }
,
  {
    "path": "langchain/errors/INVALID_PROMPT_INPUT.mdx",
    "filename": "INVALID_PROMPT_INPUT.mdx",
    "size_bytes": 2136,
    "line_count": 62,
    "preview": "---\ntitle: INVALID_PROMPT_INPUT\n---\n\n{/* TODO: fix link when porting page */}\nOccurs when a [prompt template](https://github.com/langchain-ai/langchain/blob/v0.3/docs/docs/concepts/prompt_templates.mdx) received missing or invalid input variables.\n\n:::js\nOne unexpected way this can occur is if you add a JSON object directly into a prompt template:\n\n"
  }
,
  {
    "path": "langchain/errors/MESSAGE_COERCION_FAILURE.mdx",
    "filename": "MESSAGE_COERCION_FAILURE.mdx",
    "size_bytes": 1663,
    "line_count": 59,
    "preview": "---\ntitle: MESSAGE_COERCION_FAILURE\n---\n\nThis error occurs when message objects don't conform to the expected format.\n\n:::python\n## Accepted message formats\n\nLangChain modules accept `MessageLikeRepresentation`, which is defined as:\n"
  }
,
  {
    "path": "langchain/errors/MODEL_RATE_LIMIT.mdx",
    "filename": "MODEL_RATE_LIMIT.mdx",
    "size_bytes": 1078,
    "line_count": 20,
    "preview": "---\ntitle: MODEL_RATE_LIMIT\n---\n\n<Note>\n    Currently only used in `langchainjs` (JavaScript/TypeScript).\n</Note>\n\nYou have hit the maximum number of requests that a model provider allows over a given time period and are being temporarily blocked.\n\n"
  }
,
  {
    "path": "langchain/errors/INVALID_TOOL_RESULTS.mdx",
    "filename": "INVALID_TOOL_RESULTS.mdx",
    "size_bytes": 2027,
    "line_count": 61,
    "preview": "---\ntitle: INVALID_TOOL_RESULTS\n---\n\n<Note>\n    Currently only used in `langchainjs` (JavaScript/TypeScript).\n</Note>\n\nThis error occurs when passing mismatched, insufficient, or excessive @[`ToolMessage`] objects to a model during tool calling operations.\n\n"
  }
,
  {
    "path": "langchain/errors/MODEL_NOT_FOUND.mdx",
    "filename": "MODEL_NOT_FOUND.mdx",
    "size_bytes": 725,
    "line_count": 18,
    "preview": "---\ntitle: MODEL_NOT_FOUND\n---\n\n<Note>\n    Currently only used in `langchainjs` (JavaScript/TypeScript).\n</Note>\n\nThe model name you have specified is not acknowledged by your provider.\n\n"
  }
,
  {
    "path": "langchain/errors/OUTPUT_PARSING_FAILURE.mdx",
    "filename": "OUTPUT_PARSING_FAILURE.mdx",
    "size_bytes": 695,
    "line_count": 15,
    "preview": "---\ntitle: OUTPUT_PARSING_FAILURE\n---\n\nAn [output parser](https://reference.langchain.com/python/langchain_core/output_parsers/) was unable to handle model output as expected.\n\n<Note>\n    Some prebuilt constructs like legacy LangChain agents and chains may use output parsers internally, so you may see this error even if you're not visibly instantiating and using an output parser.\n</Note>\n\n"
  }
,
  {
    "path": "langchain/errors/MODEL_AUTHENTICATION.mdx",
    "filename": "MODEL_AUTHENTICATION.mdx",
    "size_bytes": 1062,
    "line_count": 40,
    "preview": "---\ntitle: MODEL_AUTHENTICATION\n---\n\n<Note>\n    Currently only used in `langchainjs` (JavaScript/TypeScript).\n</Note>\n\nYour model provider is denying you access to their service.\n\n"
  }
,
  {
    "path": "langchain/context-engineering.mdx",
    "filename": "context-engineering.mdx",
    "size_bytes": 64993,
    "line_count": 2051,
    "preview": "---\ntitle: Context engineering in agents\nsidebarTitle: Context engineering\n---\n\n## Overview\n\nThe hard part of building agents (or any LLM application) is making them reliable enough. While they may work for a prototype, they often fail in real-world use cases.\n\n### Why do agents fail?\n"
  }
,
  {
    "path": "langchain/test.mdx",
    "filename": "test.mdx",
    "size_bytes": 30637,
    "line_count": 938,
    "preview": "---\ntitle: Test\n---\n\nAgentic applications let an LLM decide its own next steps to solve a problem. That flexibility is powerful, but the model's black-box nature makes it hard to predict how a tweak in one part of your agent will affect the rest. To build production-ready agents, thorough testing is essential.\n\nThere are a few approaches to testing your agents:\n:::python\n- [Unit tests](#unit-testing) exercise small, deterministic pieces of your agent in isolation using in-memory fakes so you can assert exact behavior quickly and deterministically.\n:::\n"
  }
,
  {
    "path": "langchain/mcp.mdx",
    "filename": "mcp.mdx",
    "size_bytes": 41084,
    "line_count": 1262,
    "preview": "---\ntitle: Model Context Protocol (MCP)\n---\n\n:::python\n[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchain-mcp-adapters) library.\n:::\n:::js\n[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the [`@langchain/mcp-adapters`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-mcp-adapters) library.\n:::\n"
  }
,
  {
    "path": "langchain/voice-agent.mdx",
    "filename": "voice-agent.mdx",
    "size_bytes": 26741,
    "line_count": 784,
    "preview": "---\ntitle: Build a voice agent with LangChain\nsidebarTitle: Voice agent\n---\n\n## Overview\n\nChat interfaces have dominated how we interact with AI, but recent breakthroughs in multimodal AI are opening up exciting new possibilities. High-quality generative models and expressive text-to-speech (TTS) systems now make it possible to build agents that feel less like tools and more like conversational partners.\n\nVoice agents are one example of this. Instead of relying on a keyboard and mouse to type inputs into an agent, you can use spoken words to interact with it. This can be a more natural and engaging way to interact with AI, and can be especially useful for certain contexts.\n"
  }
,
  {
    "path": "integrations/splitters/recursive_text_splitter.mdx",
    "filename": "recursive_text_splitter.mdx",
    "size_bytes": 5044,
    "line_count": 153,
    "preview": "---\ntitle: Splitting recursively\n---\n\nThis [text splitter](/oss/integrations/splitters/) is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is `[\"\\n\\n\", \"\\n\", \" \", \"\"]`. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n\n1. How the text is split: by list of characters.\n2. How the chunk size is measured: by number of characters.\n\nBelow we show example usage.\n"
  }
,
  {
    "path": "integrations/splitters/code_splitter.mdx",
    "filename": "code_splitter.mdx",
    "size_bytes": 30949,
    "line_count": 1035,
    "preview": "---\ntitle: Splitting code\n---\n\n@[RecursiveCharacterTextSplitter] includes pre-built lists of separators that are useful for [splitting text](/oss/integrations/splitters/) in a specific programming language.\n\n:::python\nSupported languages are stored in the `langchain_text_splitters.Language` enum. They include:\n\n```\n"
  }
,
  {
    "path": "integrations/splitters/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 5008,
    "line_count": 143,
    "preview": "---\ntitle: \"Text splitters\"\n---\n\n:::python\n<CodeGroup>\n    ```bash pip\n    pip install -U langchain-text-splitters\n    ```\n\n"
  }
,
  {
    "path": "integrations/splitters/split_by_token.mdx",
    "filename": "split_by_token.mdx",
    "size_bytes": 15476,
    "line_count": 464,
    "preview": "---\ntitle: Splitting by token\n---\n\nLanguage models have a token limit. You should not exceed the token limit. When you [split your text](/oss/integrations/splitters/) into chunks it is therefore a good idea to count the number of tokens. There are many tokenizers. When you count tokens in your text you should use the same tokenizer as used in the language model.\n\n:::python\n## tiktoken\n\n<Note>\n"
  }
,
  {
    "path": "integrations/splitters/character_text_splitter.mdx",
    "filename": "character_text_splitter.mdx",
    "size_bytes": 8886,
    "line_count": 145,
    "preview": "---\ntitle: Splitting by character\n---\n\nCharacter-based splitting is the simplest approach to text splitting. It divides text using a specified character sequence (default: `\"\\n\\n\"`), with chunk length measured by the number of characters.\n\n**Key points**:\n1. **How text is split**: by a given character separator.\n2. **How chunk size is measured**: by character count.\n\n"
  }
,
  {
    "path": "concepts/memory.mdx",
    "filename": "memory.mdx",
    "size_bytes": 22286,
    "line_count": 277,
    "preview": "---\ntitle: Memory overview\nsidebarTitle: Memory\n---\n\n\n\n[Memory](/oss/langgraph/add-memory) is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.\n\nThis conceptual guide covers two types of memory, based on their recall scope:\n"
  }
,
  {
    "path": "concepts/context.mdx",
    "filename": "context.mdx",
    "size_bytes": 11944,
    "line_count": 285,
    "preview": "---\ntitle: Context overview\nsidebarTitle: Context\n---\n\n**Context engineering** is the practice of building dynamic systems that provide the right information and tools, in the right format, so that an AI application can accomplish a task. Context can be characterized along two key dimensions:\n\n1. By **mutability**:\n  * **Static context**: Immutable data that doesn't change during execution (e.g., user metadata, database connections, tools)\n  * **Dynamic context**: Mutable data that evolves as the application runs (e.g., conversation history, intermediate results, tool call observations)\n"
  }
,
  {
    "path": "learn.mdx",
    "filename": "learn.mdx",
    "size_bytes": 3947,
    "line_count": 99,
    "preview": "---\ntitle: Learn\ndescription: Tutorials, conceptual guides, and resources to help you get started.\n---\n\n\n\nIn the **Learn** section of the documentation, you'll find a collection of tutorials, conceptual overviews, and additional resources to help you build powerful applications with LangChain and LangGraph.\n\n## Use cases\n"
  }
,
  {
    "path": "javascript/integrations/middleware/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 320,
    "line_count": 10,
    "preview": "---\ntitle: Provider-specific middleware\nsidebarTitle: Middleware\n---\n\nMiddleware designed for specific providers. Learn more about [middleware](/oss/langchain/middleware/overview).\n\n| Provider | Middleware available |\n|------------|-------------|\n| [Anthropic](/oss/integrations/middleware/anthropic) | Prompt caching |\n"
  }
,
  {
    "path": "javascript/integrations/middleware/anthropic.mdx",
    "filename": "anthropic.mdx",
    "size_bytes": 3291,
    "line_count": 80,
    "preview": "---\ntitle: Anthropic middleware\n---\n\nMiddleware specifically designed for Anthropic's Claude models. Learn more about [middleware](/oss/langchain/middleware/overview).\n\n| Middleware | Description |\n|------------|-------------|\n| [Prompt caching](#prompt-caching) | Reduce costs by caching repetitive prompt prefixes |\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/azure_openai.mdx",
    "filename": "azure_openai.mdx",
    "size_bytes": 16367,
    "line_count": 340,
    "preview": "---\ntitle: AzureOpenAIEmbeddings\n---\n\n[Azure OpenAI](https://azure.microsoft.com/products/ai-services/openai-service/) is a cloud service to help you quickly develop generative AI experiences with a diverse set of prebuilt and curated models from OpenAI, Meta and beyond.\n\nLangChain.js supports integration with [Azure OpenAI](https://azure.microsoft.com/products/ai-services/openai-service/) using the new Azure integration in the [OpenAI SDK](https://github.com/openai/openai-node).\n\nYou can learn more about Azure OpenAI and its difference with the OpenAI API on [this page](https://learn.microsoft.com/azure/ai-services/openai/overview). If you don't have an Azure account, you can [create a free account](https://azure.microsoft.com/free/) to get started.\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/gradient_ai.mdx",
    "filename": "gradient_ai.mdx",
    "size_bytes": 1222,
    "line_count": 45,
    "preview": "---\ntitle: Gradient AI\n---\n\nThe `GradientEmbeddings` class uses the Gradient AI API to generate embeddings for a given text.\n\n## Setup\n\nYou'll need to install the official Gradient Node SDK as a peer dependency:\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/tencent_hunyuan.mdx",
    "filename": "tencent_hunyuan.mdx",
    "size_bytes": 1572,
    "line_count": 51,
    "preview": "---\ntitle: TencentHunyuan\n---\n\nThe `TencentHunyuanEmbeddings` class uses the Tencent Hunyuan API to generate embeddings for a given text.\n\n## Setup\n\n1. Sign up for a Tencent Cloud account [here](https://cloud.tencent.com/register).\n2. Create SecretID & SecretKey [here](https://console.cloud.tencent.com/cam/capi).\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/google_generative_ai.mdx",
    "filename": "google_generative_ai.mdx",
    "size_bytes": 9807,
    "line_count": 207,
    "preview": "---\ntitle: GoogleGenerativeAIEmbeddings\n---\n\nThis will help you get started with Google Generative AI [embedding models](/oss/integrations/text_embedding) using LangChain. For detailed documentation on `GoogleGenerativeAIEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_google_genai.GoogleGenerativeAIEmbeddings.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/mistralai.mdx",
    "filename": "mistralai.mdx",
    "size_bytes": 14239,
    "line_count": 302,
    "preview": "---\ntitle: MistralAIEmbeddings\n---\n\nThis will help you get started with MistralAIEmbeddings [embedding models](/oss/integrations/text_embedding) using LangChain. For detailed documentation on `MistralAIEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_mistralai.MistralAIEmbeddings.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/nomic.mdx",
    "filename": "nomic.mdx",
    "size_bytes": 1080,
    "line_count": 42,
    "preview": "---\ntitle: Nomic\n---\n\nThe `NomicEmbeddings` class uses the Nomic AI API to generate embeddings for a given text.\n\n## Setup\n\nIn order to use the Nomic API you'll need an API key.\nYou can sign up for a Nomic account and create an API key [here](https://atlas.nomic.ai/).\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/ibm.mdx",
    "filename": "ibm.mdx",
    "size_bytes": 7427,
    "line_count": 243,
    "preview": "---\ntitle: IBM watsonx.ai\n---\n\nThis will help you get started with IBM watsonx.ai [embedding models](/oss/integrations/text_embedding) using LangChain. For detailed documentation on `IBM watsonx.ai` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/modules/_langchain_community.embeddings_ibm.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/hugging_face_inference.mdx",
    "filename": "hugging_face_inference.mdx",
    "size_bytes": 1702,
    "line_count": 40,
    "preview": "---\ntitle: HuggingFace Inference\n---\n\nThis Embeddings integration uses the HuggingFace Inference API to generate embeddings for a given text, using the `BAAI/bge-base-en-v1.5` model by default. You can pass a different model name to the constructor to use a different model.\n\n## Setup\n\nYou'll first need to install the [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) package and the required peer dependency:\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/premai.mdx",
    "filename": "premai.mdx",
    "size_bytes": 1266,
    "line_count": 43,
    "preview": "---\ntitle: Prem AI\n---\n\nThe `PremEmbeddings` class uses the Prem AI API to generate embeddings for a given text.\n\n## Setup\n\nIn order to use the Prem API you'll need an API key. You can sign up for a Prem account and create an API key [here](https://studio.premai.io/sign-up).\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 12219,
    "line_count": 586,
    "preview": "---\ntitle: \"Embedding models\"\n---\n\n## Overview\n\n<Note>\nThis overview covers **text-based embedding models**. LangChain does not currently support multimodal embeddings.\n</Note>\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/openai.mdx",
    "filename": "openai.mdx",
    "size_bytes": 11052,
    "line_count": 257,
    "preview": "---\ntitle: OpenAIEmbeddings\n---\n\nThis will help you get started with OpenAIEmbeddings [embedding models](/oss/integrations/text_embedding) using LangChain. For detailed documentation on `OpenAIEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_openai.OpenAIEmbeddings.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/zhipuai.mdx",
    "filename": "zhipuai.mdx",
    "size_bytes": 1013,
    "line_count": 38,
    "preview": "---\ntitle: ZhipuAI\n---\n\nThe `ZhipuAIEmbeddings` class uses the ZhipuAI API to generate embeddings for a given text.\n\n## Setup\n\nYou'll need to sign up for an ZhipuAI API key and set it as an environment variable named `ZHIPUAI_API_KEY`.\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/bedrock.mdx",
    "filename": "bedrock.mdx",
    "size_bytes": 10436,
    "line_count": 227,
    "preview": "---\ntitle: BedrockEmbeddings\n---\n\n\n[Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI.\n\nThis will help you get started with Amazon Bedrock [embedding models](/oss/integrations/text_embedding) using LangChain. For detailed documentation on `Bedrock` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_aws.BedrockEmbeddings.html).\n\n## Overview\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/bytedance_doubao.mdx",
    "filename": "bytedance_doubao.mdx",
    "size_bytes": 9580,
    "line_count": 198,
    "preview": "---\ntitle: ByteDanceDoubaoEmbeddings\n---\n\nThis will help you get started with ByteDanceDoubao [embedding models](/oss/integrations/text_embedding) using LangChain.  For detailed documentation on `ByteDanceDoubaoEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/_langchain_community.embeddings_bytedance_doubao.ByteDanceDoubaoEmbeddings.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/fireworks.mdx",
    "filename": "fireworks.mdx",
    "size_bytes": 12253,
    "line_count": 239,
    "preview": "---\ntitle: FireworksEmbeddings\n---\n\nThis will help you get started with FireworksEmbeddings [embedding models](/oss/integrations/text_embedding) using LangChain. For detailed documentation on `FireworksEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_community_embeddings_fireworks.FireworksEmbeddings.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/tensorflow.mdx",
    "filename": "tensorflow.mdx",
    "size_bytes": 1374,
    "line_count": 22,
    "preview": "---\ntitle: TensorFlow\n---\n\nThis Embeddings integration runs the embeddings entirely in your browser or Node.js environment, using [TensorFlow.js](https://www.tensorflow.org/js). This means that your data isn't sent to any third party, and you don't need to sign up for any API keys. However, it does require more memory and processing power than the other integrations.\n\n```bash npm\nnpm install @langchain/community @langchain/core @tensorflow/tfjs-core@3.6.0 @tensorflow/tfjs-converter@3.6.0 @tensorflow-models/universal-sentence-encoder@1.3.3 @tensorflow/tfjs-backend-cpu\n```\n```typescript\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/baidu_qianfan.mdx",
    "filename": "baidu_qianfan.mdx",
    "size_bytes": 1195,
    "line_count": 38,
    "preview": "---\ntitle: Baidu Qianfan\n---\n\nThe `BaiduQianfanEmbeddings` class uses the Baidu Qianfan API to generate embeddings for a given text.\n\n## Setup\n\nAn API key is required to use this embedding model. You can get one by registering at https://cloud.baidu.com/doc/WENXINWORKSHOP/s/alj562vvu.\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/alibaba_tongyi.mdx",
    "filename": "alibaba_tongyi.mdx",
    "size_bytes": 1013,
    "line_count": 36,
    "preview": "---\ntitle: Alibaba Tongyi\n---\n\nThe `AlibabaTongyiEmbeddings` class uses the Alibaba Tongyi API to generate embeddings for a given text.\n\n## Setup\n\nYou'll need to sign up for an Alibaba API key and set it as an environment variable named `ALIBABA_API_KEY`.\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/voyageai.mdx",
    "filename": "voyageai.mdx",
    "size_bytes": 1986,
    "line_count": 36,
    "preview": "---\ntitle: Voyage AI\n---\n\nThe `VoyageEmbeddings` class uses the Voyage AI REST API to generate embeddings for a given text.\n\nThe `inputType` parameter allows you to specify the type of input text for better embedding results. You can set it to `query`, `document`, or leave it undefined (which is equivalent to `None`).\n\n- `query`: Use this for search or retrieval queries. Voyage AI will prepend a prompt to optimize the embeddings for query use cases.\n- `document`: Use this for documents or content that you want to be retrievable. Voyage AI will prepend a prompt to optimize the embeddings for document use cases.\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/pinecone.mdx",
    "filename": "pinecone.mdx",
    "size_bytes": 12084,
    "line_count": 239,
    "preview": "---\ntitle: PineconeEmbeddings\n---\n\nThis will help you get started with PineconeEmbeddings [embedding models](/oss/integrations/text_embedding) using LangChain. For detailed documentation on `PineconeEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/_langchain_pinecone.PineconeEmbeddings.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/minimax.mdx",
    "filename": "minimax.mdx",
    "size_bytes": 797,
    "line_count": 33,
    "preview": "---\ntitle: Minimax\n---\n\nThe `MinimaxEmbeddings` class uses the Minimax API to generate embeddings for a given text.\n\n# Setup\n\nTo use Minimax model, you'll need a Minimax account, an API key, and a Group ID.\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/google_vertex_ai.mdx",
    "filename": "google_vertex_ai.mdx",
    "size_bytes": 12888,
    "line_count": 253,
    "preview": "---\ntitle: VertexAIEmbeddings\n---\n\n[Google Vertex](https://cloud.google.com/vertex-ai) is a service that exposes all foundation models available in Google Cloud.\n\nThis will help you get started with Google Vertex AI [embedding models](/oss/integrations/text_embedding) using LangChain. For detailed documentation on `VertexAIEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_google_vertexai.GoogleVertexAIEmbeddings.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/deepinfra.mdx",
    "filename": "deepinfra.mdx",
    "size_bytes": 4006,
    "line_count": 124,
    "preview": "---\ntitle: DeepInfra Embeddings\n---\n\nThe `DeepInfraEmbeddings` class utilizes the DeepInfra API to generate embeddings for given text inputs. This guide will walk you through the setup and usage of the `DeepInfraEmbeddings` class, helping you integrate it into your project seamlessly.\n\n## Installation\n\nInstall the `@langchain/community` package as shown below:\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/jina.mdx",
    "filename": "jina.mdx",
    "size_bytes": 3678,
    "line_count": 128,
    "preview": "---\ntitle: Jina Embeddings\n---\n\nThe `JinaEmbeddings` class utilizes the Jina API to generate embeddings for given text inputs. This guide will walk you through the setup and usage of the `JinaEmbeddings` class, helping you integrate it into your project seamlessly.\n\n## Installation\n\nInstall the `@langchain/community` package as shown below:\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/mixedbread_ai.mdx",
    "filename": "mixedbread_ai.mdx",
    "size_bytes": 2599,
    "line_count": 87,
    "preview": "---\ntitle: Mixedbread AI\n---\n\nThe `MixedbreadAIEmbeddings` class uses the [Mixedbread AI](https://mixedbread.ai/) API to generate text embeddings. This guide will walk you through setting up and using the `MixedbreadAIEmbeddings` class, helping you integrate it into your project effectively.\n\n## Installation\n\nTo install the `@langchain/mixedbread-ai` package, use the following command:\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/togetherai.mdx",
    "filename": "togetherai.mdx",
    "size_bytes": 9242,
    "line_count": 197,
    "preview": "---\ntitle: TogetherAIEmbeddings\n---\n\nThis will help you get started with TogetherAIEmbeddings [embedding models](/oss/integrations/text_embedding) using LangChain. For detailed documentation on `TogetherAIEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_community_embeddings_togetherai.TogetherAIEmbeddings.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/cloudflare_ai.mdx",
    "filename": "cloudflare_ai.mdx",
    "size_bytes": 4071,
    "line_count": 132,
    "preview": "---\ntitle: CloudflareWorkersAIEmbeddings\n---\n\nThis will help you get started with Cloudflare Workers AI [embedding models](/oss/integrations/text_embedding) using LangChain. For detailed documentation on `CloudflareWorkersAIEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_cloudflare.CloudflareWorkersAIEmbeddings.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/transformers.mdx",
    "filename": "transformers.mdx",
    "size_bytes": 1940,
    "line_count": 59,
    "preview": "---\ntitle: HuggingFace Transformers\n---\n\nThe `TransformerEmbeddings` class uses the [Transformers.js](https://huggingface.co/docs/transformers.js/index) package to generate embeddings for a given text.\n\nIt runs locally and even works directly in the browser, allowing you to create web apps with built-in embeddings.\n\n## Setup\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/cohere.mdx",
    "filename": "cohere.mdx",
    "size_bytes": 10122,
    "line_count": 221,
    "preview": "---\ntitle: CohereEmbeddings\n---\n\nThis will help you get started with CohereEmbeddings [embedding models](/oss/integrations/text_embedding) using LangChain. For detailed documentation on `CohereEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_cohere.CohereEmbeddings.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/ollama.mdx",
    "filename": "ollama.mdx",
    "size_bytes": 9607,
    "line_count": 211,
    "preview": "---\ntitle: OllamaEmbeddings\n---\n\nThis will help you get started with Ollama [embedding models](/oss/integrations/text_embedding) using LangChain. For detailed documentation on `OllamaEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_ollama.OllamaEmbeddings.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "javascript/integrations/text_embedding/llama_cpp.mdx",
    "filename": "llama_cpp.mdx",
    "size_bytes": 2869,
    "line_count": 85,
    "preview": "---\ntitle: Llama CPP\n---\n\n<Tip>\n**Compatibility**\n\nOnly available on Node.js.\n</Tip>\n\n"
  }
,
  {
    "path": "javascript/integrations/retrievers/vespa-retriever.mdx",
    "filename": "vespa-retriever.mdx",
    "size_bytes": 1780,
    "line_count": 53,
    "preview": "---\ntitle: Vespa Retriever\n---\n\nThis shows how to use Vespa.ai as a LangChain retriever.\nVespa.ai is a platform for highly efficient structured text and vector search.\nPlease refer to [Vespa.ai](https://vespa.ai) for more information.\n\nThe following sets up a retriever that fetches results from Vespa's documentation search:\n\n"
  }
,
  {
    "path": "javascript/integrations/retrievers/hyde.mdx",
    "filename": "hyde.mdx",
    "size_bytes": 1650,
    "line_count": 49,
    "preview": "---\ntitle: HyDE Retriever\n---\n\nThis example shows how to use the HyDE Retriever, which implements Hypothetical Document Embeddings (HyDE) as described in [this paper](https://arxiv.org/abs/2212.10496).\n\nAt a high level, HyDE is an embedding technique that takes queries, generates a hypothetical answer, and then embeds that generated document and uses that as the final example.\n\nIn order to use HyDE, we therefore need to provide a base embedding model, as well as an LLM that can be used to generate those documents. By default, the HyDE class comes with some default prompts to use (see the paper for more details on them), but we can also create our own, which should have a single input variable `{question}`.\n\n"
  }
,
  {
    "path": "javascript/integrations/retrievers/azion-edgesql.mdx",
    "filename": "azion-edgesql.mdx",
    "size_bytes": 3598,
    "line_count": 116,
    "preview": "---\ntitle: AzionRetriever\n---\n\n## Overview\n\nThis will help you getting started with the [AzionRetriever](/oss/langchain/retrieval). For detailed documentation of all AzionRetriever features and configurations head to the [API reference](https://api.js.langchain.com/classes/_langchain_community.retrievers_azion_edgesql.AzionRetriever.html).\n\n### Integration details\n\n"
  }
,
  {
    "path": "javascript/integrations/retrievers/zep-retriever.mdx",
    "filename": "zep-retriever.mdx",
    "size_bytes": 5234,
    "line_count": 173,
    "preview": "---\ntitle: Zep Open Source Retriever\n---\n\n> [Zep](https://www.getzep.com) is a long-term memory service for AI Assistant apps.\n> With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant,\n> while also reducing hallucinations, latency, and cost.\n\n> Interested in Zep Cloud? See [Zep Cloud Installation Guide](https://help.getzep.com/sdks)\n\n"
  }
,
  {
    "path": "javascript/integrations/retrievers/zep-cloud-retriever.mdx",
    "filename": "zep-cloud-retriever.mdx",
    "size_bytes": 4857,
    "line_count": 161,
    "preview": "---\ntitle: Zep Cloud Retriever\n---\n\n> [Zep](https://www.getzep.com) is a long-term memory service for AI Assistant apps.\n> With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant,\n> while also reducing hallucinations, latency, and cost.\n\nThis example shows how to use the Zep Retriever in a retrieval chain to retrieve documents from Zep Open Source memory store.\n\n"
  }
,
  {
    "path": "javascript/integrations/retrievers/supabase-hybrid.mdx",
    "filename": "supabase-hybrid.mdx",
    "size_bytes": 3378,
    "line_count": 115,
    "preview": "---\ntitle: Supabase Hybrid Search\n---\n\nLangChain supports hybrid search with a Supabase Postgres database. The hybrid search combines the postgres `pgvector` extension (similarity search) and Full-Text Search (keyword search) to retrieve documents. You can add documents via SupabaseVectorStore `addDocuments` function. SupabaseHybridKeyWordSearch accepts embedding, supabase client, number of results for similarity search, and number of results for keyword search as parameters. The `getRelevantDocuments` function produces a list of documents that has duplicates removed and is sorted by relevance score.\n\n## Setup\n\n### Install the library with\n\n"
  }
,
  {
    "path": "javascript/integrations/retrievers/time-weighted-retriever.mdx",
    "filename": "time-weighted-retriever.mdx",
    "size_bytes": 2374,
    "line_count": 70,
    "preview": "---\ntitle: Time-Weighted Retriever\n---\n\nA Time-Weighted Retriever is a retriever that takes into account recency in addition to similarity. The scoring algorithm is:\n\n```typescript\nlet score = (1.0 - this.decayRate) ** hoursPassed + vectorRelevance;\n```\n\n"
  }
,
  {
    "path": "javascript/integrations/retrievers/bedrock-knowledge-bases.mdx",
    "filename": "bedrock-knowledge-bases.mdx",
    "size_bytes": 3164,
    "line_count": 86,
    "preview": "---\ntitle: Knowledge Bases for Amazon Bedrock\n---\n\n## Overview\n\nThis will help you getting started with the [AmazonKnowledgeBaseRetriever](/oss/langchain/retrieval). For detailed documentation of all AmazonKnowledgeBaseRetriever features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_aws.AmazonKnowledgeBaseRetriever.html).\n\nKnowledge Bases for Amazon Bedrock is a fully managed support for end-to-end RAG workflow provided by Amazon Web Services (AWS).\nIt provides an entire ingestion workflow of converting your documents into embeddings (vector) and storing the embeddings in a specialized vector database.\n"
  }
,
  {
    "path": "javascript/integrations/retrievers/chatgpt-retriever-plugin.mdx",
    "filename": "chatgpt-retriever-plugin.mdx",
    "size_bytes": 802,
    "line_count": 32,
    "preview": "---\ntitle: ChatGPT Plugin Retriever\n---\n\n<Warning>\nThis module has been deprecated and is no longer supported. The documentation below will not work in versions 0.2.0 or later.\n</Warning>\n\nThis example shows how to use the ChatGPT Retriever Plugin within LangChain.\n\n"
  }
,
  {
    "path": "javascript/integrations/retrievers/chaindesk-retriever.mdx",
    "filename": "chaindesk-retriever.mdx",
    "size_bytes": 777,
    "line_count": 33,
    "preview": "---\ntitle: Chaindesk Retriever\n---\n\nThis example shows how to use the Chaindesk Retriever in a retrieval chain to retrieve documents from a Chaindesk.ai datastore.\n\n## Usage\n\n<Tip>\nSee [this section for general instructions on installing LangChain packages](/oss/langchain/install).\n"
  }
,
  {
    "path": "javascript/integrations/retrievers/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 3893,
    "line_count": 142,
    "preview": "---\ntitle: Retrievers\n---\n\nA [retriever](/oss/langchain/retrieval) is an interface that returns documents given an unstructured query.\nIt is more general than a vector store.\nA retriever does not need to be able to store documents, only to return (or retrieve) them.\n\nRetrievers accept a string query as input and return a list of `Document` objects.\n\n"
  }
,
  {
    "path": "javascript/integrations/retrievers/dria.mdx",
    "filename": "dria.mdx",
    "size_bytes": 1878,
    "line_count": 52,
    "preview": "---\ntitle: Dria Retriever\n---\n\nThe [Dria](https://dria.co/) retriever allows an agent to perform a text-based search across a comprehensive knowledge hub.\n\n## Setup\n\nTo use Dria retriever, first install Dria JS client:\n\n"
  }
,
  {
    "path": "javascript/integrations/retrievers/kendra-retriever.mdx",
    "filename": "kendra-retriever.mdx",
    "size_bytes": 2790,
    "line_count": 81,
    "preview": "---\ntitle: AWSKendraRetriever\n---\n\n## Overview\n\n[Amazon Kendra](https://aws.amazon.com/kendra/) is an intelligent search service provided by Amazon Web Services (AWS).\nIt utilizes advanced natural language processing (NLP) and machine learning algorithms to enable powerful search capabilities across various data sources within an organization.\nKendra is designed to help users find the information they need quickly and accurately, improving productivity and decision-making.\n\n"
  }
,
  {
    "path": "javascript/integrations/retrievers/tavily.mdx",
    "filename": "tavily.mdx",
    "size_bytes": 4694,
    "line_count": 105,
    "preview": "---\ntitle: TavilySearchAPIRetriever\n---\n\n[Tavily's Search API](https://tavily.com) is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.\n\n## Overview\n\nThis will help you getting started with the Tavily Search API [retriever](/oss/langchain/retrieval). For detailed documentation of all `TavilySearchAPIRetriever` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_retrievers_tavily_search_api.TavilySearchAPIRetriever.html).\n\n"
  }
,
  {
    "path": "javascript/integrations/retrievers/exa.mdx",
    "filename": "exa.mdx",
    "size_bytes": 24252,
    "line_count": 192,
    "preview": "---\ntitle: ExaRetriever\n---\n\n## Overview\n\n[Exa](https://exa.ai/) is a search engine that retrieves relevant content from the web given some input query.\n\nThis guide will help you getting started with the Exa [retriever](/oss/langchain/retrieval). For detailed documentation of all `ExaRetriever` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_exa.ExaRetriever.html).\n\n"
  }
,
  {
    "path": "javascript/integrations/retrievers/bm25.mdx",
    "filename": "bm25.mdx",
    "size_bytes": 1975,
    "line_count": 62,
    "preview": "---\ntitle: BM25\n---\n\nBM25, also known as [Okapi BM25](https://en.wikipedia.org/wiki/Okapi_BM25), is a ranking function used in information retrieval systems to estimate the relevance of documents to a given search query.\n\nYou can use it as part of your retrieval pipeline as a to rerank documents as a postprocessing step after retrieving an initial set of documents from another source.\n\n## Setup\n\n"
  }
,
  {
    "path": "javascript/integrations/retrievers/alchemystai-retriever.mdx",
    "filename": "alchemystai-retriever.mdx",
    "size_bytes": 2332,
    "line_count": 73,
    "preview": "---\ntitle: Alchemyst AI Retriever\ndescription: \"Integrate the Alchemyst AI Retriever into your Generative AI Application\"\n---\n\n# Alchemyst AI retriever\n\nThe [**Alchemyst AI Retriever**](https://getalchemystai.com) enables your generative AI applications to retrieve relevant context and knowledge. It sources this information from the Alchemyst platform. It provides a unified interface for accessing, searching, and retrieving data to enhance LLM and agent responses.\n## Setup\n\n"
  }
,
  {
    "path": "javascript/integrations/retrievers/metal-retriever.mdx",
    "filename": "metal-retriever.mdx",
    "size_bytes": 934,
    "line_count": 42,
    "preview": "---\ntitle: Metal Retriever\n---\n\nThis example shows how to use the Metal Retriever in a retrieval chain to retrieve documents from a Metal index.\n\n## Setup\n\n<Tip>\nSee [this section for general instructions on installing LangChain packages](/oss/langchain/install).\n"
  }
,
  {
    "path": "javascript/integrations/retrievers/arxiv-retriever.mdx",
    "filename": "arxiv-retriever.mdx",
    "size_bytes": 3320,
    "line_count": 107,
    "preview": "---\ntitle: ArxivRetriever\n---\n\nThe `arXiv Retriever` allows users to query the arXiv database for academic articles. It supports both full-document retrieval (PDF parsing) and summary-based retrieval.\n\nFor detailed documentation of all ArxivRetriever features and configurations, head to the [API reference](https://api.js.langchain.com/classes/_langchain_community.retrievers_arxiv.ArxivRetriever.html)\n\n## Features\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/sfn_agent.mdx",
    "filename": "sfn_agent.mdx",
    "size_bytes": 3179,
    "line_count": 88,
    "preview": "---\ntitle: AWS Step Functions Toolkit\n---\n\n**AWS Step Functions** are a visual workflow service that helps developers use AWS services to build distributed applications, automate processes, orchestrate microservices, and create data and machine learning (ML) pipelines.\n\nBy including a `AWSSfn` tool in the list of tools provided to an Agent, you can grant your Agent the ability to invoke async workflows running in your AWS Cloud.\n\nWhen an Agent uses the `AWSSfn` tool, it will provide an argument of type `string` which will in turn be passed into one of the supported actions this tool supports. The supported actions are: `StartExecution`, `DescribeExecution`, and `SendTaskSuccess`.\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/duckduckgo_search.mdx",
    "filename": "duckduckgo_search.mdx",
    "size_bytes": 5284,
    "line_count": 162,
    "preview": "---\ntitle: DuckDuckGoSearch\n---\n\nThis notebook provides a quick overview for getting started with [DuckDuckGoSearch](/oss/integrations/tools/). For detailed documentation of all DuckDuckGoSearch features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_tools_duckduckgo_search.DuckDuckGoSearch.html).\n\nDuckDuckGoSearch offers a privacy-focused search API designed for LLM Agents. It provides seamless integration with a wide range of data sources, prioritizing user privacy and relevant search results.\n\n## Overview\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/serpapi.mdx",
    "filename": "serpapi.mdx",
    "size_bytes": 5092,
    "line_count": 173,
    "preview": "---\ntitle: SerpApi\n---\n\n[SerpApi](https://serpapi.com/) allows you to integrate search engine results into your LLM apps\n\nThis guide provides a quick overview for getting started with the SerpApi [tool](/oss/integrations/tools/). For detailed documentation of all `SerpAPI` features and configurations head to the [API reference](https://api.js.langchain.com/classes/_langchain_community.tools_serpapi.SerpAPI.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/vectorstore.mdx",
    "filename": "vectorstore.mdx",
    "size_bytes": 5158,
    "line_count": 159,
    "preview": "---\ntitle: VectorStoreToolkit\n---\n\nThis will help you getting started with the [VectorStoreToolkit](/oss/langchain/tools#toolkits). For detailed documentation of all VectorStoreToolkit features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.agents.VectorStoreToolkit.html).\n\nThe `VectorStoreToolkit` is a toolkit which takes in a vector store, and converts it to a tool which can then be invoked, passed to LLMs, agents and more.\n\n## Setup\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/searxng.mdx",
    "filename": "searxng.mdx",
    "size_bytes": 2502,
    "line_count": 71,
    "preview": "---\ntitle: Searxng Search tool\n---\n\nThe `SearxngSearch` tool connects your agents and chains to the internet.\n\nA wrapper around the SearxNG API, this tool is useful for performing meta-search engine queries using the SearxNG API. It is particularly helpful in answering questions about current events.\n\n## Usage\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/searchapi.mdx",
    "filename": "searchapi.mdx",
    "size_bytes": 1710,
    "line_count": 62,
    "preview": "---\ntitle: SearchApi tool\n---\n\nThe `SearchApi` tool connects your agents and chains to the internet.\n\nA wrapper around the Search API. This tool is handy when you need to answer questions about current events.\n\n## Usage\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/google_routes.mdx",
    "filename": "google_routes.mdx",
    "size_bytes": 2634,
    "line_count": 87,
    "preview": "---\ntitle: Google Routes Tool\n---\n\nThe Google Routes Tool allows your agent to utilize the Google Routes API in order to find a route between\ntwo or more destinations. You can get a route by walk, transit, car, motorcycle and bicycle.\n\n## Setup\n\nYou will need to get an API key from [Google here](https://developers.google.com/maps/documentation/places/web-service/overview)\n"
  }
,
  {
    "path": "javascript/integrations/tools/tavily_map.mdx",
    "filename": "tavily_map.mdx",
    "size_bytes": 4840,
    "line_count": 167,
    "preview": "---\ntitle: TavilyMap\n---\n\n[Tavily](https://tavily.com/) is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed. Tavily offers four key endpoints, one of which being Map, which traverses websites like a graph and can explore hundreds of paths in parallel with intelligent discovery to generate comprehensive site maps.\n\nThis guide provides a quick overview for getting started with the Tavily [tool](/oss/integrations/tools/). For a complete breakdown of the Tavily tool, you can find more detailed documentation in the [API reference](https://v03.api.js.langchain.com/modules/_langchain_tavily.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/sql.mdx",
    "filename": "sql.mdx",
    "size_bytes": 7455,
    "line_count": 216,
    "preview": "---\ntitle: SqlToolkit\n---\n\nThis will help you getting started with the [SqlToolkit](/oss/langchain/tools#toolkits). For detailed documentation of all SqlToolkit features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.agents_toolkits_sql.SqlToolkit.html). You can also find the documentation for the Python equivalent [here](https://python.langchain.com/docs/integrations/toolkits/sql_database/).\n\nThis toolkit contains a the following tools:\n\n| Name              | Description                                                                                                                                                                                                                               |\n|-------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n"
  }
,
  {
    "path": "javascript/integrations/tools/google_scholar.mdx",
    "filename": "google_scholar.mdx",
    "size_bytes": 2748,
    "line_count": 89,
    "preview": "---\ntitle: Google Scholar Tool\n---\n\nThis notebook provides a quick overview for getting started with [`SERPGoogleScholarTool`](https://api.js.langchain.com/classes/_langchain_community.tools_google_scholar.SERPGoogleScholarAPITool.html). For detailed documentation of all `SERPGoogleScholarAPITool` features and configurations, head to the [API reference](https://api.js.langchain.com/classes/_langchain_community.tools_google_scholar.SERPGoogleScholarAPITool.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/ibm.mdx",
    "filename": "ibm.mdx",
    "size_bytes": 7942,
    "line_count": 176,
    "preview": "---\ntitle: WatsonxToolkit\n---\n\nThis will help you getting started with the [WatsonxToolkit](/oss/concepts/#toolkits). For detailed documentation of all WatsonxToolkit features and configurations head to the [API reference](https://api.js.langchain.com/modules/_langchain_community.agents_toolkits_ibm.html).\n\nThe toolkit contains following tools:\n\n| Name | Description |\n| ---- | ----------- |\n"
  }
,
  {
    "path": "javascript/integrations/tools/composio.mdx",
    "filename": "composio.mdx",
    "size_bytes": 15492,
    "line_count": 529,
    "preview": "---\ntitle: Composio\ndescription: Access 500+ tools and integrations through Composio's unified API platform for AI agents, with OAuth handling, event-driven workflows, and multi-user support.\n---\n\n[Composio](https://composio.dev) is an integration platform that provides access to 500+ tools across popular applications like GitHub, Slack, Notion, and more. It enables AI agents to interact with external services through a unified API, handling authentication, permissions, and event-driven workflows.\n\n## Overview\n\n### Integration details\n"
  }
,
  {
    "path": "javascript/integrations/tools/google_places.mdx",
    "filename": "google_places.mdx",
    "size_bytes": 1524,
    "line_count": 52,
    "preview": "---\ntitle: Google Places Tool\n---\n\nThe Google Places Tool allows your agent to utilize the Google Places API in order to find addresses,\nphone numbers, website, etc. from text about a location listed on Google Places.\n\n## Setup\n\nYou will need to get an API key from [Google here](https://developers.google.com/maps/documentation/places/web-service/overview)\n"
  }
,
  {
    "path": "javascript/integrations/tools/wikipedia.mdx",
    "filename": "wikipedia.mdx",
    "size_bytes": 478,
    "line_count": 25,
    "preview": "---\ntitle: Wikipedia tool\n---\n\nThe `WikipediaQueryRun` tool connects your agents and chains to Wikipedia.\n\n## Usage\n\n```typescript\nimport { WikipediaQueryRun } from \"@langchain/community/tools/wikipedia_query_run\";\n"
  }
,
  {
    "path": "javascript/integrations/tools/stagehand.mdx",
    "filename": "stagehand.mdx",
    "size_bytes": 2805,
    "line_count": 123,
    "preview": "---\ntitle: Stagehand Toolkit\n---\n\n```typescript\nimport { Stagehand } from \"@browserbasehq/stagehand\";\nimport {\n  StagehandActTool,\n  StagehandNavigateTool,\n} from \"@langchain/community/agents/toolkits/stagehand\";\n"
  }
,
  {
    "path": "javascript/integrations/tools/exa_search.mdx",
    "filename": "exa_search.mdx",
    "size_bytes": 31057,
    "line_count": 302,
    "preview": "---\ntitle: ExaSearchResults\n---\n\nExa (formerly Metaphor Search) is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.\n\nUnlike keyword-based search (Google), Exa's neural search capabilities allow it to semantically understand queries and return relevant documents. For example, we could search `\"fascinating article about cats\"` and compare the search results from Google and Exa. Google gives us SEO-optimized listicles based on the keyword “fascinating”. Exa just works.\n\nThis page goes over how to use `ExaSearchResults` with LangChain.\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/azure_dynamic_sessions.mdx",
    "filename": "azure_dynamic_sessions.mdx",
    "size_bytes": 3874,
    "line_count": 103,
    "preview": "---\ntitle: Azure Container Apps Dynamic Sessions\n---\n\n> [Azure Container Apps dynamic sessions](https://learn.microsoft.com/azure/container-apps/sessions) provide fast access to secure sandboxed environments that are ideal for running code or applications that require strong isolation from other workloads.\n\nYou can learn more about Azure Container Apps dynamic sessions and its code interpretation capabilities on [this page](https://learn.microsoft.com/azure/container-apps/sessions). If you don't have an Azure account, you can [create a free account](https://azure.microsoft.com/free/) to get started.\n\n## Setup\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/json.mdx",
    "filename": "json.mdx",
    "size_bytes": 1397,
    "line_count": 54,
    "preview": "---\ntitle: JSON Agent Toolkit\n---\n\nThis example shows how to load and use an agent with a JSON toolkit.\n\n<Tip>\nSee [this section for general instructions on installing LangChain packages](/oss/langchain/install).\n</Tip>\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 5446,
    "line_count": 241,
    "preview": "---\ntitle: Tools and Toolkits\n---\n\n[Tools](/oss/langchain/tools) are utilities designed to be called by a model: their inputs are designed to be generated by models, and their outputs are designed to be passed back to models.\n\nA [toolkit](/oss/langchain/tools#toolkits) is a collection of tools meant to be used together.\n\n## Integration platforms\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/openai.mdx",
    "filename": "openai.mdx",
    "size_bytes": 19896,
    "line_count": 584,
    "preview": "---\ntitle: Tools\n---\n\nThe `@langchain/openai` package provides LangChain-compatible wrappers for OpenAI's built-in tools. These tools can be bound to `ChatOpenAI` using `bindTools()` or @[`createAgent`].\n\n### Web search tool\n\nThe web search tool allows OpenAI models to search the web for up-to-date information before generating a response. Web search supports three main types:\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/mcp_toolbox.mdx",
    "filename": "mcp_toolbox.mdx",
    "size_bytes": 3186,
    "line_count": 75,
    "preview": "---\ntitle: MCP Toolbox For Databases\n---\n\n[MCP Toolbox for Databases](https://github.com/googleapis/genai-toolbox) is an open source MCP server for databases. It was designed with enterprise-grade and production-quality in mind. It enables you to develop tools easier, faster, and more securely by handling the complexities such as connection pooling, authentication, and more.\n\nToolbox Tools can be seemlessly integrated with LangChain applications. For more\ninformation on [getting\nstarted](https://googleapis.github.io/genai-toolbox/getting-started/local_quickstart_js/) or\n[configuring](https://googleapis.github.io/genai-toolbox/getting-started/configure/)\n"
  }
,
  {
    "path": "javascript/integrations/tools/tavily_crawl.mdx",
    "filename": "tavily_crawl.mdx",
    "size_bytes": 4943,
    "line_count": 170,
    "preview": "---\ntitle: TavilyCrawl\n---\n\n[Tavily](https://tavily.com/) is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed. Tavily offers four key endpoints, one of which being Crawl, which provides a graph-based website traversal tool that can explore hundreds of paths in parallel with built-in extraction and intelligent discovery.\n\nThis guide provides a quick overview for getting started with the Tavily [tool](/oss/integrations/tools/). For a complete breakdown of the Tavily tool, you can find more detailed documentation in the [API reference](https://v03.api.js.langchain.com/modules/_langchain_tavily.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/google_trends.mdx",
    "filename": "google_trends.mdx",
    "size_bytes": 1505,
    "line_count": 46,
    "preview": "---\ntitle: Google Trends Tool\n---\n\nThe Google Trends Tool allows your agent to utilize the Google Trends API from SerpApi to retrieve and analyze search interest data.\nThis can be useful for understanding trending topics, regional search interest, and historical popularity of search terms.\n\nFor API details see [here](https://serpapi.com/google-trends-api)\n\nSerpApi caches queries, so the first query will be slower while subsequent identical queries will be fast.\n"
  }
,
  {
    "path": "javascript/integrations/tools/connery.mdx",
    "filename": "connery.mdx",
    "size_bytes": 1630,
    "line_count": 53,
    "preview": "---\ntitle: Connery Action Tool\n---\n\n```typescript\nimport { ConneryService } from \"@langchain/community/tools/connery\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { initializeAgentExecutorWithOptions } from \"@langchain/classic/agents\";\n\n// Specify your Connery Runner credentials.\n"
  }
,
  {
    "path": "javascript/integrations/tools/jigsawstack.mdx",
    "filename": "jigsawstack.mdx",
    "size_bytes": 5938,
    "line_count": 159,
    "preview": "---\ntitle: JigsawStack Tool\n---\n\nThe JigsawStack Tool provides your agent with the following capabilities:\n\n- JigsawStackAIScrape: Scrape web content using advanced AI.\n\n- JigsawStackAISearch: Perform AI-powered web searches and retrieve high-quality results.\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/aiplugin-tool.mdx",
    "filename": "aiplugin-tool.mdx",
    "size_bytes": 5076,
    "line_count": 73,
    "preview": "---\ntitle: ChatGPT Plugins\n---\n\n```typescript\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { initializeAgentExecutorWithOptions } from \"@langchain/classic/agents\";\nimport { RequestsGetTool, RequestsPostTool } from \"@langchain/classic/tools\";\nimport { AIPluginTool } from \"@langchain/community/tools/aiplugin\";\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/connery_toolkit.mdx",
    "filename": "connery_toolkit.mdx",
    "size_bytes": 1543,
    "line_count": 45,
    "preview": "---\ntitle: Connery Toolkit\n---\n\n```typescript\nimport { ConneryService } from \"@langchain/community/tools/connery\";\nimport { ConneryToolkit } from \"@langchain/community/agents/toolkits/connery\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { initializeAgentExecutorWithOptions } from \"@langchain/classic/agents\";\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/goat.mdx",
    "filename": "goat.mdx",
    "size_bytes": 3169,
    "line_count": 115,
    "preview": "---\ntitle: GOAT\n---\n\n[GOAT](https://github.com/goat-sdk/goat) is the finance toolkit for AI agents.\n\n<Warning>\n**This tool exists outside of the main LangChain repository [here](https://github.com/goat-sdk/goat/).**\n\nPlease use caution when linking wallets to external providers and make sure they are trusted.\n"
  }
,
  {
    "path": "javascript/integrations/tools/dalle.mdx",
    "filename": "dalle.mdx",
    "size_bytes": 465,
    "line_count": 23,
    "preview": "---\ntitle: Dall-E Tool\n---\n\n```typescript\n/* eslint-disable no-process-env */\nimport { DallEAPIWrapper } from \"@langchain/openai\";\n\nconst tool = new DallEAPIWrapper({\n  n: 1, // Default\n"
  }
,
  {
    "path": "javascript/integrations/tools/wolframalpha.mdx",
    "filename": "wolframalpha.mdx",
    "size_bytes": 634,
    "line_count": 28,
    "preview": "---\ntitle: WolframAlpha Tool\n---\n\nThe WolframAlpha tool connects your agents and chains to WolframAlpha's state-of-the-art computational intelligence engine.\n\n## Setup\n\nYou'll need to create an app from the [WolframAlpha portal](https://developer.wolframalpha.com/) and obtain an `appid`.\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/tavily_extract.mdx",
    "filename": "tavily_extract.mdx",
    "size_bytes": 4785,
    "line_count": 163,
    "preview": "---\ntitle: Tavily Extract\n---\n\n[Tavily](https://tavily.com/) is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed. Tavily offers four key endpoints, one of which being Extract, which provides raw extracted content from a URL.\n\nThis guide provides a quick overview for getting started with the Tavily [tool](/oss/integrations/tools/). For a complete breakdown of the Tavily tool, you can find more detailed documentation in the [API reference](https://v03.api.js.langchain.com/modules/_langchain_tavily.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/stackexchange.mdx",
    "filename": "stackexchange.mdx",
    "size_bytes": 1001,
    "line_count": 37,
    "preview": "---\ntitle: StackExchange Tool\n---\n\nThe StackExchange tool connects your agents and chains to StackExchange's API.\n\n## Usage\n\n```typescript\nimport { StackExchangeAPI } from \"@langchain/community/tools/stackexchange\";\n"
  }
,
  {
    "path": "javascript/integrations/tools/webbrowser.mdx",
    "filename": "webbrowser.mdx",
    "size_bytes": 6913,
    "line_count": 138,
    "preview": "---\ntitle: Web Browser Tool\n---\n\nThe Webbrowser Tool gives your agent the ability to visit a website and extract information. It is described to the agent as\n\n```\nuseful for when you need to find something on or summarize a webpage. input should be a comma separated list of \"valid URL including protocol\",\"what you want to find on the page or empty string for a summary\".\n```\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/openapi.mdx",
    "filename": "openapi.mdx",
    "size_bytes": 7428,
    "line_count": 219,
    "preview": "---\ntitle: OpenApi Toolkit\n---\n\n\n<Warning>\nThis agent can make requests to external APIs. Use with caution, especially when granting access to users.\n\nBe aware that this agent could theoretically send requests with provided credentials or other sensitive data to unverified or potentially malicious URLs --although it should never in theory.\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/anthropic.mdx",
    "filename": "anthropic.mdx",
    "size_bytes": 19968,
    "line_count": 614,
    "preview": "---\ntitle: Tools\n---\n\nThe `@langchain/anthropic` package provides LangChain-compatible wrappers for Anthropic's built-in tools. These tools can be bound to `ChatAnthropic` using `bindTools()` or @[`createAgent`].\n\n### Memory tool\n\nThe memory tool (`memory_20250818`) enables Claude to store and retrieve information across conversations through a memory file directory. Claude can create, read, update, and delete files that persist between sessions, allowing it to build knowledge over time without keeping everything in the context window.\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/decodo.mdx",
    "filename": "decodo.mdx",
    "size_bytes": 4302,
    "line_count": 178,
    "preview": "---\ntitle: Decodo Tools\n---\n\nThe [@decodo/langchain-ts](https://www.npmjs.com/package/@decodo/langchain-ts) package enables developers to use Decodo's Web Scraper API alongside their LangChain applications.\n\nThe Web Scraper API features:\n\n- Easy web data access. Simplified retrieval of information from websites and online sources.\n- Geographic flexibility. Access content regardless of regional restrictions.\n"
  }
,
  {
    "path": "javascript/integrations/tools/pyinterpreter.mdx",
    "filename": "pyinterpreter.mdx",
    "size_bytes": 2149,
    "line_count": 65,
    "preview": "---\ntitle: Python interpreter tool\n---\n\n<Warning>\nThis tool executes code and can potentially perform destructive actions. Be careful that you trust any code passed to it!\n</Warning>\n\nLangChain offers an experimental tool for executing arbitrary Python code.\nThis can be useful in combination with an LLM that can generate code to perform more powerful computations.\n"
  }
,
  {
    "path": "javascript/integrations/tools/falkordb.mdx",
    "filename": "falkordb.mdx",
    "size_bytes": 5410,
    "line_count": 207,
    "preview": "---\ntitle: FalkorDB\ndescription: Use FalkorDB's ultra-fast graph database with LangChain for natural language queries over knowledge graphs using Cypher\nsidebar_label: FalkorDB\n---\n\n# FalkorDB LangChain JS/TS integration\n\nThe [@falkordb/langchain-ts](https://www.npmjs.com/package/@falkordb/langchain-ts) package enables developers to use FalkorDB's ultra-fast graph database alongside their LangChain applications. What this means in practice is that your application can now take natural language questions, automatically generate the corresponding Cypher queries, pull the relevant context from your graph database, and return responses in plain language. The entire translation layer gets handled for you.\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/discord_tool.mdx",
    "filename": "discord_tool.mdx",
    "size_bytes": 2392,
    "line_count": 83,
    "preview": "---\ntitle: Discord Tool\n---\n\nThe Discord Tool gives your agent the ability to search, read, and write messages to discord channels.\nIt is useful for when you need to interact with a discord channel.\n\n## Setup\n\nTo use the Discord Tool you need to install the following official peer depencency:\n"
  }
,
  {
    "path": "javascript/integrations/tools/zapier_agent.mdx",
    "filename": "zapier_agent.mdx",
    "size_bytes": 2827,
    "line_count": 70,
    "preview": "---\ntitle: Agent with Zapier NLA Integration\n---\n\n<Warning>\nThis module has been deprecated and is no longer supported. The documentation below will not work in versions 0.2.0 or later.\n</Warning>\n\nFull docs here: https://nla.zapier.com/start/\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/tavily_search.mdx",
    "filename": "tavily_search.mdx",
    "size_bytes": 5245,
    "line_count": 175,
    "preview": "---\ntitle: Tavily Search\n---\n\n[Tavily](https://tavily.com/) is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed. Tavily offers two key endpoints, one of which being Search, which provides search results tailored for LLMs and RAG.\n\nThis guide provides a quick overview for getting started with the Tavily [tool](/oss/integrations/tools/). For a complete breakdown of the Tavily tool, you can find more detailed documentation in the [API reference](https://v03.api.js.langchain.com/modules/_langchain_tavily.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/google_gmail.mdx",
    "filename": "google_gmail.mdx",
    "size_bytes": 4112,
    "line_count": 97,
    "preview": "---\ntitle: Gmail Tool\n---\n\nThe Gmail Tool allows your agent to create and view messages from a linked email account.\n\n## Setup\n\nYou can authenticate via two methods:\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/tavily_search_community.mdx",
    "filename": "tavily_search_community.mdx",
    "size_bytes": 4822,
    "line_count": 168,
    "preview": "---\ntitle: TavilySearchResults (Deprecated)\n---\n\n<Info>\n**Deprecation Notice**\n\nThis tool has been deprecated. Please use the [TavilySearch](./tavily_search) tool in the `@langchain/tavily` package, instead.\n\n</Info>\n"
  }
,
  {
    "path": "javascript/integrations/tools/lambda_agent.mdx",
    "filename": "lambda_agent.mdx",
    "size_bytes": 3238,
    "line_count": 57,
    "preview": "---\ntitle: Agent with AWS Lambda Integration\n---\n\nFull docs here: https://docs.aws.amazon.com/lambda/index.html\n\n**AWS Lambda** is a serverless computing service provided by Amazon Web Services (AWS), designed to allow developers to build and run applications and services without the need for provisioning or managing servers. This serverless architecture enables you to focus on writing and deploying code, while AWS automatically takes care of scaling, patching, and managing the infrastructure required to run your applications.\n\nBy including a AWSLambda in the list of tools provided to an Agent, you can grant your Agent the ability to invoke code running in your AWS Cloud for whatever purposes you need.\n\n"
  }
,
  {
    "path": "javascript/integrations/tools/google_calendar.mdx",
    "filename": "google_calendar.mdx",
    "size_bytes": 2248,
    "line_count": 81,
    "preview": "---\ntitle: Google Calendar Tool\n---\n\nThe Google Calendar Tools allow your agent to create and view Google Calendar events from a linked calendar.\n\n## Setup\n\nTo use the Google Calendar Tools you need to install the following official peer dependency:\n\n"
  }
,
  {
    "path": "javascript/integrations/llm_caching/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 485,
    "line_count": 18,
    "preview": "---\ntitle: Model caches\n---\n\n[Caching LLM calls](/oss/langchain/models#caching) can be useful for testing, cost savings, and speed.\n\nBelow are some integrations that allow you to cache results of individual LLM calls using different caches with different strategies.\n\n<Columns cols={3}>\n  <Card\n"
  }
,
  {
    "path": "javascript/integrations/llm_caching/azure_cosmosdb_nosql.mdx",
    "filename": "azure_cosmosdb_nosql.mdx",
    "size_bytes": 3356,
    "line_count": 83,
    "preview": "---\ntitle: Azure Cosmos DB NoSQL Semantic Cache\n---\n\n> The Semantic Cache feature is supported with Azure Cosmos DB for NoSQL integration, enabling users to retrieve cached responses based on semantic similarity between the user input and previously cached results. It leverages [AzureCosmosDBNoSQLVectorStore](/oss/integrations/vectorstores/azure_cosmosdb_nosql), which stores vector embeddings of cached prompts. These embeddings enable similarity-based searches, allowing the system to retrieve relevant cached results.\n\nIf you don't have an Azure account, you can [create a free account](https://azure.microsoft.com/free/) to get started.\n\n## Setup\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/cloudflare_workersai.mdx",
    "filename": "cloudflare_workersai.mdx",
    "size_bytes": 4481,
    "line_count": 127,
    "preview": "---\ntitle: ChatCloudflareWorkersAI\n---\n\n[Workers AI](https://developers.cloudflare.com/workers-ai/) allows you to run machine learning models, on the Cloudflare network, from your own code.\n\nThis will help you getting started with Cloudflare Workers AI [chat models](/oss/langchain/models). For detailed documentation of all `ChatCloudflareWorkersAI` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_cloudflare.ChatCloudflareWorkersAI.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/tencent_hunyuan.mdx",
    "filename": "tencent_hunyuan.mdx",
    "size_bytes": 3611,
    "line_count": 151,
    "preview": "---\ntitle: ChatTencentHunyuan\n---\n\nLangChain.js supports the Tencent Hunyuan family of models.\n\nhttps://cloud.tencent.com/document/product/1729/104753\n\n## Setup\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/google_generative_ai.mdx",
    "filename": "google_generative_ai.mdx",
    "size_bytes": 23381,
    "line_count": 612,
    "preview": "---\ntitle: ChatGoogleGenerativeAI\n---\n\n[Google AI](https://ai.google.dev/) offers a number of different chat models, including the powerful Gemini series. For information on the latest models, their features, context windows, etc. head to the [Google AI docs](https://ai.google.dev/gemini-api/docs/models/gemini).\n\nThis will help you getting started with `ChatGoogleGenerativeAI` [chat models](/oss/langchain/models). For detailed documentation of all `ChatGoogleGenerativeAI` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_google_genai.ChatGoogleGenerativeAI.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/ollama_functions.mdx",
    "filename": "ollama_functions.mdx",
    "size_bytes": 6386,
    "line_count": 251,
    "preview": "---\ntitle: Ollama Functions\n---\n\n<Warning>\n**The LangChain Ollama integration package has official support for tool calling. [View the Ollama tool calling documentation](/oss/integrations/chat/ollama#tools).**\n\n\n</Warning>\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/ni_bittensor.mdx",
    "filename": "ni_bittensor.mdx",
    "size_bytes": 807,
    "line_count": 31,
    "preview": "---\ntitle: NIBittensorChatModel\n---\n\n<Warning>\nThis module has been deprecated and is no longer supported. The documentation below will not work in versions 0.2.0 or later.\n</Warning>\n\nLangChain.js offers experimental support for Neural Internet's Bittensor chat models.\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/ibm.mdx",
    "filename": "ibm.mdx",
    "size_bytes": 9025,
    "line_count": 339,
    "preview": "---\ntitle: IBM watsonx.ai\n---\n\nThis will help you getting started with IBM watsonx.ai [chat models](/oss/langchain/models). For detailed documentation of all `IBM watsonx.ai` features and configurations head to the [IBM watsonx.ai](https://api.js.langchain.com/modules/_langchain_community.chat_models_ibm.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/bedrock_converse.mdx",
    "filename": "bedrock_converse.mdx",
    "size_bytes": 5243,
    "line_count": 136,
    "preview": "---\ntitle: ChatBedrockConverse\n---\n\n[Amazon Bedrock Converse](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html) is a fully managed service that makes Foundation Models (FMs) from leading AI startups and Amazon available via an API. You can choose from a wide range of FMs to find the model that is best suited for your use case. It provides a unified conversational interface for Bedrock models, but does not yet have feature parity for all functionality within the older [Bedrock model service](/oss/integrations/chat/bedrock).\n\nThis will help you getting started with Amazon Bedrock Converse [chat models](/oss/langchain/models). For detailed documentation of all `ChatBedrockConverse` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_aws.ChatBedrockConverse.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/xai.mdx",
    "filename": "xai.mdx",
    "size_bytes": 4058,
    "line_count": 132,
    "preview": "---\ntitle: ChatXAI\n---\n\n[xAI](https://x.ai/) is an artificial intelligence company that develops Large Language Models (LLMs). Their flagship model, Grok, is trained on real-time X (formerly Twitter) data and aims to provide witty, personality-rich responses while maintaining high capability on technical tasks.\n\nThis guide will help you getting started with `ChatXAI` [chat models](/oss/langchain/models). For detailed documentation of all `ChatXAI` features and configurations head to the [API reference](https://api.js.langchain.com/classes/_langchain_xai.ChatXAI.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/premai.mdx",
    "filename": "premai.mdx",
    "size_bytes": 1039,
    "line_count": 41,
    "preview": "---\ntitle: ChatPrem\n---\n\n## Setup\n\n1. Create a Prem AI account and get your API key [here](https://studio.premai.io/sign-up).\n2. Export or set your API key inline. The ChatPrem class defaults to `process.env.PREM_API_KEY`.\n\n```bash\n"
  }
,
  {
    "path": "javascript/integrations/chat/groq.mdx",
    "filename": "groq.mdx",
    "size_bytes": 5055,
    "line_count": 149,
    "preview": "---\ntitle: ChatGroq\n---\n\n[Groq](https://groq.com/) is a company that offers fast AI inference, powered by LPU™ AI inference technology which delivers fast, affordable, and energy efficient AI.\n\nThis will help you getting started with ChatGroq [chat models](/oss/langchain/models). For detailed documentation of all ChatGroq features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_groq.ChatGroq.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 14503,
    "line_count": 555,
    "preview": "---\ntitle: Chat models\n---\n\n[Chat models](/oss/langchain/models) are language models that use a sequence of [messages](/oss/langchain/messages) as inputs and return messages as outputs <Tooltip tip=\"Older models that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output. These models typically do not include the prefix 'Chat' in their name or include 'LLM' as a suffix.\">(as opposed to plaintext)</Tooltip>.\n\n## Install and use\n\n<Tip>\n    See [this section for general instructions on installing LangChain packages](/oss/langchain/install).\n"
  }
,
  {
    "path": "javascript/integrations/chat/openai.mdx",
    "filename": "openai.mdx",
    "size_bytes": 44928,
    "line_count": 1381,
    "preview": "---\ntitle: ChatOpenAI\n---\n\n[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is an artificial intelligence (AI) research laboratory.\n\nThis guide will help you getting started with ChatOpenAI [chat models](/oss/langchain/models). For detailed documentation of all ChatOpenAI features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html).\n\n<Note>\n    **Chat Completions API compatibility**\n"
  }
,
  {
    "path": "javascript/integrations/chat/prompt_layer_openai.mdx",
    "filename": "prompt_layer_openai.mdx",
    "size_bytes": 1425,
    "line_count": 56,
    "preview": "---\ntitle: PromptLayerChatOpenAI\n---\n\nYou can pass in the optional `returnPromptLayerId` boolean to get a `promptLayerRequestId` like below. Here is an example of getting the PromptLayerChatOpenAI requestID:\n\n```typescript\nimport { PromptLayerChatOpenAI } from \"@langchain/classic/llms/openai\";\n\nconst chat = new PromptLayerChatOpenAI({\n"
  }
,
  {
    "path": "javascript/integrations/chat/zhipuai.mdx",
    "filename": "zhipuai.mdx",
    "size_bytes": 1674,
    "line_count": 65,
    "preview": "---\ntitle: ChatZhipuAI\n---\n\nLangChain.js supports the Zhipu AI family of models.\n\nhttps://open.bigmodel.cn/dev/howuse/model\n\n## Setup\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/bedrock.mdx",
    "filename": "bedrock.mdx",
    "size_bytes": 6768,
    "line_count": 148,
    "preview": "---\ntitle: BedrockChat\n---\n\n[Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI.\n\nThis will help you getting started with Amazon Bedrock [chat models](/oss/langchain/models). For detailed documentation of all `BedrockChat` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_chat_models_bedrock.BedrockChat.html).\n\n<Tip>\nThe newer [`ChatBedrockConverse` chat model is now available via the dedicated `@langchain/aws`](/oss/integrations/chat/bedrock_converse) integration package. Use [tool calling](/oss/langchain/tools) with more models with this package.\n"
  }
,
  {
    "path": "javascript/integrations/chat/novita.mdx",
    "filename": "novita.mdx",
    "size_bytes": 2864,
    "line_count": 84,
    "preview": "---\ntitle: ChatNovita\n---\n\nDelivers an affordable, reliable, and simple inference platform for running top LLM models.\n\nYou can find all the models we support here: [Novita AI Featured Models](https://novita.ai/models/llm?utm_source=github_langchain&utm_medium=github_readme&utm_campaign=link) or request the [Models API](https://novita.ai/docs/guides/llm-models?utm_source=github_langchain&utm_medium=github_readme&utm_campaign=link) to get all available models.\n\nTry the [Novita AI DeepSeek R1 API Demo](https://novita.ai/models/llm/deepseek-deepseek-r1?utm_source=github_langchain&utm_medium=github_readme&utm_campaign=link) today!\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/fireworks.mdx",
    "filename": "fireworks.mdx",
    "size_bytes": 4130,
    "line_count": 125,
    "preview": "---\ntitle: ChatFireworks\n---\n\n[Fireworks AI](https://fireworks.ai/) is an AI inference platform to run and customize models. For a list of all models served by Fireworks see the [Fireworks docs](https://fireworks.ai/models).\n\nThis guide will help you getting started with `ChatFireworks` [chat models](/oss/langchain/models). For detailed documentation of all `ChatFireworks` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_chat_models_fireworks.ChatFireworks.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/mistral.mdx",
    "filename": "mistral.mdx",
    "size_bytes": 7742,
    "line_count": 253,
    "preview": "---\ntitle: ChatMistralAI\n---\n\n[Mistral AI](https://mistral.ai/) is a platform that offers hosting for their powerful [open source models](https://docs.mistral.ai/getting-started/models/).\n\nThis will help you getting started with ChatMistralAI [chat models](/oss/langchain/models). For detailed documentation of all ChatMistralAI features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_mistralai.ChatMistralAI.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/deepseek.mdx",
    "filename": "deepseek.mdx",
    "size_bytes": 4347,
    "line_count": 140,
    "preview": "---\ntitle: ChatDeepSeek\n---\n\n\nThis will help you getting started with DeepSeek [chat models](/oss/langchain/models). For detailed documentation of all `ChatDeepSeek` features and configurations head to the [API reference](https://api.js.langchain.com/classes/_langchain_deepseek.ChatDeepSeek.html).\n\n## Overview\n### Integration details\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/baidu_qianfan.mdx",
    "filename": "baidu_qianfan.mdx",
    "size_bytes": 6614,
    "line_count": 122,
    "preview": "---\ntitle: ChatBaiduQianfan\n---\n\n## Setup\n\nYou'll first need to install the [`@langchain/baidu-qianfan`](https://www.npmjs.com/package/@langchain/baidu-qianfan) package:\n\n<Tip>\nSee [this section for general instructions on installing LangChain packages](/oss/langchain/install).\n"
  }
,
  {
    "path": "javascript/integrations/chat/alibaba_tongyi.mdx",
    "filename": "alibaba_tongyi.mdx",
    "size_bytes": 1737,
    "line_count": 61,
    "preview": "---\ntitle: Alibaba Tongyi\n---\n\nLangChain.js supports the Alibaba qwen family of models.\n\n## Setup\n\nYou'll need to sign up for an Alibaba API key and set it as an environment variable named `ALIBABA_API_KEY`.\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/minimax.mdx",
    "filename": "minimax.mdx",
    "size_bytes": 10994,
    "line_count": 437,
    "preview": "---\ntitle: ChatMinimax\n---\n\n[Minimax](https://api.minimax.chat) is a Chinese startup that provides natural language processing models for companies and individuals.\n\nThis example demonstrates using LangChain.js to interact with Minimax.\n\n## Setup\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/baidu_wenxin.mdx",
    "filename": "baidu_wenxin.mdx",
    "size_bytes": 1942,
    "line_count": 72,
    "preview": "---\ntitle: ChatBaiduWenxin\n---\n\n<Warning>\n**This class has been deprecated.**\n\n\nUse the [`@langchain/baidu-qianfan`](/oss/integrations/chat/baidu_qianfan/) package instead.\n</Warning>\n"
  }
,
  {
    "path": "javascript/integrations/chat/google_vertex_ai.mdx",
    "filename": "google_vertex_ai.mdx",
    "size_bytes": 9952,
    "line_count": 280,
    "preview": "---\ntitle: ChatVertexAI\n---\n\n[Google Vertex](https://cloud.google.com/vertex-ai) is a service that exposes all foundation models available in Google Cloud, like `gemini-2.5-pro`, `gemini-2.5-flash`, etc.\nIt also provides some non-Google models such as [Anthropic's Claude](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude).\n\nThis will help you getting started with `ChatVertexAI` [chat models](/oss/langchain/models). For detailed documentation of all `ChatVertexAI` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_google_vertexai.ChatVertexAI.html).\n\n## Overview\n"
  }
,
  {
    "path": "javascript/integrations/chat/fake.mdx",
    "filename": "fake.mdx",
    "size_bytes": 2099,
    "line_count": 75,
    "preview": "---\ntitle: Fake LLM\n---\n\nLangChain provides a fake LLM chat model for testing purposes. This allows you to mock out calls to the LLM and and simulate what would happen if the LLM responded in a certain way.\n\n## Usage\n\n```typescript\nimport { FakeListChatModel } from \"@langchain/core/utils/testing\";\n"
  }
,
  {
    "path": "javascript/integrations/chat/yandex.mdx",
    "filename": "yandex.mdx",
    "size_bytes": 1657,
    "line_count": 57,
    "preview": "---\ntitle: ChatYandexGPT\n---\n\nLangChain.js supports calling [YandexGPT](https://cloud.yandex.com/en/services/yandexgpt) chat models.\n\n## Setup\n\nFirst, you should [create a service account](https://cloud.yandex.com/en/docs/iam/operations/sa/create) with the `ai.languageModels.user` role.\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/web_llm.mdx",
    "filename": "web_llm.mdx",
    "size_bytes": 2447,
    "line_count": 82,
    "preview": "---\ntitle: WebLLM\n---\n\n<Tip>\n**Compatibility**\n\nOnly available in web environments.\n</Tip>\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/azure.mdx",
    "filename": "azure.mdx",
    "size_bytes": 9956,
    "line_count": 253,
    "preview": "---\ntitle: AzureChatOpenAI\n---\n\nAzure OpenAI is a Microsoft Azure service that provides powerful language models from OpenAI.\n\nThis will help you getting started with AzureChatOpenAI [chat models](/oss/langchain/models). For detailed documentation of all AzureChatOpenAI features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_openai.AzureChatOpenAI.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/anthropic.mdx",
    "filename": "anthropic.mdx",
    "size_bytes": 35965,
    "line_count": 1025,
    "preview": "---\ntitle: ChatAnthropic\n---\n\n[Anthropic](https://www.anthropic.com/) is an AI safety and research company. They are the creator of Claude.\n\nThis will help you getting started with Anthropic [chat models](/oss/langchain/models). For detailed documentation of all `ChatAnthropic` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_anthropic.ChatAnthropic.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/togetherai.mdx",
    "filename": "togetherai.mdx",
    "size_bytes": 4069,
    "line_count": 122,
    "preview": "---\ntitle: ChatTogetherAI\n---\n\n[Together AI](https://www.together.ai/) offers an API to query [50+ leading open-source models](https://docs.together.ai/docs/inference-models) in a couple lines of code.\n\nThis guide will help you getting started with `ChatTogetherAI` [chat models](/oss/langchain/models). For detailed documentation of all `ChatTogetherAI` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_chat_models_togetherai.ChatTogetherAI.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/friendli.mdx",
    "filename": "friendli.mdx",
    "size_bytes": 2284,
    "line_count": 81,
    "preview": "---\ntitle: ChatFriendli\n---\n\n> [Friendli](https://friendli.ai/) enhances AI application performance and optimizes cost savings with scalable, efficient deployment options, tailored for high-demand AI workloads.\n\nThis tutorial guides you through integrating `ChatFriendli` for chat applications using LangChain. `ChatFriendli` offers a flexible approach to generating conversational AI responses, supporting both synchronous and asynchronous calls.\n\n## Setup\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/moonshot.mdx",
    "filename": "moonshot.mdx",
    "size_bytes": 1774,
    "line_count": 65,
    "preview": "---\ntitle: ChatMoonshot\n---\n\nLangChain.js supports the Moonshot AI family of models.\n\nhttps://platform.moonshot.cn/docs/intro\n\n## Setup\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/cohere.mdx",
    "filename": "cohere.mdx",
    "size_bytes": 99508,
    "line_count": 1407,
    "preview": "---\ntitle: ChatCohere\n---\n\n[Cohere](https://cohere.com/) is a Canadian startup that provides natural language processing models that help companies improve human-machine interactions.\n\nThis will help you getting started with Cohere [chat models](/oss/langchain/models). For detailed documentation of all `ChatCohere` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_cohere.ChatCohere.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/arcjet.mdx",
    "filename": "arcjet.mdx",
    "size_bytes": 3229,
    "line_count": 101,
    "preview": "---\ntitle: Arcjet Redact\n---\n\nThe [Arcjet](https://arcjet.com) redact integration allows you to redact sensitive user information from your prompts before sending it to a Chat Model.\n\nArcjet Redact runs entirely on your own machine and never sends data anywhere else, ensuring best in class privacy and performance.\n\nThe Arcjet Redact object is not a chat model itself, instead it wraps an LLM. It redacts the text that is inputted to it and then unredacts the output of the wrapped chat model before returning it.\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/ollama.mdx",
    "filename": "ollama.mdx",
    "size_bytes": 9841,
    "line_count": 317,
    "preview": "---\ntitle: ChatOllama\n---\n\n[Ollama](https://ollama.ai/) allows you to run open-source Large Language Models (LLMs), such as Llama 3.1, locally.\n\nOllama bundles model weights, configuration, and data into a single package, defined by a Modelfile. It optimizes setup and configuration details, including GPU usage.\n\nThis guide will help you getting started with `ChatOllama` [chat models](/oss/langchain/models). For detailed documentation of all `ChatOllama` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_ollama.ChatOllama.html).\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/deep_infra.mdx",
    "filename": "deep_infra.mdx",
    "size_bytes": 1105,
    "line_count": 42,
    "preview": "---\ntitle: ChatDeepInfra\n---\n\nLangChain supports chat models hosted by [Deep Infra](https://deepinfra.com/) through the `ChatDeepInfra` wrapper.\nFirst, you'll need to install the `@langchain/community` package:\n\n<Tip>\nSee [this section for general instructions on installing LangChain packages](/oss/langchain/install).\n</Tip>\n"
  }
,
  {
    "path": "javascript/integrations/chat/llama_cpp.mdx",
    "filename": "llama_cpp.mdx",
    "size_bytes": 7125,
    "line_count": 288,
    "preview": "---\ntitle: ChatLlamaCpp\n---\n\n<Tip>\n**Compatibility**\n\nOnly available on Node.js.\n</Tip>\n\n"
  }
,
  {
    "path": "javascript/integrations/chat/cerebras.mdx",
    "filename": "cerebras.mdx",
    "size_bytes": 5631,
    "line_count": 163,
    "preview": "---\ntitle: ChatCerebras\n---\n\n[Cerebras](https://cerebras.ai/) is a model provider that serves open source models with an emphasis on speed. The Cerebras CS-3 system, powered by the Wafer-Scale Engine-3 (WSE-3), represents a new class of AI supercomputer that sets the standard for generative AI training and inference with unparalleled performance and scalability.\n\nWith Cerebras as your inference provider, you can:\n\n- Achieve unprecedented speed for AI inference workloads\n- Build commercially with high throughput\n"
  }
,
  {
    "path": "javascript/integrations/chat/perplexity.mdx",
    "filename": "perplexity.mdx",
    "size_bytes": 4174,
    "line_count": 130,
    "preview": "---\ntitle: ChatPerplexity\n---\n\nThis guide will help you getting started with Perplexity [chat models](/oss/langchain/models). For detailed documentation of all `ChatPerplexity` features and configurations head to the [API reference](https://api.js.langchain.com/classes/_langchain_community.chat_models_perplexity.ChatPerplexity.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "javascript/integrations/callbacks/upstash_ratelimit_callback.mdx",
    "filename": "upstash_ratelimit_callback.mdx",
    "size_bytes": 6947,
    "line_count": 200,
    "preview": "---\ntitle: Upstash Ratelimit Callback\n---\n\nIn this guide, we will go over how to add rate limiting based on number of requests or the number of tokens using `UpstashRatelimitHandler`. This handler uses [Upstash's ratelimit library](https://github.com/upstash/ratelimit-js/), which utilizes [Upstash Redis](https://upstash.com/docs/redis/overall/getstarted).\n\nUpstash Ratelimit works by sending an HTTP request to Upstash Redis every time the `limit` method is called. Remaining tokens/requests of the user are checked and updated. Based on the remaining tokens, we can stop the execution of costly operations, like invoking an LLM or querying a vector store:\n\n```tsx\nconst response = await ratelimit.limit();\n"
  }
,
  {
    "path": "javascript/integrations/callbacks/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 386,
    "line_count": 22,
    "preview": "---\ntitle: Callbacks\n---\n\n<Columns cols={3}>\n  <Card\n    title=\"Datadog Tracer\"\n    icon=\"link\"\n    href=\"/oss/integrations/callbacks/datadog_tracer\"\n    arrow=\"true\"\n"
  }
,
  {
    "path": "javascript/integrations/callbacks/datadog_tracer.mdx",
    "filename": "datadog_tracer.mdx",
    "size_bytes": 1734,
    "line_count": 56,
    "preview": "---\ntitle: Datadog LLM Observability\n---\n\n<Warning>\nLLM Observability is in public beta, and its API is subject to change.\n</Warning>\n\nWith [Datadog LLM Observability](https://docs.datadoghq.com/llm_observability/), you can monitor, troubleshoot, and evaluate your LLM-powered applications, such as chatbots. You can investigate the root cause of issues, monitor operational performance, and evaluate the quality, privacy, and safety of your LLM applications.\n\n"
  }
,
  {
    "path": "javascript/integrations/providers/microsoft.mdx",
    "filename": "microsoft.mdx",
    "size_bytes": 9948,
    "line_count": 193,
    "preview": "---\ntitle: Overview\n---\n\nAll functionality related to `Microsoft Azure` and other `Microsoft` products.\n\n## Chat models\n\n### Azure OpenAI\n\n"
  }
,
  {
    "path": "javascript/integrations/providers/aws.mdx",
    "filename": "aws.mdx",
    "size_bytes": 1726,
    "line_count": 67,
    "preview": "---\ntitle: Overview\n---\n\nAll functionality related to the [Amazon AWS](https://aws.amazon.com/) platform.\n\n## Chat models\n\n### Bedrock\n\n"
  }
,
  {
    "path": "javascript/integrations/providers/overview.mdx",
    "filename": "overview.mdx",
    "size_bytes": 5953,
    "line_count": 43,
    "preview": "---\ntitle: Integration Packages\nsidebarTitle: Overview\nmode: wide\n---\n\nLangChain integrates with a wide variety of chat & embedding models, tools & toolkits, document loaders, vector stores, and more.\n\nA **provider** is a third-party service or platform that LangChain integrates with to access AI capabilities like chat models, embeddings, and vector stores. These providers have standalone `langchain-provider` packages for improved versioning, dependency management, and testing.\n\n"
  }
,
  {
    "path": "javascript/integrations/providers/openai.mdx",
    "filename": "openai.mdx",
    "size_bytes": 6491,
    "line_count": 205,
    "preview": "---\ntitle: Overview\n---\n\nAll functionality related to OpenAI\n\n> [OpenAI](https://en.wikipedia.org/wiki/OpenAI) is American artificial intelligence (AI) research laboratory\n> consisting of the non-profit `OpenAI Incorporated`\n> and its for-profit subsidiary corporation `OpenAI Limited Partnership`.\n> OpenAI conducts AI research with the declared intention of promoting and developing a friendly AI.\n"
  }
,
  {
    "path": "javascript/integrations/providers/google.mdx",
    "filename": "google.mdx",
    "size_bytes": 7266,
    "line_count": 278,
    "preview": "---\ntitle: Overview\n---\n\nFunctionality related to [Google Cloud Platform](https://cloud.google.com/)\nand [AI Studio](https://aistudio.google.com/)\n\n## Chat models\n\n### Gemini models\n"
  }
,
  {
    "path": "javascript/integrations/providers/anthropic.mdx",
    "filename": "anthropic.mdx",
    "size_bytes": 1603,
    "line_count": 54,
    "preview": "---\ntitle: Overview\n---\n\nAll functionality related to Anthropic models..\n\n[Anthropic](https://www.anthropic.com/) is an AI safety and research company, and is the creator of Claude.\nThis page covers all integrations between Anthropic models and LangChain.\n\n## Prompting best practices\n"
  }
,
  {
    "path": "javascript/integrations/providers/all_providers.mdx",
    "filename": "all_providers.mdx",
    "size_bytes": 44740,
    "line_count": 2076,
    "preview": "---\ntitle: \"All integrations\"\nsidebarTitle: \"All providers\"\n---\n\nBrowse the complete collection of integrations available for JavaScript/TypeScript. LangChain.js offers hundreds of integrations across providers, tools, vector stores, document loaders, and more.\n\n## Top providers\n\n<Columns cols={3}>\n"
  }
,
  {
    "path": "javascript/integrations/stores/file_system.mdx",
    "filename": "file_system.mdx",
    "size_bytes": 3899,
    "line_count": 161,
    "preview": "---\ntitle: LocalFileStore\n---\n\n<Tip>\n**Compatibility**: Only available on Node.js.\n</Tip>\n\n\nThis will help you get started with [LocalFileStore](/oss/integrations/stores). For detailed documentation of all LocalFileStore features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.storage_file_system.LocalFileStore.html).\n"
  }
,
  {
    "path": "javascript/integrations/stores/vercel_kv_storage.mdx",
    "filename": "vercel_kv_storage.mdx",
    "size_bytes": 2802,
    "line_count": 99,
    "preview": "---\ntitle: Vercel KV\n---\n\nThis example demonstrates how to setup chat history storage using the `VercelKVStore` @[`BaseStore`] integration.\n\n## Setup\n\n```bash npm\nnpm install @langchain/community @langchain/core @vercel/kv\n"
  }
,
  {
    "path": "javascript/integrations/stores/ioredis_storage.mdx",
    "filename": "ioredis_storage.mdx",
    "size_bytes": 2346,
    "line_count": 85,
    "preview": "---\ntitle: IORedis\n---\n\nThis example demonstrates how to setup chat history storage using the `RedisByteStore` @[`BaseStore`] integration.\n\n## Setup\n\n```bash npm\nnpm install @langchain/community @langchain/core ioredis\n"
  }
,
  {
    "path": "javascript/integrations/stores/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 2535,
    "line_count": 67,
    "preview": "---\ntitle: \"Key-value stores\"\n---\n\n## Overview\n\nLangChain provides a key-value store interface for storing and retrieving data by key. The key-value store interface in LangChain is primarily used for caching [embeddings](/oss/integrations/text_embedding).\n\n## Interface\n\n"
  }
,
  {
    "path": "javascript/integrations/stores/cassandra_storage.mdx",
    "filename": "cassandra_storage.mdx",
    "size_bytes": 4219,
    "line_count": 140,
    "preview": "---\ntitle: Cassandra KV\n---\n\nThis example demonstrates how to setup chat history storage using the `CassandraKVStore` @[`BaseStore`] integration. Note there is a `CassandraChatMessageHistory`\nintegration which may be easier to use for chat history storage; the `CassandraKVStore` is useful if you want a more general-purpose key-value store with\nprefixable keys.\n\n## Setup\n\n"
  }
,
  {
    "path": "javascript/integrations/stores/in_memory.mdx",
    "filename": "in_memory.mdx",
    "size_bytes": 3398,
    "line_count": 141,
    "preview": "---\ntitle: InMemoryStore\n---\n\nThis will help you get started with [InMemoryStore](/oss/integrations/stores). For detailed documentation of all InMemoryStore features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_core.stores.InMemoryStore.html).\n\nThe `InMemoryStore` allows for a generic type to be assigned to the values in the store. We'll assign type `BaseMessage` as the type of our values, keeping with the theme of a chat history store.\n\n## Overview\n\n"
  }
,
  {
    "path": "javascript/integrations/stores/upstash_redis_storage.mdx",
    "filename": "upstash_redis_storage.mdx",
    "size_bytes": 2861,
    "line_count": 102,
    "preview": "---\ntitle: Upstash Redis\n---\n\nThis example demonstrates how to setup chat history storage using the `UpstashRedisStore` @[`BaseStore`] integration.\n\n## Setup\n\n```bash npm\nnpm install @langchain/community @langchain/core @upstash/redis\n"
  }
,
  {
    "path": "javascript/integrations/document_compressors/cohere_rerank.mdx",
    "filename": "cohere_rerank.mdx",
    "size_bytes": 8241,
    "line_count": 192,
    "preview": "---\ntitle: Cohere Rerank\n---\n\nReranking documents can greatly improve any RAG application and document retrieval system.\n\nAt a high level, a rerank API is a language model which analyzes documents and reorders them based on their relevance to a given query.\n\nCohere offers an API for reranking documents. In this example we'll show you how to use it.\n\n"
  }
,
  {
    "path": "javascript/integrations/document_compressors/ibm.mdx",
    "filename": "ibm.mdx",
    "size_bytes": 10208,
    "line_count": 278,
    "preview": "---\ntitle: IBM watsonx.ai\n---\n\n## Overview\n\nThis will help you getting started with the [Watsonx document compressor](/oss/concepts/#document_compressors). For detailed documentation of all Watsonx document compressor features and configurations head to the [API reference](https://api.js.langchain.com/modules/_langchain_community.document_compressors_ibm.html).\n\n### Integration details\n\n"
  }
,
  {
    "path": "javascript/integrations/document_compressors/mixedbread_ai.mdx",
    "filename": "mixedbread_ai.mdx",
    "size_bytes": 1872,
    "line_count": 56,
    "preview": "---\ntitle: Mixedbread AI reranking\n---\n\n## Overview\n\nThis guide will help you integrate and use the [Mixedbread AI](https://mixedbread.ai/) reranking API. The reranking API allows you to reorder a list of documents based on a given query, improving the relevance of search results or any ranked list.\n\n## Installation\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/myscale.mdx",
    "filename": "myscale.mdx",
    "size_bytes": 2845,
    "line_count": 90,
    "preview": "---\ntitle: MyScale\n---\n\n<Tip>\n**Compatibility**\n\nOnly available on Node.js.\n</Tip>\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/azion-edgesql.mdx",
    "filename": "azion-edgesql.mdx",
    "size_bytes": 7315,
    "line_count": 205,
    "preview": "---\ntitle: AzionVectorStore\n---\n\nThe `AzionVectorStore` is used to manage and search through a collection of documents using vector embeddings, directly on Azion's Edge Plataform using Edge SQL.\n\nThis guide provides a quick overview for getting started with Azion EdgeSQL [vector stores](/oss/integrations/vectorstores). For detailed documentation of all `AzionVectorStore` features and configurations head to the [API reference](https://api.js.langchain.com/classes/_langchain_community.vectorstores_azion_edgesql.AzionVectorStore.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/typeorm.mdx",
    "filename": "typeorm.mdx",
    "size_bytes": 2439,
    "line_count": 92,
    "preview": "---\ntitle: TypeORM\n---\n\nTo enable vector search in a generic PostgreSQL database, LangChain.js supports using [TypeORM](https://typeorm.io/) with the [`pgvector`](https://github.com/pgvector/pgvector) Postgres extension.\n\n## Setup\n\nTo work with TypeORM, you need to install the `typeorm` and `pg` packages:\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/mongodb_atlas.mdx",
    "filename": "mongodb_atlas.mdx",
    "size_bytes": 10517,
    "line_count": 313,
    "preview": "---\ntitle: MongoDB Atlas\n---\n\n<Tip>\n**Compatibility**: Only available on Node.js.\n\nYou can still create API routes that use MongoDB with Next.js by setting the `runtime` variable to `nodejs` like so:\n\n`export const runtime = \"nodejs\";`\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/supabase.mdx",
    "filename": "supabase.mdx",
    "size_bytes": 9308,
    "line_count": 270,
    "preview": "---\ntitle: SupabaseVectorStore\n---\n\n[Supabase](https://supabase.com/docs) is an open-source Firebase alternative. Supabase is built on top of PostgreSQL, which offers strong SQL querying capabilities and enables a simple interface with already-existing tools and frameworks.\n\nLangChain.js supports using a Supabase Postgres database as a vector store, using the [`pgvector`](https://github.com/pgvector/pgvector) extension. Refer to the [Supabase blog post](https://supabase.com/blog/openai-embeddings-postgres-vector) for more information.\n\nThis guide provides a quick overview for getting started with Supabase [vector stores](/oss/integrations/vectorstores). For detailed documentation of all `SupabaseVectorStore` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_vectorstores_supabase.SupabaseVectorStore.html).\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/neo4jvector.mdx",
    "filename": "neo4jvector.mdx",
    "size_bytes": 10131,
    "line_count": 279,
    "preview": "---\ntitle: Neo4j Vector Index\n---\n\nNeo4j is an open-source graph database with integrated support for vector similarity search.\nIt supports:\n\n- approximate nearest neighbor search\n- Euclidean similarity and cosine similarity\n- Hybrid search combining vector and keyword searches\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/voy.mdx",
    "filename": "voy.mdx",
    "size_bytes": 1842,
    "line_count": 75,
    "preview": "---\ntitle: Voy\n---\n\n[Voy](https://github.com/tantaraio/voy) is a WASM vector similarity search engine written in Rust.\nIt's supported in non-Node environments like browsers. You can use Voy as a vector store with LangChain.js.\n\n### Install voy\n\n<Tip>\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/closevector.mdx",
    "filename": "closevector.mdx",
    "size_bytes": 6478,
    "line_count": 196,
    "preview": "---\ntitle: CloseVector\n---\n\n<Tip>\n**Compatibility**\n\navailable on both browser and Node.js\n</Tip>\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/vectara.mdx",
    "filename": "vectara.mdx",
    "size_bytes": 4621,
    "line_count": 155,
    "preview": "---\ntitle: Vectara\n---\n\nVectara is a platform for building GenAI applications. It provides an easy-to-use API for document indexing and querying that is managed by Vectara and is optimized for performance and accuracy.\n\nYou can use Vectara as a vector store with LangChain.js.\n\n## 👉 Embeddings included\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/zep_cloud.mdx",
    "filename": "zep_cloud.mdx",
    "size_bytes": 5718,
    "line_count": 173,
    "preview": "---\ntitle: Zep Cloud\n---\n\n> [Zep](https://www.getzep.com) is a long-term memory service for AI Assistant apps.\n> With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant,\n> while also reducing hallucinations, latency, and cost.\n\n**Note:** The `ZepCloudVectorStore` works with `Documents` and is intended to be used as a `Retriever`.\nIt offers separate functionality to Zep's `ZepCloudMemory` class, which is designed for persisting, enriching\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/typesense.mdx",
    "filename": "typesense.mdx",
    "size_bytes": 6180,
    "line_count": 146,
    "preview": "---\ntitle: Typesense\n---\n\nVector store that utilizes the Typesense search engine.\n\n### Basic usage\n\n<Tip>\nSee [this section for general instructions on installing LangChain packages](/oss/langchain/install).\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/azure_cosmosdb_mongodb.mdx",
    "filename": "azure_cosmosdb_mongodb.mdx",
    "size_bytes": 5393,
    "line_count": 128,
    "preview": "---\ntitle: Azure Cosmos DB for MongoDB vCore\n---\n\n> [Azure Cosmos DB for MongoDB vCore](https://learn.microsoft.com/azure/cosmos-db/mongodb/vcore/) makes it easy to create a database with full native MongoDB support. You can apply your MongoDB experience and continue to use your favorite MongoDB drivers, SDKs, and tools by pointing your application to the API for MongoDB vCore account’s connection string. Use vector search in Azure Cosmos DB for MongoDB vCore to seamlessly integrate your AI-based applications with your data that’s stored in Azure Cosmos DB.\n\nAzure Cosmos DB for MongoDB vCore provides developers with a fully managed MongoDB-compatible database service for building modern applications with a familiar architecture.\n\nLearn how to leverage the vector search capabilities of Azure Cosmos DB for MongoDB vCore from [this page](https://learn.microsoft.com/azure/cosmos-db/mongodb/vcore/vector-search). If you don't have an Azure account, you can [create a free account](https://azure.microsoft.com/free/) to get started.\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 18182,
    "line_count": 996,
    "preview": "---\ntitle: \"Vector stores\"\n---\n\n## Overview\n\nA [vector store](/oss/integrations/vectorstores) stores [embedded](/oss/integrations/text_embedding) data and performs similarity search.\n\n```mermaid\nflowchart LR\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/convex.mdx",
    "filename": "convex.mdx",
    "size_bytes": 2343,
    "line_count": 101,
    "preview": "---\ntitle: Convex\n---\n\nLangChain.js supports [Convex](https://convex.dev/) as a [vector store](https://docs.convex.dev/vector-search), and supports the standard similarity search.\n\n## Setup\n\n### Create project\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/zep.mdx",
    "filename": "zep.mdx",
    "size_bytes": 9439,
    "line_count": 270,
    "preview": "---\ntitle: Zep Open Source\n---\n\n> [Zep](https://www.getzep.com) is a long-term memory service for AI Assistant apps.\n> With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant,\n> while also reducing hallucinations, latency, and cost.\n\n> Interested in Zep Cloud? See [Zep Cloud Installation Guide](https://help.getzep.com/sdks)\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/hnswlib.mdx",
    "filename": "hnswlib.mdx",
    "size_bytes": 6960,
    "line_count": 214,
    "preview": "---\ntitle: HNSWLib\n---\n\n<Tip>\n**Compatibility**: Only available on Node.js.\n</Tip>\n\nHNSWLib is an in-memory vector store that can be saved to a file. It uses the [HNSWLib library](https://github.com/nmslib/hnswlib).\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/couchbase_search.mdx",
    "filename": "couchbase_search.mdx",
    "size_bytes": 15936,
    "line_count": 420,
    "preview": "---\ntitle: Couchbase Search Vector Store\n---\n\nThe `CouchbaseSearchVectorStore` is an implementation of Vector Search that is a part of the [Full Text Search Service](https://docs.couchbase.com/server/current/learn/services-and-indexes/services/search-service.html) (Search Service) in Couchbase.\n\nThis tutorial explains how to use Vector Search via the Couchbase Search Service. You can work with both [Couchbase Capella](https://www.couchbase.com/products/capella/) and your self-managed Couchbase Server.\n\n## Installation\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/vercel_postgres.mdx",
    "filename": "vercel_postgres.mdx",
    "size_bytes": 4872,
    "line_count": 187,
    "preview": "---\ntitle: Vercel Postgres\n---\n\nLangChain.js supports using the [`@vercel/postgres`](https://www.npmjs.com/package/@vercel/postgres) package to use generic Postgres databases\nas vector stores, provided they support the [`pgvector`](https://github.com/pgvector/pgvector) Postgres extension.\n\nThis integration is particularly useful from web environments like Edge functions.\n\n## Setup\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/tigris.mdx",
    "filename": "tigris.mdx",
    "size_bytes": 3181,
    "line_count": 121,
    "preview": "---\ntitle: Tigris\n---\n\nTigris makes it easy to build AI applications with vector embeddings.\nIt is a fully managed cloud-native database that allows you store and\nindex documents and vector embeddings for fast and scalable vector search.\n\n<Tip>\n**Compatibility**\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/weaviate.mdx",
    "filename": "weaviate.mdx",
    "size_bytes": 10867,
    "line_count": 318,
    "preview": "---\ntitle: WeaviateStore\n---\n\n[Weaviate](https://weaviate.io/) is an open source vector database that stores both objects and vectors, allowing for combining vector search with structured filtering. LangChain connects to Weaviate via the weaviate-client package, the official Typescript client for Weaviate.\n\nThis guide provides a quick overview for getting started with Weaviate [vector stores](/oss/integrations/vectorstores). For detailed documentation of all `WeaviateStore` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_weaviate.WeaviateStore.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/clickhouse.mdx",
    "filename": "clickhouse.mdx",
    "size_bytes": 3718,
    "line_count": 109,
    "preview": "---\ntitle: ClickHouse\n---\n\n<Tip>\n**Compatibility**\n\nOnly available on Node.js.\n</Tip>\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/singlestore.mdx",
    "filename": "singlestore.mdx",
    "size_bytes": 9297,
    "line_count": 209,
    "preview": "---\ntitle: SingleStore\n---\n\n[SingleStoreDB](https://singlestore.com/) is a robust, high-performance distributed SQL database solution designed to excel in both [cloud](https://www.singlestore.com/cloud/) and on-premises environments. Boasting a versatile feature set, it offers seamless deployment options while delivering unparalleled performance.\n\nA standout feature of SingleStoreDB is its advanced support for vector storage and operations, making it an ideal choice for applications requiring intricate AI capabilities such as text similarity matching. With built-in vector functions like [dot_product](https://docs.singlestore.com/managed-service/en/reference/sql-reference/vector-functions/dot_product.html) and [euclidean_distance](https://docs.singlestore.com/managed-service/en/reference/sql-reference/vector-functions/euclidean_distance.html), SingleStoreDB empowers developers to implement sophisticated algorithms efficiently.\n\nFor developers keen on leveraging vector data within SingleStoreDB, a comprehensive tutorial is available, guiding them through the intricacies of [working with vector data](https://docs.singlestore.com/managed-service/en/developer-resources/functional-extensions/working-with-vector-data.html). This tutorial delves into the Vector Store within SingleStoreDB, showcasing its ability to facilitate searches based on vector similarity. Leveraging vector indexes, queries can be executed with remarkable speed, enabling swift retrieval of relevant data.\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/elasticsearch.mdx",
    "filename": "elasticsearch.mdx",
    "size_bytes": 9605,
    "line_count": 276,
    "preview": "---\ntitle: Elasticsearch\n---\n\n<Tip>\n**Compatibility**: Only available on Node.js.\n</Tip>\n\n[Elasticsearch](https://github.com/elastic/elasticsearch) is a distributed, RESTful search engine optimized for speed and relevance on production-scale workloads. It supports also vector search using the [k-nearest neighbor](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) (kNN) algorithm and also [custom models for Natural Language Processing](https://www.elastic.co/blog/how-to-deploy-nlp-text-embeddings-and-vector-search) (NLP).\nYou can read more about the support of vector search in Elasticsearch [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/knn-search.html).\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/libsql.mdx",
    "filename": "libsql.mdx",
    "size_bytes": 5480,
    "line_count": 163,
    "preview": "---\ntitle: libSQL\n---\n\n[Turso](https://turso.tech) is a SQLite-compatible database built on [libSQL](https://docs.turso.tech/libsql), the Open Contribution fork of SQLite. Vector Similiarity Search is built into Turso and libSQL as a native datatype, enabling you to store and query vectors directly in the database.\n\nLangChain.js supports using a local libSQL, or remote Turso database as a vector store, and provides a simple API to interact with it.\n\nThis guide provides a quick overview for getting started with libSQL vector stores. For detailed documentation of all libSQL features and configurations head to the API reference.\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/cloudflare_vectorize.mdx",
    "filename": "cloudflare_vectorize.mdx",
    "size_bytes": 3647,
    "line_count": 133,
    "preview": "---\ntitle: Cloudflare Vectorize\n---\n\nIf you're deploying your project in a Cloudflare worker, you can use [Cloudflare Vectorize](https://developers.cloudflare.com/vectorize/) with LangChain.js.\nIt's a powerful and convenient option that's built directly into Cloudflare.\n\n## Setup\n\n<Tip>\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/milvus.mdx",
    "filename": "milvus.mdx",
    "size_bytes": 3292,
    "line_count": 104,
    "preview": "---\ntitle: Milvus\n---\n\n[Milvus](https://milvus.io/) is a vector database built for embeddings similarity search and AI applications.\n\n<Tip>\n**Compatibility**\n\nOnly available on Node.js.\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/qdrant.mdx",
    "filename": "qdrant.mdx",
    "size_bytes": 6458,
    "line_count": 197,
    "preview": "---\ntitle: QdrantVectorStore\n---\n\n<Tip>\n**Compatibility**: Only available on Node.js.\n</Tip>\n\n[Qdrant](https://qdrant.tech/) is a vector similarity search engine. It provides a production-ready service with a convenient API to store, search, and manage points - vectors with an additional payload.\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/opensearch.mdx",
    "filename": "opensearch.mdx",
    "size_bytes": 3826,
    "line_count": 119,
    "preview": "---\ntitle: OpenSearch\n---\n\n<Tip>\n**Compatibility**\n\nOnly available on Node.js.\n</Tip>\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/pinecone.mdx",
    "filename": "pinecone.mdx",
    "size_bytes": 7135,
    "line_count": 212,
    "preview": "---\ntitle: PineconeStore\n---\n\n[Pinecone](https://www.pinecone.io/) is a vector database that helps power AI for some of the world’s best companies.\n\nThis guide provides a quick overview for getting started with Pinecone [vector stores](/oss/integrations/vectorstores). For detailed documentation of all `PineconeStore` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_pinecone.PineconeStore.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/rockset.mdx",
    "filename": "rockset.mdx",
    "size_bytes": 2272,
    "line_count": 77,
    "preview": "---\ntitle: Rockset\n---\n\nRockset (acquired by OpenAI) is a real-time analyitics SQL database that runs in the cloud.\nRockset provides vector search capabilities, in the form of SQL functions, to support AI applications that rely on text similarity.\n\n## Setup\n\nInstall the rockset client.\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/astradb.mdx",
    "filename": "astradb.mdx",
    "size_bytes": 2622,
    "line_count": 94,
    "preview": "---\ntitle: Astra DB\n---\n\n<Tip>\n**Compatibility**\n\nOnly available on Node.js.\n</Tip>\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/google_cloudsql_pg.mdx",
    "filename": "google_cloudsql_pg.mdx",
    "size_bytes": 7352,
    "line_count": 221,
    "preview": "---\ntitle: Google Cloud SQL for PostgreSQL\n---\n\n[Cloud SQL](https://cloud.google.com/sql) is a fully managed relational database service that offers high performance, seamless integration, and impressive scalability and offers database engines such as PostgreSQL.\n\nThis guide provides a quick overview of how to use Cloud SQL for PostgreSQL to store vector embeddings with the `PostgresVectorStore` class.\n\n## Overview\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/neon.mdx",
    "filename": "neon.mdx",
    "size_bytes": 4236,
    "line_count": 150,
    "preview": "---\ntitle: Neon Postgres\n---\n\nNeon is a fully managed serverless PostgreSQL database. It separates storage and compute to offer\nfeatures such as instant branching and automatic scaling.\n\nWith the `pgvector` extension, Neon provides a vector store that can be used with LangChain.js to store and query embeddings.\n\n## Setup\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/couchbase_query.mdx",
    "filename": "couchbase_query.mdx",
    "size_bytes": 13864,
    "line_count": 381,
    "preview": "---\ntitle: Couchbase Query Vector Store\n---\n\nThe `CouchbaseQueryVectorStore` is the preferred implementation of Vector Search in Couchbase. It uses the [Query Service](https://docs.couchbase.com/server/current/learn/services-and-indexes/services/query-service.html) (SQL++) and [Index Service](https://docs.couchbase.com/server/current/learn/services-and-indexes/services/index-service.html) for vector similarity search, instead of the Search service. This provides a more powerful and straightforward approach for vector operations using SQL++ queries with vector functions.\n\nMore information about Couchbase's vector search capabilities can be found in the official documentation: [Choose the Right Vector Index](https://docs.couchbase.com/server/current/vector-index/use-vector-indexes.html).\n\n<Warning>\n    This functionality is only available in [Couchbase 8.0](https://docs.couchbase.com/server/8.0/introduction/whats-new.html) and above.\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/azure_cosmosdb_nosql.mdx",
    "filename": "azure_cosmosdb_nosql.mdx",
    "size_bytes": 8325,
    "line_count": 188,
    "preview": "---\ntitle: Azure Cosmos DB for NoSQL\n---\n\n> [Azure Cosmos DB for NoSQL](https://learn.microsoft.com/azure/cosmos-db/nosql/) provides support for querying items with flexible schemas and native support for JSON. It now offers vector indexing and search. This feature is designed to handle high-dimensional vectors, enabling efficient and accurate vector search at any scale. You can now store vectors directly in the documents alongside your data. Each document in your database can contain not only traditional schema-free data, but also high-dimensional vectors as other properties of the documents.\n\nLearn how to leverage the vector search capabilities of Azure Cosmos DB for NoSQL from [this page](https://learn.microsoft.com/azure/cosmos-db/nosql/vector-search). If you don't have an Azure account, you can [create a free account](https://azure.microsoft.com/free/) to get started.\n\n## Setup\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/mariadb.mdx",
    "filename": "mariadb.mdx",
    "size_bytes": 9823,
    "line_count": 325,
    "preview": "---\ntitle: MariaDB\n---\n\n<Tip>\n**Compatibility**: Only available on Node.js.\n</Tip>\n\nThis requires MariaDB 11.7 or later version\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/turbopuffer.mdx",
    "filename": "turbopuffer.mdx",
    "size_bytes": 2774,
    "line_count": 122,
    "preview": "---\ntitle: Turbopuffer\n---\n\n## Setup\n\nFirst you must sign up for a Turbopuffer account [here](https://turbopuffer.com/join).\nThen, once you have an account you can create an API key.\n\nSet your API key as an environment variable:\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/momento_vector_index.mdx",
    "filename": "momento_vector_index.mdx",
    "size_bytes": 5811,
    "line_count": 192,
    "preview": "---\ntitle: Momento Vector Index (MVI)\n---\n\n[MVI](https://gomomento.com): the most productive, easiest to use, serverless vector index for your data. To get started with MVI, simply sign up for an account. There's no need to handle infrastructure, manage servers, or be concerned about scaling. MVI is a service that scales automatically to meet your needs. Whether in Node.js, browser, or edge, Momento has you covered.\n\nTo sign up and access MVI, visit the [Momento Console](https://console.gomomento.com).\n\n## Setup\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/lancedb.mdx",
    "filename": "lancedb.mdx",
    "size_bytes": 4462,
    "line_count": 147,
    "preview": "---\ntitle: LanceDB\n---\n\nLanceDB is an embedded vector database for AI applications. It is open source and distributed with an Apache-2.0 license.\n\nLanceDB datasets are persisted to disk and can be shared between Node.js and Python.\n\n## Setup\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/upstash.mdx",
    "filename": "upstash.mdx",
    "size_bytes": 8169,
    "line_count": 236,
    "preview": "---\ntitle: UpstashVectorStore\n---\n\n[Upstash Vector](https://upstash.com/) is a REST based serverless vector database, designed for working with vector embeddings.\n\nThis guide provides a quick overview for getting started with Upstash [vector stores](/oss/integrations/vectorstores). For detailed documentation of all `UpstashVectorStore` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_vectorstores_upstash.UpstashVectorStore.html).\n\n## Overview\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/memory.mdx",
    "filename": "memory.mdx",
    "size_bytes": 6724,
    "line_count": 208,
    "preview": "---\ntitle: MemoryVectorStore\n---\n\nLangChain offers is an in-memory, ephemeral vectorstore that stores embeddings in-memory and does an exact, linear search for the most similar embeddings. The default similarity metric is cosine similarity, but can be changed to any of the similarity metrics supported by [ml-distance](https://mljs.github.io/distance/modules/similarity.html).\n\nAs it is intended for demos, it does not yet support ids or deletion.\n\nThis guide provides a quick overview for getting started with in-memory [`vector stores`](/oss/integrations/vectorstores). For detailed documentation of all `MemoryVectorStore` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.vectorstores_memory.MemoryVectorStore.html).\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/analyticdb.mdx",
    "filename": "analyticdb.mdx",
    "size_bytes": 3540,
    "line_count": 103,
    "preview": "---\ntitle: AnalyticDB\n---\n\n[AnalyticDB for PostgreSQL](https://www.alibabacloud.com/help/en/analyticdb-for-postgresql/latest/product-introduction-overview) is a massively parallel processing (MPP) data warehousing service that is designed to analyze large volumes of data online.\n\n`AnalyticDB for PostgreSQL` is developed based on the open source `Greenplum Database` project and is enhanced with in-depth extensions by `Alibaba Cloud`. AnalyticDB for PostgreSQL is compatible with the ANSI SQL 2003 syntax and the PostgreSQL and Oracle database ecosystems. AnalyticDB for PostgreSQL also supports row store and column store. AnalyticDB for PostgreSQL processes petabytes of data offline at a high performance level and supports highly concurrent online queries.\n\nThis notebook shows how to use functionality related to the `AnalyticDB` vector database.\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/usearch.mdx",
    "filename": "usearch.mdx",
    "size_bytes": 1874,
    "line_count": 68,
    "preview": "---\ntitle: USearch\n---\n\n<Tip>\n**Compatibility**\n\nOnly available on Node.js.\n</Tip>\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/xata.mdx",
    "filename": "xata.mdx",
    "size_bytes": 6485,
    "line_count": 188,
    "preview": "---\ntitle: Xata\n---\n\n[Xata](https://xata.io) is a serverless data platform, based on PostgreSQL. It provides a type-safe TypeScript/JavaScript SDK for interacting with your database, and a UI for managing your data.\n\nXata has a native vector type, which can be added to any table, and supports similarity search. LangChain inserts vectors directly to Xata, and queries it for the nearest neighbors of a given vector, so that you can use all the LangChain Embeddings integrations with Xata.\n\n## Setup\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/googlevertexai.mdx",
    "filename": "googlevertexai.mdx",
    "size_bytes": 5486,
    "line_count": 184,
    "preview": "---\ntitle: Google Vertex AI Matching Engine\n---\n\n<Tip>\n**Compatibility**\n\nOnly available on Node.js.\n</Tip>\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/pgvector.mdx",
    "filename": "pgvector.mdx",
    "size_bytes": 13727,
    "line_count": 443,
    "preview": "---\ntitle: PGVectorStore\n---\n\n<Tip>\n**Compatibility**: Only available on Node.js.\n</Tip>\n\nTo enable vector search in generic PostgreSQL databases, LangChain.js supports using the [`pgvector`](https://github.com/pgvector/pgvector) Postgres extension.\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/faiss.mdx",
    "filename": "faiss.mdx",
    "size_bytes": 8522,
    "line_count": 280,
    "preview": "---\ntitle: FaissStore\n---\n\n<Tip>\n**Compatibility**: Only available on Node.js.\n</Tip>\n\n[Faiss](https://github.com/facebookresearch/faiss) is a library for efficient similarity search and clustering of dense vectors.\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/redis.mdx",
    "filename": "redis.mdx",
    "size_bytes": 15552,
    "line_count": 482,
    "preview": "---\ntitle: RedisVectorStore\n---\n\n<Tip>\n**Compatibility**: Only available on Node.js.\n</Tip>\n\n[Redis](https://redis.io/) is a fast open source, in-memory data store. As part of the [Redis Stack](https://redis.io/docs/latest/operate/oss_and_stack/install/install-stack/), [RediSearch](https://redis.io/docs/latest/develop/interact/search-and-query/) is the module that enables vector similarity semantic search, as well as many other types of searching.\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/chroma.mdx",
    "filename": "chroma.mdx",
    "size_bytes": 7553,
    "line_count": 235,
    "preview": "---\ntitle: Chroma\n---\n\n[Chroma](https://docs.trychroma.com/getting-started) is a AI-native open-source vector database focused on developer productivity and happiness. Chroma is licensed under Apache 2.0.\n\nThis guide provides a quick overview for getting started with Chroma [`vector stores`](/oss/integrations/vectorstores). For detailed documentation of all `Chroma` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_vectorstores_chroma.Chroma.html).\n\n<Info>\n**Chroma Cloud**\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/cassandra.mdx",
    "filename": "cassandra.mdx",
    "size_bytes": 8522,
    "line_count": 215,
    "preview": "---\ntitle: Cassandra\n---\n\n<Tip>\n**Compatibility**\n\nOnly available on Node.js.\n</Tip>\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/hanavector.mdx",
    "filename": "hanavector.mdx",
    "size_bytes": 24429,
    "line_count": 726,
    "preview": "---\ntitle: SAP HANA Cloud Vector Engine\n---\n\n[SAP HANA Cloud Vector Engine](https://www.sap.com/events/teched/news-guide/ai.html#article8) is a vector store fully integrated into the `SAP HANA Cloud database`.\n\n## Setup\n\nYou'll first need to install either the [`@sap/hana-client`](https://www.npmjs.com/package/@sap/hana-client) or the [`hdb`](https://www.npmjs.com/package/hdb) package, and the [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) package:\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/azure_aisearch.mdx",
    "filename": "azure_aisearch.mdx",
    "size_bytes": 7306,
    "line_count": 145,
    "preview": "---\ntitle: Azure AI Search\n---\n\n[Azure AI Search](https://azure.microsoft.com/products/ai-services/ai-search) (formerly known as Azure Search and Azure Cognitive Search) is a distributed, RESTful search engine optimized for speed and relevance on production-scale workloads on Azure. It supports also vector search using the [k-nearest neighbor](https://en.wikipedia.org/wiki/Nearest_neighbor_search) (kNN) algorithm and also [semantic search](https://learn.microsoft.com/azure/search/semantic-search-overview).\n\nThis vector store integration supports full text search, vector search and [hybrid search for best ranking performance](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-cognitive-search-outperforming-vector-search-with-hybrid/ba-p/3929167).\n\nLearn how to leverage the vector search capabilities of Azure AI Search from [this page](https://learn.microsoft.com/azure/search/vector-search-overview). If you don't have an Azure account, you can [create a free account](https://azure.microsoft.com/free/) to get started.\n\n"
  }
,
  {
    "path": "javascript/integrations/vectorstores/prisma.mdx",
    "filename": "prisma.mdx",
    "size_bytes": 4209,
    "line_count": 154,
    "preview": "---\ntitle: Prisma\n---\n\nFor augmenting existing models in PostgreSQL database with vector search, LangChain supports using [Prisma](https://www.prisma.io/) together with PostgreSQL and [`pgvector`](https://github.com/pgvector/pgvector) Postgres extension.\n\n## Setup\n\n### Setup database instance with Supabase\n\n"
  }
,
  {
    "path": "javascript/integrations/document_transformers/html-to-text.mdx",
    "filename": "html-to-text.mdx",
    "size_bytes": 5771,
    "line_count": 128,
    "preview": "---\ntitle: html-to-text\n---\n\nWhen ingesting HTML documents for later retrieval, we are often interested only in the actual content of the webpage rather than semantics.\nStripping HTML tags from documents with the HtmlToTextTransformer can result in more content-rich chunks, making retrieval more effective.\n\n## Setup\n\nYou'll need to install the [`html-to-text`](https://www.npmjs.com/package/html-to-text) npm package:\n"
  }
,
  {
    "path": "javascript/integrations/document_transformers/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 606,
    "line_count": 29,
    "preview": "---\ntitle: Document transformers\n---\n<Columns cols={3}>\n  <Card\n    title=\"html-to-text\"\n    icon=\"link\"\n    href=\"/oss/integrations/document_transformers/html-to-text\"\n    arrow=\"true\"\n    cta=\"View guide\"\n"
  }
,
  {
    "path": "javascript/integrations/document_transformers/openai_metadata_tagger.mdx",
    "filename": "openai_metadata_tagger.mdx",
    "size_bytes": 5020,
    "line_count": 157,
    "preview": "---\ntitle: OpenAI functions metadata tagger\n---\n\nIt can often be useful to tag ingested documents with structured metadata, such as the title, tone, or length of a document, to allow for more targeted similarity search later. However, for large numbers of documents, performing this labelling process manually can be tedious.\n\nThe `MetadataTagger` document transformer automates this process by extracting metadata from each provided document according to a provided schema. It uses a configurable OpenAI Functions-powered chain under the hood, so if you pass a custom LLM instance, it must be an OpenAI model with functions support.\n\n**Note:** This document transformer works best with complete documents, so it's best to run it first with whole documents before doing any other splitting or processing!\n\n"
  }
,
  {
    "path": "javascript/integrations/document_transformers/mozilla_readability.mdx",
    "filename": "mozilla_readability.mdx",
    "size_bytes": 5910,
    "line_count": 128,
    "preview": "---\ntitle: mozilla/readability\n---\n\nWhen ingesting HTML documents for later retrieval, we are often interested only in the actual content of the webpage rather than semantics.\nStripping HTML tags from documents with the MozillaReadabilityTransformer can result in more content-rich chunks, making retrieval more effective.\n\n## Setup\n\nYou'll need to install the [`@mozilla/readability`](https://www.npmjs.com/package/@mozilla/readability) and the [`jsdom`](https://www.npmjs.com/package/jsdom) npm package:\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 15980,
    "line_count": 188,
    "preview": "---\nsidebar_position: 0\nsidebarTitle: \"Document loaders\"\n---\n\nDocument loaders provide a **standard interface** for reading data from different sources (such as Slack, Notion, or Google Drive) into LangChain's @[Document] format.\nThis ensures that data can be handled consistently regardless of the source.\n\nAll document loaders implement the @[BaseLoader] interface.\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/file_loaders/docx.mdx",
    "filename": "docx.mdx",
    "size_bytes": 1444,
    "line_count": 58,
    "preview": "---\ntitle: Docx files\n---\n\nThe `DocxLoader` allows you to extract text data from Microsoft Word documents. It supports both the modern `.docx` format and the legacy `.doc` format. Depending on the file type, additional dependencies are required.\n\n---\n\n## Setup\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/file_loaders/subtitles.mdx",
    "filename": "subtitles.mdx",
    "size_bytes": 458,
    "line_count": 22,
    "preview": "---\ntitle: Subtitles\n---\n\nThis example goes over how to load data from subtitle files. One document will be created for each subtitles file.\n\n## Setup\n\n```bash npm\nnpm install srt-parser-2\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/file_loaders/jsonlines.mdx",
    "filename": "jsonlines.mdx",
    "size_bytes": 980,
    "line_count": 45,
    "preview": "---\ntitle: JSONLines files\n---\n\nThis example goes over how to load data from JSONLines or JSONL files. The second argument is a JSONPointer to the property to extract from each JSON object in the file. One document will be created for each JSON object in the file.\n\nExample JSONLines file:\n\n```json\n{\"html\": \"This is a sentence.\"}\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/file_loaders/json.mdx",
    "filename": "json.mdx",
    "size_bytes": 2371,
    "line_count": 105,
    "preview": "---\ntitle: JSON files\n---\n\nThe JSON loader use [JSON pointer](https://github.com/janl/node-jsonpointer) to target keys in your JSON files you want to target.\n\n### No JSON pointer example\n\nThe most simple way of using it is to specify no JSON pointer.\nThe loader will load all strings it finds in the JSON object.\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/file_loaders/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 2773,
    "line_count": 125,
    "preview": "---\ntitle: File Loaders\n---\n\n<Tip>\n**Compatibility**\n\nOnly available on Node.js.\n</Tip>\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/file_loaders/unstructured.mdx",
    "filename": "unstructured.mdx",
    "size_bytes": 4277,
    "line_count": 138,
    "preview": "---\ntitle: UnstructuredLoader\n---\n\n<Tip>\n**Compatibility**: Only available on Node.js.\n</Tip>\n\nThis notebook provides a quick overview for getting started with `UnstructuredLoader` [document loaders](/oss/integrations/document_loaders). For detailed documentation of all `UnstructuredLoader` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_document_loaders_fs_unstructured.UnstructuredLoader.html).\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/file_loaders/openai_whisper_audio.mdx",
    "filename": "openai_whisper_audio.mdx",
    "size_bytes": 879,
    "line_count": 35,
    "preview": "---\ntitle: Open AI Whisper Audio\n---\n\n<Tip>\n**Compatibility**\n\nOnly available on Node.js.\n</Tip>\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/file_loaders/notion_markdown.mdx",
    "filename": "notion_markdown.mdx",
    "size_bytes": 902,
    "line_count": 23,
    "preview": "---\ntitle: Notion markdown export\n---\n\nThis example goes over how to load data from your Notion pages exported from the notion dashboard.\n\nFirst, export your notion pages as **Markdown & CSV** as per the offical explanation [here](https://www.notion.so/help/export-your-content). Make sure to select `include subpages` and `Create folders for subpages.`\n\nThen, unzip the downloaded file and move the unzipped folder into your repository. It should contain the markdown files of your pages.\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/file_loaders/text.mdx",
    "filename": "text.mdx",
    "size_bytes": 2006,
    "line_count": 80,
    "preview": "---\ntitle: TextLoader\n---\n\n<Tip>\n**Compatibility**: Only available on Node.js.\n</Tip>\n\nThis notebook provides a quick overview for getting started with `TextLoader` [document loaders](/oss/integrations/document_loaders). For detailed documentation of all `TextLoader` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.document_loaders_fs_text.TextLoader.html).\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/file_loaders/csv.mdx",
    "filename": "csv.mdx",
    "size_bytes": 3834,
    "line_count": 124,
    "preview": "---\ntitle: CSV\n---\n\n<Tip>\n**Compatibility**: Only available on Node.js.\n</Tip>\n\nThis notebook provides a quick overview for getting started with `CSVLoader` [document loaders](/oss/integrations/document_loaders). For detailed documentation of all `CSVLoader` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_document_loaders_fs_csv.CSVLoader.html).\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/file_loaders/chatgpt.mdx",
    "filename": "chatgpt.mdx",
    "size_bytes": 831,
    "line_count": 36,
    "preview": "---\ntitle: ChatGPT files\n---\n\nThis example goes over how to load conversations.json from your ChatGPT data export folder. You can get your data export by email by going to: ChatGPT -> (Profile) - Settings -> Export data -> Confirm export -> Check email.\n\n## Usage, extracting all logs\n\nExample code:\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/file_loaders/epub.mdx",
    "filename": "epub.mdx",
    "size_bytes": 861,
    "line_count": 34,
    "preview": "---\ntitle: EPUB files\n---\n\nThis example goes over how to load data from EPUB files. By default, one document will be created for each chapter in the EPUB file, you can change this behavior by setting the `splitChapters` option to `false`.\n\n# Setup\n\n```bash npm\nnpm install @langchain/community @langchain/core epub2 html-to-text\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/file_loaders/pdf.mdx",
    "filename": "pdf.mdx",
    "size_bytes": 16379,
    "line_count": 367,
    "preview": "---\ntitle: PDFLoader\n---\n\n<Tip>\n**Compatibility**: Only available on Node.js.\n</Tip>\n\nThis notebook provides a quick overview for getting started with `PDFLoader` [document loaders](/oss/integrations/document_loaders). For detailed documentation of all `PDFLoader` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_document_loaders_fs_pdf.PDFLoader.html).\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/file_loaders/multi_file.mdx",
    "filename": "multi_file.mdx",
    "size_bytes": 1427,
    "line_count": 46,
    "preview": "---\ntitle: Multiple individual files\n---\n\nThis example goes over how to load data from multiple file paths. The second argument is a map of file extensions to loader factories. Each file will be passed to the matching loader, and the resulting documents will be concatenated together.\n\nExample files:\n\n```text\nsrc/document_loaders/example_data/example/\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/file_loaders/directory.mdx",
    "filename": "directory.mdx",
    "size_bytes": 3131,
    "line_count": 109,
    "preview": "---\ntitle: DirectoryLoader\n---\n\n\n<Tip>\n**Compatibility**: Only available on Node.js.\n</Tip>\n\nThis notebook provides a quick overview for getting started with `DirectoryLoader` [document loaders](/oss/integrations/document_loaders). For detailed documentation of all `DirectoryLoader` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.document_loaders_fs_directory.DirectoryLoader.html).\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/file_loaders/pptx.mdx",
    "filename": "pptx.mdx",
    "size_bytes": 453,
    "line_count": 20,
    "preview": "---\ntitle: PPTX files\n---\n\nThis example goes over how to load data from PPTX files. By default, one document will be created for all pages in the PPTX file.\n\n## Setup\n\n```bash npm\nnpm install officeparser\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/figma.mdx",
    "filename": "figma.mdx",
    "size_bytes": 740,
    "line_count": 25,
    "preview": "---\ntitle: Figma\n---\n\nThis example goes over how to load data from a Figma file.\nYou will need a Figma access token in order to get started.\n\n```typescript\nimport { FigmaFileLoader } from \"@langchain/community/document_loaders/web/figma\";\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/web_playwright.mdx",
    "filename": "web_playwright.mdx",
    "size_bytes": 3593,
    "line_count": 96,
    "preview": "---\ntitle: Webpages, with Playwright\n---\n\n<Tip>\n**Compatibility**\n\nOnly available on Node.js.\n</Tip>\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/hn.mdx",
    "filename": "hn.mdx",
    "size_bytes": 456,
    "line_count": 20,
    "preview": "---\ntitle: Hacker News\n---\n\nThis example goes over how to load data from the hacker news website, using Cheerio. One document will be created for each page.\n\n## Setup\n\n```bash npm\nnpm install @langchain/community @langchain/core cheerio\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/college_confidential.mdx",
    "filename": "college_confidential.mdx",
    "size_bytes": 547,
    "line_count": 22,
    "preview": "---\ntitle: College Confidential\n---\n\nThis example goes over how to load data from the college confidential website, using Cheerio. One document will be created for each page.\n\n## Setup\n\n```bash npm\nnpm install @langchain/community @langchain/core cheerio\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/s3.mdx",
    "filename": "s3.mdx",
    "size_bytes": 1475,
    "line_count": 51,
    "preview": "---\ntitle: S3FileLoader\n---\n\n<Tip>\n**Compatibility**\n\nOnly available on Node.js.\n</Tip>\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/serpapi.mdx",
    "filename": "serpapi.mdx",
    "size_bytes": 2694,
    "line_count": 72,
    "preview": "---\ntitle: SerpAPI Loader\n---\n\nThis guide shows how to use SerpApi with LangChain to load web search results.\n\n## Overview\n\n[SerpApi](https://serpapi.com/) is a real-time API that provides access to search results from various search engines. It is commonly used for tasks like competitor analysis and rank tracking. It empowers businesses to scrape, extract, and make sense of data from all search engines' result pages.\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/apify_dataset.mdx",
    "filename": "apify_dataset.mdx",
    "size_bytes": 11863,
    "line_count": 337,
    "preview": "---\ntitle: Apify Dataset\n---\n\nThis guide shows how to use [Apify](https://apify.com) with LangChain to load documents from an Apify Dataset.\n\n## Overview\n\n[Apify](https://apify.com) is a cloud platform for web scraping and data extraction, which provides an [ecosystem](https://apify.com/store) of more than 10,000 ready-made apps called _Actors_ for various web scraping, crawling, and data extraction use cases.\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/firecrawl.mdx",
    "filename": "firecrawl.mdx",
    "size_bytes": 5744,
    "line_count": 148,
    "preview": "---\ntitle: FireCrawlLoader\n---\n\n\nThis notebook provides a quick overview for getting started with [FireCrawlLoader](/oss/integrations/document_loaders/). For detailed documentation of all FireCrawlLoader features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_document_loaders_web_firecrawl.FireCrawlLoader.html).\n\n## Overview\n\n### Integration details\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/recursive_url_loader.mdx",
    "filename": "recursive_url_loader.mdx",
    "size_bytes": 18245,
    "line_count": 400,
    "preview": "---\ntitle: RecursiveUrlLoader\n---\n\n<Tip>\n**Compatibility**: Only available on Node.js.\n</Tip>\n\nThis notebook provides a quick overview for getting started with [RecursiveUrlLoader](/oss/integrations/document_loaders/). For detailed documentation of all RecursiveUrlLoader features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_document_loaders_web_recursive_url.RecursiveUrlLoader.html).\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/searchapi.mdx",
    "filename": "searchapi.mdx",
    "size_bytes": 3324,
    "line_count": 85,
    "preview": "---\ntitle: SearchApiLoader\n---\n\nThis guide shows how to use SearchApi with LangChain to load web search results.\n\n## Overview\n\n[SearchApi](https://www.searchapi.io/) is a real-time API that grants developers access to results from a variety of search engines, including engines like [Google Search](https://www.searchapi.io/docs/google),\n[Google News](https://www.searchapi.io/docs/google-news), [Google Scholar](https://www.searchapi.io/docs/google-scholar), [YouTube Transcripts](https://www.searchapi.io/docs/youtube-transcripts) or any other engine that could be found in documentation.\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/github.mdx",
    "filename": "github.mdx",
    "size_bytes": 3977,
    "line_count": 134,
    "preview": "---\ntitle: GitHub\n---\n\nThis example goes over how to load data from a GitHub repository.\nYou can set the `GITHUB_ACCESS_TOKEN` environment variable to a GitHub access token to increase the rate limit and access private repositories.\n\n## Setup\n\nThe GitHub loader requires the [ignore npm package](https://www.npmjs.com/package/ignore) as a peer dependency. Install it like this:\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/web_puppeteer.mdx",
    "filename": "web_puppeteer.mdx",
    "size_bytes": 18545,
    "line_count": 444,
    "preview": "---\ntitle: PuppeteerWebBaseLoader\n---\n\n<Tip>\n**Compatibility**: Only available on Node.js.\n</Tip>\n\nThis notebook provides a quick overview for getting started with [PuppeteerWebBaseLoader](/oss/integrations/document_loaders/). For detailed documentation of all PuppeteerWebBaseLoader features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_document_loaders_web_puppeteer.PuppeteerWebBaseLoader.html).\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/airtable.mdx",
    "filename": "airtable.mdx",
    "size_bytes": 1704,
    "line_count": 65,
    "preview": "---\ntitle: AirtableLoader\n---\n\nThe `AirtableLoader` class provides functionality to load documents from Airtable tables. It supports two main methods:\n\n1. `load()`: Retrieves all records at once, ideal for small to moderate datasets.\n2. `loadLazy()`: Fetches records one by one, which is more memory-efficient for large datasets.\n\n## Prerequisites\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/confluence.mdx",
    "filename": "confluence.mdx",
    "size_bytes": 1676,
    "line_count": 55,
    "preview": "---\ntitle: Confluence\n---\n\n<Tip>\n**Compatibility**\n\nOnly available on Node.js.\n</Tip>\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/azure_blob_storage_container.mdx",
    "filename": "azure_blob_storage_container.mdx",
    "size_bytes": 1253,
    "line_count": 46,
    "preview": "---\ntitle: Azure Blob Storage Container\n---\n\n<Tip>\n**Compatibility**\n\nOnly available on Node.js.\n</Tip>\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 5869,
    "line_count": 252,
    "preview": "---\ntitle: Web Loaders\n---\n\nThese loaders are used to load web resources. They do not involve the local file system.\n\n## All web loaders\n\n<Columns cols={3}>\n  <Card\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/browserbase.mdx",
    "filename": "browserbase.mdx",
    "size_bytes": 1643,
    "line_count": 46,
    "preview": "---\ntitle: Browserbase Loader\n---\n\n## Description\n\n[Browserbase](https://browserbase.com) is a developer platform to reliably run, manage, and monitor headless browsers.\n\nPower your AI data retrievals with:\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/langsmith.mdx",
    "filename": "langsmith.mdx",
    "size_bytes": 5516,
    "line_count": 194,
    "preview": "---\ntitle: LangSmithLoader\n---\n\n\nThis notebook provides a quick overview for getting started with the [LangSmithLoader](/oss/integrations/document_loaders/). For detailed documentation of all `LangSmithLoader` features and configurations head to the [API reference](https://api.js.langchain.com/classes/_langchain_core.document_loaders_langsmith.LangSmithLoader.html).\n\n## Overview\n\n### Integration details\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/spider.mdx",
    "filename": "spider.mdx",
    "size_bytes": 1684,
    "line_count": 41,
    "preview": "---\ntitle: Spider\n---\n\n[Spider](https://spider.cloud/?ref=langchainjs) is the [fastest](https://github.com/spider-rs/spider/blob/main/benches/BENCHMARKS.md#benchmark-results) crawler. It converts any website into pure HTML, markdown, metadata or text while enabling you to crawl with custom actions using AI.\n\n## Overview\n\nSpider allows you to use high performance proxies to prevent detection, caches AI actions, webhooks for crawling status, scheduled crawls etc...\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/jira.mdx",
    "filename": "jira.mdx",
    "size_bytes": 1217,
    "line_count": 47,
    "preview": "---\ntitle: Jira\n---\n\n<Tip>\n**Compatibility**\n\nOnly available on Node.js.\n</Tip>\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/gitbook.mdx",
    "filename": "gitbook.mdx",
    "size_bytes": 929,
    "line_count": 35,
    "preview": "---\ntitle: GitBook\n---\n\nThis example goes over how to load data from any GitBook, using Cheerio. One document will be created for each page.\n\n## Setup\n\n```bash npm\nnpm install @langchain/community @langchain/core cheerio\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/assemblyai_audio_transcription.mdx",
    "filename": "assemblyai_audio_transcription.mdx",
    "size_bytes": 3142,
    "line_count": 73,
    "preview": "---\ntitle: AssemblyAI Audio Transcript\n---\n\nThis covers how to load audio (and video) transcripts as document objects from a file using the [AssemblyAI API](https://www.assemblyai.com/docs/api-reference/transcripts/submit?utm_source=langchainjs).\n\n## Usage\n\nFirst, you'll need to install the official AssemblyAI package:\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/google_cloudsql_pg.mdx",
    "filename": "google_cloudsql_pg.mdx",
    "size_bytes": 4617,
    "line_count": 138,
    "preview": "---\ntitle: Google Cloud SQL for PostgreSQL\n---\n\n[Cloud SQL](https://cloud.google.com/sql) is a fully managed relational database service that offers high\nperformance, seamless integration, and impressive scalability and offers database engines such as PostgreSQL.\n\nThis guide provides a quick overview of how to use Cloud SQL for PostgreSQL to load Documents with the `PostgresLoader` class.\n\n## Overview\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/sort_xyz_blockchain.mdx",
    "filename": "sort_xyz_blockchain.mdx",
    "size_bytes": 3050,
    "line_count": 94,
    "preview": "---\ntitle: Blockchain Data\n---\n\nThis example shows how to load blockchain data, including NFT metadata and transactions for a contract address, via the sort.xyz SQL API.\n\nYou will need a free Sort API key, visiting sort.xyz to obtain one.\n\n```typescript\nimport { SortXYZBlockchainLoader } from \"@langchain/community/document_loaders/web/sort_xyz_blockchain\";\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/youtube.mdx",
    "filename": "youtube.mdx",
    "size_bytes": 771,
    "line_count": 30,
    "preview": "---\ntitle: YouTube transcripts\n---\n\nThis covers how to load YouTube transcripts into LangChain documents.\n\n## Setup\n\nYou'll need to install the [youtubei.js](https://www.npmjs.com/package/youtubei.js) to extract metadata:\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/couchbase.mdx",
    "filename": "couchbase.mdx",
    "size_bytes": 2943,
    "line_count": 96,
    "preview": "---\ntitle: Couchbase\n---\n\n[Couchbase](http://couchbase.com/) is an award-winning distributed NoSQL cloud database that delivers unmatched versatility, performance, scalability, and financial value for all of your cloud, mobile, AI, and edge computing applications.\n\nThis guide shows how to use load documents from couchbase database.\n\n# Installation\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/pdf.mdx",
    "filename": "pdf.mdx",
    "size_bytes": 9616,
    "line_count": 232,
    "preview": "---\ntitle: WebPDFLoader\n---\n\n\nThis notebook provides a quick overview for getting started with [WebPDFLoader](/oss/integrations/document_loaders/). For detailed documentation of all WebPDFLoader features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_document_loaders_web_pdf.WebPDFLoader.html).\n\n## Overview\n\n### Integration details\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/imsdb.mdx",
    "filename": "imsdb.mdx",
    "size_bytes": 478,
    "line_count": 20,
    "preview": "---\ntitle: IMSDB\n---\n\nThis example goes over how to load data from the internet movie script database website, using Cheerio. One document will be created for each page.\n\n## Setup\n\n```bash npm\nnpm install @langchain/community @langchain/core cheerio\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/sitemap.mdx",
    "filename": "sitemap.mdx",
    "size_bytes": 5682,
    "line_count": 92,
    "preview": "---\ntitle: SitemapLoader\n---\n\nThis notebook goes over how to use the [`SitemapLoader`](https://api.js.langchain.com/classes/_langchain_community.document_loaders_web_sitemap.SitemapLoader.html) class to load sitemaps into `Document`s.\n\n## Setup\n\nFirst, we need to install the `langchain` package:\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/azure_blob_storage_file.mdx",
    "filename": "azure_blob_storage_file.mdx",
    "size_bytes": 1237,
    "line_count": 47,
    "preview": "---\ntitle: Azure Blob Storage File\n---\n\n<Tip>\n**Compatibility**\n\nOnly available on Node.js.\n</Tip>\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/sonix_audio_transcription.mdx",
    "filename": "sonix_audio_transcription.mdx",
    "size_bytes": 1262,
    "line_count": 44,
    "preview": "---\ntitle: Sonix Audio\n---\n\n<Tip>\n**Compatibility**\n\nOnly available on Node.js.\n</Tip>\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/taskade.mdx",
    "filename": "taskade.mdx",
    "size_bytes": 1162,
    "line_count": 27,
    "preview": "---\ntitle: Taskade\n---\n\n[Taskade](https://www.taskade.com) is the ultimate tool for AI-driven writing, project management, and task automation. Designed to be your second brain, Taskade simplifies project execution and enhances team collaboration from start to finish.\n\n## Overview\n\nWith [Taskade](https://www.taskade.com), you can build, train, and deploy your own team of AI agents to automate tasks and streamline workflows. Taskade features a seamless blend of ideation, collaboration, and execution tools—from structured lists to modern tables and mind maps, all customizable to fit your unique workflow and adapt to your needs.\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/web_cheerio.mdx",
    "filename": "web_cheerio.mdx",
    "size_bytes": 8531,
    "line_count": 200,
    "preview": "---\ntitle: Cheerio\n---\n\n\nThis notebook provides a quick overview for getting started with [CheerioWebBaseLoader](/oss/integrations/document_loaders/). For detailed documentation of all CheerioWebBaseLoader features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_document_loaders_web_cheerio.CheerioWebBaseLoader.html).\n\n## Overview\n\n### Integration details\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/notionapi.mdx",
    "filename": "notionapi.mdx",
    "size_bytes": 3078,
    "line_count": 77,
    "preview": "---\ntitle: NotionAPI\n---\n\nThis guide will take you through the steps required to load documents from Notion pages and databases using the Notion API.\n\n## Overview\n\nNotion is a versatile productivity platform that consolidates note-taking, task management, and data organization tools into one interface.\n\n"
  }
,
  {
    "path": "javascript/integrations/document_loaders/web_loaders/google_cloud_storage.mdx",
    "filename": "google_cloud_storage.mdx",
    "size_bytes": 1470,
    "line_count": 49,
    "preview": "---\ntitle: Google Cloud Storage\n---\n\n<Tip>\n**Compatibility**\n\nOnly available on Node.js.\n</Tip>\n\n"
  }
,
  {
    "path": "javascript/integrations/llms/cloudflare_workersai.mdx",
    "filename": "cloudflare_workersai.mdx",
    "size_bytes": 2930,
    "line_count": 81,
    "preview": "---\ntitle: CloudflareWorkersAI\n---\n\nThis will help you get started with Cloudflare Workers AI text completion models (LLMs) using LangChain. For detailed documentation on `CloudflareWorkersAI` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_cloudflare.CloudflareWorkersAI.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "javascript/integrations/llms/raycast.mdx",
    "filename": "raycast.mdx",
    "size_bytes": 1470,
    "line_count": 35,
    "preview": "---\ntitle: RaycastAI\n---\n\n> **Note:** This is a community-built integration and is not officially supported by Raycast.\n\nYou can utilize the LangChain's RaycastAI class within the [Raycast Environment](https://developers.raycast.com/api-reference/ai) to enhance your Raycast extension with LangChain's capabilities.\n\n- The RaycastAI class is only available in the Raycast environment and only to [Raycast Pro](https://www.raycast.com/pro) users as of August 2023. You may check how to create an extension for Raycast [here](https://developers.raycast.com/).\n\n"
  }
,
  {
    "path": "javascript/integrations/llms/chrome_ai.mdx",
    "filename": "chrome_ai.mdx",
    "size_bytes": 2388,
    "line_count": 116,
    "preview": "---\ntitle: ChromeAI\n---\n\n<Info>\n**This feature is **experimental** and is subject to change.**\n</Info>\n\n<Note>\nThe `Built-in AI Early Preview Program` by Google is currently in beta. To apply for access or find more information, please visit [this link](https://developer.chrome.com/docs/ai/built-in).\n"
  }
,
  {
    "path": "javascript/integrations/llms/gradient_ai.mdx",
    "filename": "gradient_ai.mdx",
    "size_bytes": 1526,
    "line_count": 65,
    "preview": "---\ntitle: Gradient AI\n---\n\nLangChain.js supports integration with Gradient AI. Check out [Gradient AI](https://docs.gradient.ai/docs) for a list of available models.\n\n## Setup\n\nYou'll need to install the official Gradient Node SDK as a peer dependency:\n\n"
  }
,
  {
    "path": "javascript/integrations/llms/huggingface_inference.mdx",
    "filename": "huggingface_inference.mdx",
    "size_bytes": 698,
    "line_count": 29,
    "preview": "---\ntitle: HuggingFaceInference\n---\n\nHere's an example of calling a HugggingFaceInference model as an LLM:\n\n```bash npm\nnpm install @langchain/community @langchain/core @huggingface/inference@4\n```\n\n"
  }
,
  {
    "path": "javascript/integrations/llms/ni_bittensor.mdx",
    "filename": "ni_bittensor.mdx",
    "size_bytes": 615,
    "line_count": 32,
    "preview": "---\ntitle: NIBittensor\n---\n\n<Warning>\nThis module has been deprecated and is no longer supported. The documentation below will not work in versions 0.2.0 or later.\n</Warning>\n\nLangChain.js offers experimental support for Neural Internet's Bittensor LLM models.\n\n"
  }
,
  {
    "path": "javascript/integrations/llms/ibm.mdx",
    "filename": "ibm.mdx",
    "size_bytes": 7113,
    "line_count": 258,
    "preview": "---\ntitle: IBM watsonx.ai\n---\n\nThis will help you get started with IBM text completion models (LLMs) using LangChain. For detailed documentation on `IBM watsonx.ai` features and configuration options, please refer to the [IBM watsonx.ai](https://api.js.langchain.com/modules/_langchain_community.llms_ibm.html).\n\n## Overview\n\n### Integration details\n\n"
  }
,
  {
    "path": "javascript/integrations/llms/ai21.mdx",
    "filename": "ai21.mdx",
    "size_bytes": 853,
    "line_count": 37,
    "preview": "---\ntitle: AI21\n---\n\nYou can get started with AI21Labs' Jurassic family of models, as well as see a full list of available foundational models, by signing up for an API key [on their website](https://www.ai21.com/).\n\nHere's an example of initializing an instance in LangChain.js:\n\n<Tip>\nSee [this section for general instructions on installing LangChain packages](/oss/langchain/install).\n"
  }
,
  {
    "path": "javascript/integrations/llms/index.mdx",
    "filename": "index.mdx",
    "size_bytes": 4178,
    "line_count": 205,
    "preview": "---\ntitle: LLMs\n---\n\n<Warning>\n**You are currently on a page documenting the use of text completion models. Many of the latest and most popular models are [chat completion models](/oss/langchain/models).**\n\nUnless you are specifically using more advanced prompting techniques, you are probably looking for [this page instead](/oss/integrations/chat/).\n</Warning>\n\n"
  }
,
  {
    "path": "javascript/integrations/llms/openai.mdx",
    "filename": "openai.mdx",
    "size_bytes": 4731,
    "line_count": 118,
    "preview": "---\ntitle: OpenAI\n---\n\n<Warning>\n**You are currently on a page documenting the use of OpenAI text completion models. The latest and most popular OpenAI models are [chat completion models](/oss/langchain/models).**\n\nUnless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/oss/integrations/chat/openai/).\n</Warning>\n\n"
  }
,
  {
    "path": "javascript/integrations/llms/writer.mdx",
    "filename": "writer.mdx",
    "size_bytes": 1005,
    "line_count": 43,
    "preview": "---\ntitle: WRITER\n---\n\nLangChain.js supports calling [WRITER](https://writer.com/) LLMs.\n\n## Setup\n\nFirst, you'll need to sign up for an account at https://writer.com/. Create a service account and note your API key.\n\n"
  }
,
  {
    "path": "javascript/integrations/llms/prompt_layer_openai.mdx",
    "filename": "prompt_layer_openai.mdx",
    "size_bytes": 2619,
    "line_count": 59,
    "preview": "---\ntitle: PromptLayer OpenAI\n---\n\n<Warning>\nThis module has been deprecated and is no longer supported. The documentation below will not work in versions 0.2.0 or later.\n</Warning>\n\nLangChain integrates with PromptLayer for logging and debugging prompts and responses. To add support for PromptLayer:\n\n"
  }
,
  {
    "path": "javascript/integrations/llms/bedrock.mdx",
    "filename": "bedrock.mdx",
    "size_bytes": 9076,
    "line_count": 243,
    "preview": "---\ntitle: Bedrock\n---\n\n<Warning>\n**You are currently on a page documenting the use of Amazon Bedrock models as text completion models. Many popular models available on Bedrock are [chat completion models](/oss/langchain/models).**\n\nYou may be looking for [this page instead](/oss/integrations/chat/bedrock/).\n</Warning>\n\n"
  }
,
  {
    "path": "javascript/integrations/llms/fireworks.mdx",
    "filename": "fireworks.mdx",
    "size_bytes": 4597,
    "line_count": 109,
    "preview": "---\ntitle: Fireworks\n---\n\n\n<Warning>\n**You are currently on a page documenting the use of Fireworks models as text completion models. Many popular models available on Fireworks are [chat completion models](/oss/langchain/models).**\n\nYou may be looking for [this page instead](/oss/integrations/chat/fireworks/).\n</Warning>\n"
  }
,
  {
    "path": "javascript/integrations/llms/jigsawstack.mdx",
    "filename": "jigsawstack.mdx",
    "size_bytes": 920,
    "line_count": 42,
    "preview": "---\ntitle: JigsawStack Prompt Engine\n---\n\nLangChain.js supports calling JigsawStack [Prompt Engine](https://docs.jigsawstack.com/api-reference/prompt-engine/run-direct) LLMs.\n\n## Setup\n\n- Set up an [account](https://jigsawstack.com/dashboard) (Get started for free)\n- Create and retrieve your [API key](https://jigsawstack.com/dashboard)\n"
  }
,
  {
    "path": "javascript/integrations/llms/mistral.mdx",
    "filename": "mistral.mdx",
    "size_bytes": 6520,
    "line_count": 166,
    "preview": "---\ntitle: MistralAI\n---\n\n<Warning>\nYou are currently on a page documenting the use of Mistral models as text completion models. Many popular models available on Mistral are [chat completion models](/oss/langchain/models).\n\nYou may be looking for [this page instead](/oss/integrations/chat/mistral/).\n</Warning>\n\n"
  }
,
  {
    "path": "javascript/integrations/llms/google_vertex_ai.mdx",
    "filename": "google_vertex_ai.mdx",
    "size_bytes": 4786,
    "line_count": 142,
    "preview": "---\ntitle: Google Vertex AI\n---\n\n<Warning>\n**You are currently on a page documenting the use of Google Vertex models as text completion models. Many popular models available on Google Vertex are [chat completion models](/oss/langchain/models).**\n\nYou may be looking for [this page instead](/oss/integrations/chat/google_vertex_ai/).\n</Warning>\n\n"
  }
,
  {
    "path": "javascript/integrations/llms/aleph_alpha.mdx",
    "filename": "aleph_alpha.mdx",
    "size_bytes": 815,
    "line_count": 38,
    "preview": "---\ntitle: AlephAlpha\n---\n\nLangChain.js supports AlephAlpha's Luminous family of models. You'll need to sign up for an API key [on their website](https://www.aleph-alpha.com/).\n\nHere's an example:\n\n<Tip>\nSee [this section for general instructions on installing LangChain packages](/oss/langchain/install).\n"
  }
,
  {
    "path": "javascript/integrations/llms/yandex.mdx",
    "filename": "yandex.mdx",
    "size_bytes": 1153,
    "line_count": 39,
    "preview": "---\ntitle: YandexGPT\n---\n\nLangChain.js supports calling [YandexGPT](https://cloud.yandex.com/en/services/yandexgpt) LLMs.\n\n## Setup\n\nFirst, you should [create service account](https://cloud.yandex.com/en/docs/iam/operations/sa/create) with the `ai.languageModels.user` role.\n\n"
  }
,
  {
    "path": "javascript/integrations/llms/aws_sagemaker.mdx",
    "filename": "aws_sagemaker.mdx",
    "size_bytes": 2432,
    "line_count": 107,
    "preview": "---\ntitle: AWS SageMakerEndpoint\n---\n\nLangChain.js supports integration with AWS SageMaker-hosted endpoints. Check [Amazon SageMaker JumpStart](https://aws.amazon.com/sagemaker/jumpstart/) for a list of available models, and how to deploy your own.\n\n## Setup\n\nYou'll need to install the official SageMaker SDK as a peer dependency:\n\n"
  }
,
  {
    "path": "javascript/integrations/llms/azure.mdx",
    "filename": "azure.mdx",
    "size_bytes": 8787,
    "line_count": 191,
    "preview": "---\ntitle: Azure OpenAI\n---\n\n<Warning>\n**You are currently on a page documenting the use of Azure OpenAI text completion models. The latest and most popular Azure OpenAI models are [chat completion models](/oss/langchain/models).**\n\nUnless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/oss/integrations/chat/azure/).\n</Warning>\n\n"
  }
,
  {
    "path": "javascript/integrations/llms/friendli.mdx",
    "filename": "friendli.mdx",
    "size_bytes": 1849,
    "line_count": 76,
    "preview": "---\ntitle: Friendli\n---\n\n> [Friendli](https://friendli.ai/) enhances AI application performance and optimizes cost savings with scalable, efficient deployment options, tailored for high-demand AI workloads.\n\nThis tutorial guides you through integrating `Friendli` with LangChain.\n\n## Setup\n\n"
  }
,
  {
    "path": "javascript/integrations/llms/together.mdx",
    "filename": "together.mdx",
    "size_bytes": 4734,
    "line_count": 94,
    "preview": "---\ntitle: TogetherAI\n---\n\n<Warning>\n**You are currently on a page documenting the use of Together AI models as text completion models. Many popular models available on Together AI are [chat completion models](/oss/langchain/models).**\n\nYou may be looking for [this page instead](/oss/integrations/chat/togetherai/).\n</Warning>\n\n"
  }
,
  {
    "path": "javascript/integrations/llms/cohere.mdx",
    "filename": "cohere.mdx",
    "size_bytes": 3595,
    "line_count": 111,
    "preview": "---\ntitle: Cohere\n---\n\n<Warning>\n**Legacy**\n\nCohere has marked their `generate` endpoint for LLMs as deprecated. Follow their [migration guide](https://docs.cohere.com/docs/migrating-from-cogenerate-to-cochat) to start using their Chat API via the [`ChatCohere`](/oss/integrations/chat/cohere) integration.\n</Warning>\n\n"
  }
,
  {
    "path": "javascript/integrations/llms/arcjet.mdx",
    "filename": "arcjet.mdx",
    "size_bytes": 3208,
    "line_count": 98,
    "preview": "---\ntitle: Arcjet Redact\n---\n\nThe [Arcjet](https://arcjet.com) redact integration allows you to redact sensitive user information from your prompts before sending it to an LLM.\n\nArcjet Redact runs entirely on your own machine and never sends data anywhere else, ensuring best in class privacy and performance.\n\nThe Arcjet Redact object is not an LLM itself, instead it wraps an LLM. It redacts the text that is inputted to it and then unredacts the output of the wrapped LLM before returning it.\n\n"
  }
,
  {
    "path": "javascript/integrations/llms/ollama.mdx",
    "filename": "ollama.mdx",
    "size_bytes": 4762,
    "line_count": 124,
    "preview": "---\ntitle: Ollama\n---\n\n<Warning>\n**You are currently on a page documenting the use of Ollama models as text completion models. Many popular models available on Ollama are [chat completion models](/oss/langchain/models).**\n\nYou may be looking for [this page instead](/oss/integrations/chat/ollama/).\n</Warning>\n\n"
  }
,
  {
    "path": "javascript/integrations/llms/replicate.mdx",
    "filename": "replicate.mdx",
    "size_bytes": 1403,
    "line_count": 47,
    "preview": "---\ntitle: Replicate\n---\n\nHere's an example of calling a Replicate model as an LLM:\n\n<Tip>\nSee [this section for general instructions on installing LangChain packages](/oss/langchain/install).\n</Tip>\n\n"
  }
,
  {
    "path": "javascript/integrations/llms/deep_infra.mdx",
    "filename": "deep_infra.mdx",
    "size_bytes": 1023,
    "line_count": 43,
    "preview": "---\ntitle: DeepInfra\n---\n\nLangChain supports LLMs hosted by [Deep Infra](https://deepinfra.com/) through the `DeepInfra` wrapper.\nFirst, you'll need to install the `@langchain/community` package:\n\n<Tip>\nSee [this section for general instructions on installing LangChain packages](/oss/langchain/install).\n</Tip>\n"
  }
,
  {
    "path": "javascript/integrations/llms/llama_cpp.mdx",
    "filename": "llama_cpp.mdx",
    "size_bytes": 6755,
    "line_count": 150,
    "preview": "---\ntitle:  Llama CPP\n---\n\n<Tip>\n**Compatibility**\n\nOnly available on Node.js.\n</Tip>\n\n"
  }
,
  {
    "path": "javascript/integrations/llms/layerup_security.mdx",
    "filename": "layerup_security.mdx",
    "size_bytes": 3140,
    "line_count": 93,
    "preview": "---\ntitle: Layerup Security\n---\n\nThe [Layerup Security](https://uselayerup.com) integration allows you to secure your calls to any LangChain LLM, LLM chain or LLM agent. The LLM object wraps around any existing LLM object, allowing for a secure layer between your users and your LLMs.\n\nWhile the Layerup Security object is designed as an LLM, it is not actually an LLM itself, it simply wraps around an LLM, allowing it to adapt the same functionality as the underlying LLM.\n\n## Setup\n\n"
  }
,
  {
    "path": "javascript/migrate/langgraph-v1.mdx",
    "filename": "langgraph-v1.mdx",
    "size_bytes": 3963,
    "line_count": 142,
    "preview": "---\ntitle: LangGraph v1 migration guide\nsidebarTitle: LangGraph v1\n---\n\nThis guide outlines changes in LangGraph v1 and how to migrate from previous versions. For a high-level overview of what's new, see the [release notes](/oss/releases/langgraph-v1).\n\nTo upgrade,\n\n<CodeGroup>\n"
  }
,
  {
    "path": "javascript/migrate/langchain-v1.mdx",
    "filename": "langchain-v1.mdx",
    "size_bytes": 22526,
    "line_count": 781,
    "preview": "---\ntitle: LangChain v1 migration guide\nsidebarTitle: LangChain v1\n---\n\nThis migration guide outlines the major changes in LangChain v1. To learn more about the new features of v1, see the [introductory post](/oss/releases/langchain-v1).\n\nTo upgrade,\n\n<CodeGroup>\n"
  }
,
  {
    "path": "deepagents/cli.mdx",
    "filename": "cli.mdx",
    "size_bytes": 9832,
    "line_count": 277,
    "preview": "---\ntitle: Deep Agents CLI\nsidebarTitle: Use the CLI\ndescription: Interactive command-line interface for building with Deep Agents\n---\n\nThe Deep Agents CLI is an open source coding assistant that runs in your terminal and retains persistent memory.\nYour CLI agents maintain context across sessions, learn project conventions, and execute code with approval controls.\n\nThe Deep Agents CLI has the following built-in capabilities:\n"
  }
,
  {
    "path": "deepagents/overview.mdx",
    "filename": "overview.mdx",
    "size_bytes": 4404,
    "line_count": 92,
    "preview": "---\ntitle: Deep Agents overview\nsidebarTitle: Overview\ndescription: Build agents that can plan, use subagents, and leverage file systems for complex tasks\n---\n\n:::python\n[`deepagents`](https://pypi.org/project/deepagents/) is a standalone library for building agents that can tackle complex, multi-step tasks. Built on LangGraph and inspired by applications like Claude Code, Deep Research, and Manus, deep agents come with planning capabilities, file systems for context management, and the ability to spawn subagents.\n:::\n\n"
  }
,
  {
    "path": "deepagents/customization.mdx",
    "filename": "customization.mdx",
    "size_bytes": 5182,
    "line_count": 200,
    "preview": "---\ntitle: Customize Deep Agents\nsidebarTitle: Customization\ndescription: Learn how to customize deep agents with system prompts, tools, subagents, and more\n---\n\n```mermaid\ngraph LR\n    Create[create_deep_agent] --> Core[Core Config]\n    Create --> Features[Features]\n"
  }
,
  {
    "path": "deepagents/harness.mdx",
    "filename": "harness.mdx",
    "size_bytes": 7025,
    "line_count": 180,
    "preview": "---\ntitle: Agent harness capabilities\nsidebarTitle: Agent harness\n---\n\nWe think of `deepagents` as an [\"agent harness\"](https://blog.langchain.com/agent-frameworks-runtimes-and-harnesses-oh-my/). It is the same core tool calling loop as other agent frameworks, but with built-in tools and capabilities.\n\n```mermaid\ngraph TB\n    Agent[Deep Agent] --> Tools[File System Tools]\n"
  }
,
  {
    "path": "deepagents/long-term-memory.mdx",
    "filename": "long-term-memory.mdx",
    "size_bytes": 11762,
    "line_count": 425,
    "preview": "---\ntitle: Long-term memory\ndescription: Learn how to extend deep agents with persistent memory across threads\n---\n\nDeep agents come with a local filesystem to offload memory. By default, this filesystem is stored in agent state and is **transient to a single thread**—files are lost when the conversation ends.\n\nYou can extend deep agents with **long-term memory** by using a `CompositeBackend` that routes specific paths to persistent storage. This enables hybrid storage where some files persist across threads while others remain ephemeral.\n\n```mermaid\n"
  }
,
  {
    "path": "deepagents/backends.mdx",
    "filename": "backends.mdx",
    "size_bytes": 15245,
    "line_count": 390,
    "preview": "---\ntitle: Backends\ndescription: Choose and configure filesystem backends for deep agents. You can specify routes to different backends, implement virtual filesystems, and enforce policies.\n---\n\nDeep agents expose a filesystem surface to the agent via tools like `ls`, `read_file`, `write_file`, `edit_file`, `glob`, and `grep`. These tools operate through a pluggable backend.\n\n```mermaid\ngraph TB\n    Tools[Filesystem Tools] --> Backend[Backend]\n"
  }
,
  {
    "path": "deepagents/middleware.mdx",
    "filename": "middleware.mdx",
    "size_bytes": 11503,
    "line_count": 354,
    "preview": "---\ntitle: Deep Agents Middleware\nsidebarTitle: Middleware\ndescription: Understand the middleware that powers deep agents\n---\n\nDeep agents are built with a modular middleware architecture. Deep agents have access to:\n\n1. A planning tool\n2. A filesystem for storing context and long-term memories\n"
  }
,
  {
    "path": "deepagents/human-in-the-loop.mdx",
    "filename": "human-in-the-loop.mdx",
    "size_bytes": 13746,
    "line_count": 511,
    "preview": "---\ntitle: Human-in-the-loop\ndescription: Learn how to configure human approval for sensitive tool operations\n---\n\nSome tool operations may be sensitive and require human approval before execution. Deep agents support human-in-the-loop workflows through LangGraph's interrupt capabilities. You can configure which tools require approval using the `interrupt_on` parameter.\n\n```mermaid\ngraph LR\n    Agent[Agent] --> Check{Interrupt?}\n"
  }
,
  {
    "path": "deepagents/quickstart.mdx",
    "filename": "quickstart.mdx",
    "size_bytes": 6101,
    "line_count": 223,
    "preview": "---\ntitle: Quickstart\ndescription: Build your first deep agent in minutes\n---\n\nThis guide walks you through creating your first deep agent with planning, file system tools, and subagent capabilities. You'll build a research agent that can conduct research and write reports.\n\n## Prerequisites\n\nBefore you begin, make sure you have an API key from a model provider (e.g., Anthropic, OpenAI).\n"
  }
,
  {
    "path": "deepagents/subagents.mdx",
    "filename": "subagents.mdx",
    "size_bytes": 20845,
    "line_count": 734,
    "preview": "---\ntitle: Subagents\ndescription: Learn how to use subagents to delegate work and keep context clean\n---\n\nDeep agents can create subagents to delegate work. You can specify custom subagents in the `subagents` parameter. Subagents are useful for [context quarantine](https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html#context-quarantine) (keeping the main agent's context clean) and for providing specialized instructions.\n\n```mermaid\ngraph TB\n    Main[Main Agent] --> |task tool| Sub[Subagent]\n"
  }
,
  {
    "path": "reference/overview.mdx",
    "filename": "overview.mdx",
    "size_bytes": 2094,
    "line_count": 71,
    "preview": "---\ntitle: Reference\nsidebarTitle: Overview\n---\n\nComprehensive API reference documentation for the LangChain and LangGraph Python and TypeScript libraries.\n\n## Reference sites\n\n<CardGroup cols={2}>\n"
  }

]
